{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "optional-directory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hid_orderings [0, 12, 4, 7, 14, 8, 10, 12, 12, 11, 12, 7, 4, 1, 3, 10, 0, 4, 14, 9, 0, 12, 10, 13, 12, 14, 7, 3, 12, 1, 8, 5, 1, 2, 0, 7, 12, 12, 11, 13, 3, 3, 4, 9, 10, 8, 0, 8, 6, 1, 4, 5, 14, 13, 8, 5, 4, 7, 11, 7, 2, 11, 2, 1] size 64\n",
      "num hid layers: 1\n",
      "expanded_input_ordering [0, 1, 1, 1, 2, 3] size 6\n",
      "expanded_output_ordering [14, 14] size 2\n",
      "hid_orderings [13, 2, 6, 22, 21, 25, 28, 30, 18, 28, 3, 9, 11, 23, 17, 9, 5, 14, 26, 1, 8, 12, 20, 16, 11, 2, 8, 2, 6, 16, 27, 29, 21, 14, 1, 8, 27, 16, 8, 5, 5, 24, 28, 14, 12, 0, 28, 12, 15, 30, 8, 22, 6, 4, 19, 21, 11, 26, 30, 26, 30, 1, 19, 19, 18, 28, 10, 5, 21, 28, 22, 7, 8, 16, 26, 27, 6, 28, 21, 23, 10, 22, 21, 23, 15, 1, 19, 3, 11, 4, 26, 29, 28, 2, 8, 3, 29, 11, 29, 19, 21, 19, 20, 6, 2, 3, 28, 20, 1, 13, 21, 15, 27, 1, 6, 4, 1, 21, 30, 16, 4, 3, 10, 16, 25, 7, 25, 10] size 128\n",
      "num hid layers: 1\n",
      "expanded_input_ordering [0, 0, 0, 1, 2, 3, 4] size 7\n",
      "expanded_output_ordering [30, 30] size 2\n",
      "hid_orderings [9, 4, 28, 26, 3, 8, 26, 23, 30, 2, 17, 19, 10, 21, 23, 13, 16, 14, 1, 2, 5, 26, 23, 27, 6, 23, 25, 27, 18, 12, 24, 27, 15, 30, 4, 15, 29, 3, 28, 10, 6, 10, 4, 3, 1, 23, 25, 23, 14, 8, 11, 22, 25, 28, 17, 27, 5, 12, 22, 22, 16, 9, 23, 28, 15, 27, 17, 10, 11, 8, 7, 23, 28, 21, 10, 28, 30, 28, 13, 1, 23, 13, 10, 0, 1, 2, 16, 10, 27, 19, 4, 26, 23, 4, 17, 11, 27, 15, 5, 19, 26, 15, 28, 16, 9, 8, 23, 11, 17, 27, 20, 2, 6, 14, 24, 25, 7, 11, 28, 19, 28, 9, 3, 1, 28, 16, 5, 20] size 128\n",
      "num hid layers: 1\n",
      "expanded_input_ordering [0, 0, 0, 1, 2, 3, 4] size 7\n",
      "expanded_output_ordering [30, 30] size 2\n",
      "number of levels: 3\n",
      "input_levels [['r', 'h', 'obs_R', 'n'], ['h', 'r', 'y_1', 'n', 'obs_R'], ['h', 'r', 'n', 'obs_R', 'y_2']]\n",
      "out_levels [['y_1'], ['y_2'], ['next_x']]\n",
      "[Step 10/400] Immediate Loss: 9.718689568340775 Accumlated Loss: 27.274253101468098\n",
      "[Step 20/400] Immediate Loss: 5.954498555362225 Accumlated Loss: 7.442938693076371\n",
      "[Step 30/400] Immediate Loss: 4.447750243544577 Accumlated Loss: 5.4181610617339615\n",
      "[Step 40/400] Immediate Loss: 5.012440741360186 Accumlated Loss: 5.140328767389058\n",
      "[Step 50/400] Immediate Loss: 4.490741641819476 Accumlated Loss: 4.7582072620987885\n",
      "[Step 60/400] Immediate Loss: 4.29553438693285 Accumlated Loss: 4.406865348130465\n",
      "[Step 70/400] Immediate Loss: 3.7840854701399813 Accumlated Loss: 4.351466654658317\n",
      "[Step 80/400] Immediate Loss: 3.970056611895562 Accumlated Loss: 4.292096051037312\n",
      "[Step 90/400] Immediate Loss: 3.6745517441630375 Accumlated Loss: 4.016824558377266\n",
      "[Step 100/400] Immediate Loss: 3.390196449160574 Accumlated Loss: 3.809407312333584\n",
      "[Step 110/400] Immediate Loss: 3.5229877656698214 Accumlated Loss: 3.720723236352205\n",
      "[Step 120/400] Immediate Loss: 3.5516776442527767 Accumlated Loss: 3.5181323789954186\n",
      "[Step 130/400] Immediate Loss: 4.005537402927876 Accumlated Loss: 3.6797128836214537\n",
      "[Step 140/400] Immediate Loss: 3.322379320561886 Accumlated Loss: 3.4825260668694975\n",
      "[Step 150/400] Immediate Loss: 3.5359922349452955 Accumlated Loss: 3.382486345291137\n",
      "[Step 160/400] Immediate Loss: 3.5511559665203096 Accumlated Loss: 3.5094061300456523\n",
      "[Step 170/400] Immediate Loss: 2.864115859866141 Accumlated Loss: 3.2546061300039293\n",
      "[Step 180/400] Immediate Loss: 3.5349232301115987 Accumlated Loss: 3.238091893464326\n",
      "[Step 190/400] Immediate Loss: 3.152664430439473 Accumlated Loss: 3.1974863410592076\n",
      "[Step 200/400] Immediate Loss: 3.1726841244101527 Accumlated Loss: 3.1872330761551857\n",
      "[Step 210/400] Immediate Loss: 3.416357156336308 Accumlated Loss: 3.180092504769564\n",
      "[Step 220/400] Immediate Loss: 3.146424490511417 Accumlated Loss: 3.0806499176621434\n",
      "[Step 230/400] Immediate Loss: 3.013438300490378 Accumlated Loss: 2.9854666061401356\n",
      "[Step 240/400] Immediate Loss: 2.9194777941703784 Accumlated Loss: 2.9923981770575043\n",
      "[Step 250/400] Immediate Loss: 2.783120102882385 Accumlated Loss: 2.877790439516306\n",
      "[Step 260/400] Immediate Loss: 2.711076886355876 Accumlated Loss: 2.822842698276043\n",
      "[Step 270/400] Immediate Loss: 2.8767710292339315 Accumlated Loss: 3.0551522636711597\n",
      "[Step 280/400] Immediate Loss: 2.771791325211524 Accumlated Loss: 2.7849093954563133\n",
      "[Step 290/400] Immediate Loss: 2.7967741933465007 Accumlated Loss: 2.8776209779679776\n",
      "[Step 300/400] Immediate Loss: 2.7759690797328944 Accumlated Loss: 2.8702705132365223\n",
      "[Step 310/400] Immediate Loss: 2.7666365104913706 Accumlated Loss: 2.9032342261075974\n",
      "[Step 320/400] Immediate Loss: 3.057903106808663 Accumlated Loss: 2.840779209971428\n",
      "[Step 330/400] Immediate Loss: 2.591823111176489 Accumlated Loss: 2.8286642355322833\n",
      "[Step 340/400] Immediate Loss: 2.78667536199093 Accumlated Loss: 2.829502729386091\n",
      "[Step 350/400] Immediate Loss: 2.7149584043025974 Accumlated Loss: 2.784488169759512\n",
      "[Step 360/400] Immediate Loss: 3.036436093747617 Accumlated Loss: 2.782432957470417\n",
      "[Step 370/400] Immediate Loss: 2.9420209306478493 Accumlated Loss: 2.9134493502080443\n",
      "[Step 380/400] Immediate Loss: 2.716368767023086 Accumlated Loss: 2.669479212582111\n",
      "[Step 390/400] Immediate Loss: 2.943860490322113 Accumlated Loss: 2.7135670669674874\n",
      "[Step 400/400] Immediate Loss: 3.0542798823118202 Accumlated Loss: 2.8114830656945697\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pyro\n",
    "import pyro.contrib.examples.polyphonic_data_loader as poly\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "from made import MADE\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal, AutoNormal, AutoMultivariateNormal\n",
    "from gmade import GMADE\n",
    "random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# NN used for p(x | y)\n",
    "class simpleNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden=32, out_size=1, t=\"normal\", out_non_linear=None):\n",
    "        super().__init__()\n",
    "        self.t = t\n",
    "        self.out_non_linear = out_non_linear\n",
    "        self.hiddeen_layer = nn.Linear(input_size, hidden)\n",
    "        if t == \"normal\":\n",
    "            self.loc_layer = nn.Linear(hidden, out_size)\n",
    "            self.std_layer = nn.Linear(hidden, out_size)\n",
    "            self.softplus = nn.Softplus()\n",
    "        elif t == \"mlp\":\n",
    "            self.out_layer = nn.Linear(hidden, out_size)\n",
    "    def forward(self, x_list):\n",
    "        for i in range(len(x_list)):\n",
    "            if x_list[i].dim() == 0:\n",
    "                x_list[i] = torch.unsqueeze(x_list[i], dim=0)\n",
    "        input_x = torch.cat(x_list)\n",
    "        hid = F.relu(self.hiddeen_layer(input_x))\n",
    "        # return loc, std\n",
    "        if self.t == \"normal\":\n",
    "            return self.loc_layer(hid), self.softplus(self.std_layer(hid))\n",
    "        else:\n",
    "            if self.out_non_linear == \"tanh\":\n",
    "                return torch.tanh(self.out_layer(hid))\n",
    "            else:\n",
    "                return self.out_layer(hid)\n",
    "        \n",
    "class simpleRNN(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=32, max_l=5, max_t=3):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_size + max_l + max_t, hidden_size=hidden_size, nonlinearity='relu',\n",
    "                          batch_first=True, num_layers=1)\n",
    "        self.h_0 = nn.Parameter(torch.zeros((1, 1, hidden_size)))\n",
    "        self.out_loc = nn.Linear(hidden_size, 1)\n",
    "        self.out_std = nn.Linear(hidden_size, 1)\n",
    "        self.max_l = max_l # max time steps\n",
    "        self.max_t = max_t # type of random variables, ex. y1 y2 next_x\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "    def forward(self, x, obs, l, t):\n",
    "        \"\"\"\n",
    "        x: x0\n",
    "        obs: R\n",
    "        l: length\n",
    "        t: type, y1, y2 or next_x\n",
    "        \"\"\"\n",
    "        length = l * 3\n",
    "        input_x = x.repeat((int(length), 1))\n",
    "        input_obs = obs.repeat((int(length), 1))\n",
    "        input_l = []\n",
    "        input_t = []\n",
    "        \n",
    "        for n in range(int(l)):\n",
    "            \n",
    "            for i in range(int(t)):\n",
    "                input_l.append(n)\n",
    "                input_t.append(i)\n",
    "        input_l = F.one_hot(torch.tensor(input_l), self.max_l) \n",
    "        input_t = F.one_hot(torch.tensor(input_t), self.max_t) \n",
    "        \n",
    "        input_ = torch.unsqueeze(torch.cat([input_x, input_obs, input_l, input_t], -1), 0)\n",
    "        \n",
    "        # the input is [x, obs, onehot(l), onehot(t)]\n",
    "        rnn_output, _ = self.rnn(input_, self.h_0)\n",
    "        rnn_output = torch.squeeze(rnn_output, 0)\n",
    "        out_loc = self.out_loc(F.relu(rnn_output))\n",
    "        out_std = self.softplus(self.out_std(F.relu(rnn_output)))\n",
    "        # the first outputs are y_1_1, y_2_1, next_x_1; y_1_2, y_2_2, next_x_2\n",
    "        return torch.squeeze(out_loc, 1), torch.squeeze(out_std, 1) # shape l * t\n",
    "\n",
    "class Experiment(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_size = 3\n",
    "        \n",
    "        # for guide_0\n",
    "        self.x_y1_nn_0 = simpleNN(input_size= 3 + self.hidden_size)\n",
    "        self.y1_y2_nn_0 = simpleNN(input_size=4 + self.hidden_size)\n",
    "        self.y_x_nn_0 = simpleNN(input_size=4 + self.hidden_size)\n",
    "        self.h_encoder_0 = simpleNN(input_size=1 + self.hidden_size, out_size=self.hidden_size, t=\"mlp\")\n",
    "        self.h0_0 = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "        \n",
    "        # for guide_1\n",
    "        self.x_y1_nn_1 = simpleNN(input_size= 2 + self.hidden_size)\n",
    "        self.y1_y2_nn_1 = simpleNN(input_size=3 + self.hidden_size)\n",
    "        self.y_x_nn_1 = simpleNN(input_size=3 + self.hidden_size)\n",
    "        self.h_encoder_1 = simpleNN(input_size=1 + self.hidden_size, out_size=self.hidden_size, t=\"mlp\")\n",
    "        self.h0_1 = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "        \n",
    "        # for guide_2\n",
    "        self.x_y1_nn_2 = simpleNN(input_size= 2 + self.hidden_size)\n",
    "        self.y1_y2_nn_2 = simpleNN(input_size=2 + self.hidden_size)\n",
    "        self.y_x_nn_2 = simpleNN(input_size=2 + self.hidden_size)\n",
    "        self.h_encoder_2 = simpleNN(input_size=1 + self.hidden_size, out_size=self.hidden_size, t=\"mlp\")\n",
    "        self.h0_2 = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "        \n",
    "        # for guide_3\n",
    "        self.x_y1_nn_3 = simpleNN(input_size= 1 + self.hidden_size)\n",
    "        self.y1_y2_nn_3 = simpleNN(input_size=2 + self.hidden_size)\n",
    "        self.y_x_nn_3 = simpleNN(input_size=2 + self.hidden_size)\n",
    "        self.h_encoder_3 = simpleNN(input_size=1 + self.hidden_size, out_size=self.hidden_size, t=\"mlp\")\n",
    "        self.h0_3 = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "        \n",
    "        # for guide_made_0\n",
    "        self.hidden_size_made_0 = 2\n",
    "        self.made_in_dim_0 = 1 + self.hidden_size_made_0\n",
    "        self.num_var_0 = 3\n",
    "        self.made_out_dim_0 = self.made_in_dim_0 * 2\n",
    "        self.made_hidden_0 = [32, 32]\n",
    "        self.made_0 = MADE(self.made_in_dim_0, self.made_hidden_0, self.made_out_dim_0, num_masks=1, natural_ordering=False)\n",
    "        self.h_encoder_made_0 = simpleNN(input_size=1 + self.hidden_size_made_0, out_size=self.hidden_size_made_0, t=\"mlp\")\n",
    "        self.h0_made_0 = nn.Parameter(torch.zeros(self.hidden_size_made_0))\n",
    "        \n",
    "        \n",
    "        # for guide_made_1\n",
    "        self.hidden_size_made_1 = 3\n",
    "        self.made_in_dim_1 = 3 + self.hidden_size_made_1\n",
    "        self.num_var_1 = 3\n",
    "        self.made_out_dim_1 = self.made_in_dim_1 * self.num_var_1\n",
    "        self.made_hidden_1 = [32, 32]\n",
    "        self.made_1 = MADE(self.made_in_dim_1, self.made_hidden_1, self.made_out_dim_1, num_masks=1, natural_ordering=True)\n",
    "        self.made_out_loc_linear_1 = nn.ModuleList([nn.Linear(self.made_in_dim_1, 1) for _ in range(self.num_var_1)])\n",
    "        self.made_out_std_linear_1 = nn.ModuleList([nn.Linear(self.made_in_dim_1, 1) for _ in range(self.num_var_1)])\n",
    "        \n",
    "        self.h_encoder_made_1 = simpleNN(input_size=1 + self.hidden_size_made_1, out_size=self.hidden_size_made_1, t=\"mlp\")\n",
    "        self.h0_made_1 = nn.Parameter(torch.zeros(self.hidden_size_made_1))\n",
    "        \n",
    "        # for guide_made_2\n",
    "        self.hidden_size_made_2 = 3\n",
    "        self.num_var_2 = 3\n",
    "        self.made_in_dim_2 = self.num_var_2\n",
    "        self.made_out_dim_2 = self.made_in_dim_2 * 2\n",
    "        self.made_hidden_2 = [32, 32]\n",
    "        self.made_2 = MADE(self.made_in_dim_2, self.made_hidden_2, self.made_out_dim_2, num_masks=1, natural_ordering=False)\n",
    "        self.h_encoder_made_2 = simpleNN(input_size=1 + self.hidden_size_made_2, out_size=self.hidden_size_made_2, t=\"mlp\")\n",
    "        self.input_encoder_made_2 = simpleNN(input_size=3 + self.hidden_size_made_2, out_size=self.made_in_dim_2, t=\"mlp\")\n",
    "        self.h0_made_2 = nn.Parameter(torch.zeros(self.hidden_size_made_2))\n",
    "        \n",
    "        \n",
    "        # for guide_made_3\n",
    "        self.hidden_size_made_3 = 3\n",
    "        self.made_in_dim_3 = 3 + self.hidden_size_made_3\n",
    "        self.num_var_3 = 3\n",
    "        self.made_out_dim_3 = self.made_in_dim_3 * self.num_var_3\n",
    "        self.made_hidden_3 = [32, 32]\n",
    "        self.made_3 = MADE(self.made_in_dim_3, self.made_hidden_3, self.made_out_dim_3, num_masks=1, natural_ordering=True)\n",
    "        self.made_out_loc_linear_3 = nn.Linear(self.made_in_dim_3, 1) \n",
    "        self.made_out_std_linear_3 = nn.Linear(self.made_in_dim_3, 1)\n",
    "        \n",
    "        self.h_encoder_made_3 = simpleNN(input_size=1 + self.hidden_size_made_3, out_size=self.hidden_size_made_3, t=\"mlp\")\n",
    "        self.h0_made_3 = nn.Parameter(torch.zeros(self.hidden_size_made_3))\n",
    "        \n",
    "        \n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "        \n",
    "        # for guide_rnn\n",
    "        self.simrnn = simpleRNN()\n",
    "        \n",
    "        \n",
    "        # for guide_g_made\n",
    "        input_dim_dict = {\n",
    "            \"r\" : 1,\n",
    "            \"h\" : 3,\n",
    "            \"obs_R\": 1,\n",
    "            \"n\" : 1\n",
    "        }\n",
    "        var_dim_dict = {\n",
    "            \"y_1\" : 1,\n",
    "            \"y_2\" : 1,\n",
    "            \"next_x\" : 1\n",
    "        }\n",
    "        dependency_dict = {\n",
    "            \"y_1\" : [\"r\", \"h\", \"obs_R\", \"n\"],\n",
    "            \"y_2\" : [\"y_1\", \"r\", \"h\", \"obs_R\", \"n\"],\n",
    "            \"next_x\" : [\"y_2\", \"r\", \"h\", \"obs_R\", \"n\"]\n",
    "        }\n",
    "        self.gmade = GMADE(input_dim_dict, dependency_dict, var_dim_dict)\n",
    "        self.h_encoder_gmade = simpleNN(input_size=1 + self.hidden_size, out_size=self.hidden_size, t=\"mlp\")\n",
    "        self.h0_gmade = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "        \n",
    "        \n",
    "    def g(self, x):\n",
    "        return torch.tanh(x) \n",
    "    \n",
    "    # assume the model has two latent variables in each time step (note that groundtruth data gen has 3)\n",
    "    # as we cannot build the perfect model in real life problems\n",
    "    def model(self, x0, l, obs_R):\n",
    "        pyro.module(\"model\", self)\n",
    "        def rec_model(x, n):\n",
    "            r = self.g(x) # TODO change r to sample statement\n",
    "            if n <= 0:\n",
    "                return r\n",
    "            else:\n",
    "                y_1 = pyro.sample(\"y_1_{}\".format(n), dist.Normal(x, 0.5))\n",
    "             \n",
    "                y_2 = pyro.sample(\"y_2_{}\".format(n), dist.Normal(y_1, 0.5))\n",
    "            \n",
    "                next_x = pyro.sample(\"next_x_{}\".format(n), dist.Normal(y_2, 0.5))\n",
    "                return rec_model(next_x, n - 1) + r\n",
    "    \n",
    "        R = rec_model(x0, l)\n",
    "    \n",
    "        pyro.sample(\"obs_R\", dist.Normal(R, 0.1), obs=obs_R)\n",
    "        \n",
    "    # guide uses simple and individual NN for each random variable, given obs_R for all random variables\n",
    "    def guide_0(self, x0, l, obs_R):\n",
    "        pyro.module(\"model\", self)\n",
    "        def rec_guide(x, n, h, obs_R):\n",
    "            r = self.g(x)\n",
    "            if n <= 0:\n",
    "                return r\n",
    "            else:\n",
    "                y_1_loc, y_1_std = self.x_y1_nn_0([r, h, obs_R, n]) \n",
    "                y_1 = pyro.sample(\"y_1_{}\".format(n), dist.Normal(y_1_loc, y_1_std))\n",
    "                y_2_loc, y_2_std = self.y1_y2_nn_0([y_1, r, h, obs_R, n])\n",
    "                y_2 = pyro.sample(\"y_2_{}\".format(n), dist.Normal(y_2_loc, y_2_std))\n",
    "                x_loc, x_std = self.y_x_nn_0([y_2, r, h, obs_R, n])\n",
    "                next_x = pyro.sample(\"next_x_{}\".format(n), dist.Normal(x_loc, x_std))\n",
    "                \n",
    "                h = self.h_encoder_0([h, r])\n",
    "                return rec_guide(next_x, n - 1, h, obs_R) + r\n",
    "    \n",
    "        rec_guide(x0, l, self.h0_0, obs_R)\n",
    "    \n",
    "    # guide uses simple and individual NN for each random variable, given obs_R for all random variables\n",
    "    def guide_gmade(self, x0, l, obs_R):\n",
    "        pyro.module(\"model\", self)\n",
    "        def rec_guide(x, n, h, obs_R):\n",
    "            r = self.g(x)\n",
    "            if n <= 0:\n",
    "                return r\n",
    "            else:\n",
    "                input_made = {\n",
    "                    \"r\" : r,\n",
    "                    \"h\" : h,\n",
    "                    \"obs_R\" : obs_R,\n",
    "                    \"n\" : n\n",
    "                }\n",
    "                output_dict = self.gmade(input_made, suffix=\"_{}\".format(n))\n",
    "                \n",
    "                h = self.h_encoder_0([h, r])\n",
    "                return rec_guide(output_dict[\"next_x\"], n - 1, h, obs_R) + r\n",
    "    \n",
    "        rec_guide(x0, l, self.h0_0, obs_R)\n",
    "    \n",
    "    # guide uses simple and individual NN for each random variable, given obs_R for all random variables\n",
    "    def guide_1(self, x0, l, obs_R):\n",
    "        pyro.module(\"model\", self)\n",
    "        def rec_guide(x, n, h, obs_R):\n",
    "            r = self.g(x)\n",
    "            if n <= 0:\n",
    "                return r\n",
    "            else:\n",
    "                y_1_loc, y_1_std = self.x_y1_nn_1([r, h, obs_R]) \n",
    "                y_1 = pyro.sample(\"y_1_{}\".format(n), dist.Normal(y_1_loc, y_1_std))\n",
    "                y_2_loc, y_2_std = self.y1_y2_nn_1([y_1, r, h, obs_R]) \n",
    "                y_2 = pyro.sample(\"y_2_{}\".format(n), dist.Normal(y_2_loc, y_2_std))\n",
    "                x_loc, x_std = self.y_x_nn_1([y_2, r, h, obs_R])\n",
    "                next_x = pyro.sample(\"next_x_{}\".format(n), dist.Normal(x_loc, x_std))\n",
    "                \n",
    "                h = self.h_encoder_1([h, r])\n",
    "                return rec_guide(next_x, n - 1, h, obs_R) + r\n",
    "    \n",
    "        rec_guide(x0, l, self.h0_1, obs_R)\n",
    "    \n",
    "    # guide uses simple and individual NN for each random variable, given obs_R for y_1\n",
    "    def guide_2(self, x0, l, obs_R):\n",
    "        pyro.module(\"model\", self)\n",
    "        def rec_guide(x, n, h, obs_R):\n",
    "            r = self.g(x)\n",
    "            if n <= 0:\n",
    "                return r\n",
    "            else:\n",
    "                y_1_loc, y_1_std = self.x_y1_nn_2([r, h, obs_R]) \n",
    "                y_1 = pyro.sample(\"y_1_{}\".format(n), dist.Normal(y_1_loc, y_1_std))\n",
    "                y_2_loc, y_2_std = self.y1_y2_nn_2([y_1, r, h]) \n",
    "                y_2 = pyro.sample(\"y_2_{}\".format(n), dist.Normal(y_2_loc, y_2_std))\n",
    "                x_loc, x_std = self.y_x_nn_2([y_2, r, h])\n",
    "                next_x = pyro.sample(\"next_x_{}\".format(n), dist.Normal(x_loc, x_std))\n",
    "                \n",
    "                h = self.h_encoder_2([h, r])\n",
    "                return rec_guide(next_x, n - 1, h, obs_R) + r\n",
    "    \n",
    "        rec_guide(x0, l, self.h0_2, obs_R)\n",
    "        \n",
    "    # guide uses simple and individual NN for each random variable, no obs_R\n",
    "    def guide_3(self, x0, l, obs_R):\n",
    "        pyro.module(\"model\", self)\n",
    "        def rec_guide(x, n, h):\n",
    "            r = self.g(x)\n",
    "            if n <= 0:\n",
    "                return r\n",
    "            else:\n",
    "                y_1_loc, y_1_std = self.x_y1_nn_3([r, h]) \n",
    "                y_1 = pyro.sample(\"y_1_{}\".format(n), dist.Normal(y_1_loc, y_1_std))\n",
    "                y_2_loc, y_2_std = self.y1_y2_nn_3([y_1, r, h])\n",
    "                y_2 = pyro.sample(\"y_2_{}\".format(n), dist.Normal(y_2_loc, y_2_std))\n",
    "                x_loc, x_std = self.y_x_nn_3([y_2, r, h])\n",
    "                next_x = pyro.sample(\"next_x_{}\".format(n), dist.Normal(x_loc, x_std))\n",
    "                \n",
    "                h = self.h_encoder_3([h, r])\n",
    "                return rec_guide(next_x, n - 1, h) + r\n",
    "    \n",
    "        rec_guide(x0, l, self.h0_3)\n",
    "    \n",
    "    # using made to replace all individual NNs, input h, r;\n",
    "    # output[i] == ith random variable mean, output[i + input_dim] == ith random variable std\n",
    "    def guide_made_0(self, x0, l, obs_R):\n",
    "        pyro.module(\"model\", self)\n",
    "        def concat_input_made(x_list):\n",
    "            for i in range(len(x_list)):\n",
    "                if x_list[i].dim() == 0:\n",
    "                    x_list[i] = torch.unsqueeze(x_list[i], dim=0)\n",
    "            input_x = torch.cat(x_list)\n",
    "            return input_x\n",
    "        \n",
    "        def rec_guide(x, n, h, obs_R):\n",
    "            r = self.g(x)\n",
    "            if n <= 0:\n",
    "                return r\n",
    "            else:\n",
    "                var_names = [\"y_1_{}\".format(n), \"y_2_{}\".format(n), \"next_x_{}\".format(n)]\n",
    "                input_x = concat_input_made([r, h])\n",
    "                made_out = self.made_0(input_x)\n",
    "                for i in range(self.num_var_0):\n",
    "                    loc = made_out[i]\n",
    "                    std = self.softplus(made_out[i + self.made_in_dim_0])\n",
    "                    var = pyro.sample(var_names[i], dist.Normal(loc, std))\n",
    "                \n",
    "                h = self.h_encoder_made_0([h, r])\n",
    "                return rec_guide(var, n - 1, h, obs_R) + r\n",
    "    \n",
    "        rec_guide(x0, l, self.h0_made_0, obs_R)\n",
    "        \n",
    "    # using made to replace all individual NNs, input r, h, obs_R, n, output dim input_dim * num_var(3)\n",
    "    # use output mapping layers to output mean and std for ith random variable, particularly:\n",
    "    # made_out[i * input_dim : (i+ 1) * input_dim] ===> mean, std ; each random var has its own output mapping layers\n",
    "    def guide_made_1(self, x0, l, obs_R):\n",
    "        pyro.module(\"model\", self)\n",
    "        def concat_input_made(x_list):\n",
    "            for i in range(len(x_list)):\n",
    "                if x_list[i].dim() == 0:\n",
    "                    x_list[i] = torch.unsqueeze(x_list[i], dim=0)\n",
    "            input_x = torch.cat(x_list)\n",
    "            return input_x\n",
    "        \n",
    "        def rec_guide(x, n, h, obs_R):\n",
    "            r = self.g(x)\n",
    "            if n <= 0:\n",
    "                return r\n",
    "            else:\n",
    "                var_names = [\"y_1_{}\".format(n), \"y_2_{}\".format(n), \"next_x_{}\".format(n)]\n",
    "                input_x = concat_input_made([r, h, obs_R, n])\n",
    "                made_out = self.made_1(input_x)\n",
    "                for i in range(self.num_var_1):\n",
    "                    hid = made_out[i * self.made_in_dim_1 : (i+1) * self.made_in_dim_1]\n",
    "                    loc = self.made_out_loc_linear_1[i](hid)\n",
    "                    std = self.softplus(self.made_out_std_linear_1[i](hid))\n",
    "                    var = pyro.sample(var_names[i], dist.Normal(loc, std))\n",
    "                \n",
    "                h = self.h_encoder_made_1([h, r])\n",
    "                return rec_guide(var, n - 1, h, obs_R) + r\n",
    "    \n",
    "        rec_guide(x0, l, self.h0_made_1, obs_R)\n",
    "        \n",
    "    # using made to replace all individual NNs, input r, h, obs_R, n;\n",
    "    # map all inputs to n hidden states, where n == num_var in this basic block\n",
    "    # output[i] == ith random variable mean, output[i + input_dim] == ith random variable std\n",
    "    def guide_made_2(self, x0, l, obs_R):\n",
    "        pyro.module(\"model\", self)\n",
    "\n",
    "        def rec_guide(x, n, h, obs_R):\n",
    "            r = self.g(x)\n",
    "            if n <= 0:\n",
    "                return r\n",
    "            else:\n",
    "                var_names = [\"y_1_{}\".format(n), \"y_2_{}\".format(n), \"next_x_{}\".format(n)]\n",
    "                input_x = self.input_encoder_made_2([r, h, obs_R, n])\n",
    "                made_out = self.made_2(input_x)\n",
    "                for i in range(self.num_var_2):\n",
    "                    loc = made_out[i]\n",
    "                    std = self.softplus(made_out[i + self.made_in_dim_2])\n",
    "                    var = pyro.sample(var_names[i], dist.Normal(loc, std))\n",
    "                \n",
    "                h = self.h_encoder_made_2([h, r])\n",
    "                return rec_guide(var, n - 1, h, obs_R) + r\n",
    "    \n",
    "        rec_guide(x0, l, self.h0_made_2, obs_R)\n",
    "\n",
    "    # using made to replace all individual NNs, input r, h, obs_R, n, output dim input_dim * num_var(3)\n",
    "    # use output mapping layers to output mean and std for ith random variable, particularly:\n",
    "    # made_out[i * input_dim : (i+ 1) * input_dim] ===> mean, std ; random variables share output mapping layers\n",
    "    def guide_made_3(self, x0, l, obs_R):\n",
    "        pyro.module(\"model\", self)\n",
    "        def concat_input_made(x_list):\n",
    "            for i in range(len(x_list)):\n",
    "                if x_list[i].dim() == 0:\n",
    "                    x_list[i] = torch.unsqueeze(x_list[i], dim=0)\n",
    "            input_x = torch.cat(x_list)\n",
    "            return input_x\n",
    "        \n",
    "        def rec_guide(x, n, h, obs_R):\n",
    "            r = self.g(x)\n",
    "            if n <= 0:\n",
    "                return r\n",
    "            else:\n",
    "                var_names = [\"y_1_{}\".format(n), \"y_2_{}\".format(n), \"next_x_{}\".format(n)]\n",
    "                input_x = concat_input_made([r, h, obs_R, n])\n",
    "                made_out = self.made_3(input_x)\n",
    "                for i in range(self.num_var_3):\n",
    "                    hid = made_out[i * self.made_in_dim_3 : (i+1) * self.made_in_dim_3]\n",
    "                    loc = self.made_out_loc_linear_3(hid)\n",
    "                    std = self.softplus(self.made_out_std_linear_3(hid))\n",
    "                    var = pyro.sample(var_names[i], dist.Normal(loc, std))\n",
    "                \n",
    "                h = self.h_encoder_made_3([h, r])\n",
    "                return rec_guide(var, n - 1, h, obs_R) + r\n",
    "    \n",
    "        rec_guide(x0, l, self.h0_made_3, obs_R)\n",
    "    \n",
    "    # guide uses a RNN to estimate distributions of all random variables\n",
    "    def guide_rnn(self, x0, l, obs_R):\n",
    "        pyro.module(\"model\", self)\n",
    "        t = 3\n",
    "        out_loc, out_std = self.simrnn(x0, obs_R, l, t)\n",
    "        \n",
    "        for i in range(int(l)):\n",
    "            pyro.sample(\"y_1_{}\".format(l - i), dist.Normal(out_loc[i * t], out_std[i * t])) # maybe i here?\n",
    "            pyro.sample(\"y_2_{}\".format(l - i), dist.Normal(out_loc[i * t + 1], out_std[i * t + 1])) # maybe i here?\n",
    "            pyro.sample(\"next_x_{}\".format(l - i), dist.Normal(out_loc[i * t + 2], out_std[i * t + 2])) # maybe i here?\n",
    "        \n",
    "def generate_data():\n",
    "    # the actual data generation has three latent variables (y_1, y_2, y_3)\n",
    "    x0 = torch.tensor(random.random())\n",
    "    base_std = 0.6\n",
    "    l = torch.tensor(random.randint(2, 5))\n",
    "    R = 0\n",
    "    x = x0\n",
    "    for i in range(l):\n",
    "        # standard deviation is decreasing\n",
    "        std = base_std - i * 0.1\n",
    "        R += torch.tanh(x)\n",
    "        \n",
    "        y_1 = dist.Normal(x, std).sample()\n",
    "        # add some noise\n",
    "        noise1 = random.random() / 5\n",
    "        \n",
    "        y_2 = dist.Normal(y_1 + noise1, std).sample()\n",
    "        # add some noise\n",
    "        noise2 = random.random() / 5\n",
    "    \n",
    "        y_3 = dist.Normal(y_2 + noise2, std).sample()\n",
    "        # add some noise\n",
    "        noise3 = random.random() / 5\n",
    "        \n",
    "        x = dist.Normal(y_3 + noise3, std).sample()\n",
    "    return x0.float(), l.float(), R.float()\n",
    "\n",
    "data = []\n",
    "num_data = 100\n",
    "for _ in range(num_data):\n",
    "    data.append(generate_data())\n",
    "\n",
    "experiment = Experiment()\n",
    "adam_params = {\"lr\": 0.001, \"betas\": (0.95, 0.999)}\n",
    "optimizer = Adam(adam_params)\n",
    "guide = experiment.guide_gmade # guide_1\n",
    "\n",
    "#guide = AutoNormal(experiment.model)\n",
    "#guide = AutoMultivariateNormal(experiment.model)\n",
    "\n",
    "svi = SVI(experiment.model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "\n",
    "n_steps = 400\n",
    "log_interval = 10\n",
    "# do gradient steps\n",
    "loss = 0\n",
    "loss_track = []\n",
    "for step in range(1, n_steps + 1):\n",
    "    imme_loss = 0\n",
    "    \n",
    "    for x0, l, R in data:\n",
    "        imme_loss += svi.step(x0, l, R) / num_data\n",
    "        \n",
    "    loss_track.append(imme_loss)\n",
    "    loss += imme_loss / log_interval\n",
    "    \n",
    "    if step % log_interval == 0:\n",
    "        print(\"[Step {}/{}] Immediate Loss: {} Accumlated Loss: {}\".format(step, n_steps, imme_loss, loss))\n",
    "        loss = 0\n",
    "    \n",
    "plt.plot(loss_track)\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-tournament",
   "metadata": {},
   "source": [
    "results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dirty-edmonton",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-67798cbaab7d>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-67798cbaab7d>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    [Step 10/200] Immediate Loss: 10.897096427083019 Accumlated Loss: 24.07399604648352\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# guide 0\n",
    "[Step 10/200] Immediate Loss: 10.897096427083019 Accumlated Loss: 24.07399604648352\n",
    "[Step 20/200] Immediate Loss: 6.586585423052311 Accumlated Loss: 8.56087434694171\n",
    "[Step 30/200] Immediate Loss: 5.427656478881836 Accumlated Loss: 6.286525824904442\n",
    "[Step 40/200] Immediate Loss: 5.76791431516409 Accumlated Loss: 5.656579471707344\n",
    "[Step 50/200] Immediate Loss: 4.822795096933843 Accumlated Loss: 4.815895015358924\n",
    "[Step 60/200] Immediate Loss: 4.40746598005295 Accumlated Loss: 4.366727933973074\n",
    "[Step 70/200] Immediate Loss: 4.060884852707386 Accumlated Loss: 4.237406772494316\n",
    "[Step 80/200] Immediate Loss: 3.2891077983379367 Accumlated Loss: 3.902767575621605\n",
    "[Step 90/200] Immediate Loss: 3.7756632891297346 Accumlated Loss: 3.7543974510431286\n",
    "[Step 100/200] Immediate Loss: 3.297055274844171 Accumlated Loss: 3.682017157822847\n",
    "[Step 110/200] Immediate Loss: 3.399244492650032 Accumlated Loss: 3.419619444400072\n",
    "[Step 120/200] Immediate Loss: 3.140221614539626 Accumlated Loss: 3.3451627815067773\n",
    "[Step 130/200] Immediate Loss: 3.3548742634057995 Accumlated Loss: 3.2445356895625594\n",
    "[Step 140/200] Immediate Loss: 3.097808829844 Accumlated Loss: 3.218830179899931\n",
    "[Step 150/200] Immediate Loss: 2.974420528709889 Accumlated Loss: 3.0854880858659746\n",
    "[Step 160/200] Immediate Loss: 3.027468394041062 Accumlated Loss: 3.081009613126516\n",
    "[Step 170/200] Immediate Loss: 3.1476346796751016 Accumlated Loss: 2.9707428410947325\n",
    "[Step 180/200] Immediate Loss: 3.1779525178670878 Accumlated Loss: 3.0013780316114427\n",
    "[Step 190/200] Immediate Loss: 3.038894810378551 Accumlated Loss: 3.060519110441208\n",
    "[Step 200/200] Immediate Loss: 2.84645428687334 Accumlated Loss: 2.885463625282049"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-source",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guide_g_made\n",
    "[Step 10/400] Immediate Loss: 10.093876553177838 Accumlated Loss: 28.116918125301602\n",
    "[Step 20/400] Immediate Loss: 5.735325578451158 Accumlated Loss: 7.710717597097159\n",
    "[Step 30/400] Immediate Loss: 4.141437416076659 Accumlated Loss: 5.249956729501486\n",
    "[Step 40/400] Immediate Loss: 4.192413958907127 Accumlated Loss: 5.026298224061727\n",
    "[Step 50/400] Immediate Loss: 4.032615008652211 Accumlated Loss: 4.37625692051649\n",
    "[Step 60/400] Immediate Loss: 3.7924616149067867 Accumlated Loss: 4.056113148778677\n",
    "[Step 70/400] Immediate Loss: 3.4080429872870455 Accumlated Loss: 3.938382657647133\n",
    "[Step 80/400] Immediate Loss: 3.91588868767023 Accumlated Loss: 3.8055605174005023\n",
    "[Step 90/400] Immediate Loss: 3.562930800914765 Accumlated Loss: 3.679675492316484\n",
    "[Step 100/400] Immediate Loss: 3.166920450031757 Accumlated Loss: 3.4407147874832154\n",
    "[Step 110/400] Immediate Loss: 3.3585270166397088 Accumlated Loss: 3.358105143129826\n",
    "[Step 120/400] Immediate Loss: 3.4754280599951755 Accumlated Loss: 3.310290638148785\n",
    "[Step 130/400] Immediate Loss: 3.7460891202092146 Accumlated Loss: 3.3028773609697817\n",
    "[Step 140/400] Immediate Loss: 3.0938018533587455 Accumlated Loss: 3.2975916117429733\n",
    "[Step 150/400] Immediate Loss: 3.1185010880231845 Accumlated Loss: 3.220666427582502\n",
    "[Step 160/400] Immediate Loss: 3.303586041331289 Accumlated Loss: 3.1039333658218378\n",
    "[Step 170/400] Immediate Loss: 2.8570777693390843 Accumlated Loss: 3.092048371642828\n",
    "[Step 180/400] Immediate Loss: 3.1323183783888813 Accumlated Loss: 2.9743766522705553\n",
    "[Step 190/400] Immediate Loss: 3.123101497292519 Accumlated Loss: 3.0700586830675602\n",
    "[Step 200/400] Immediate Loss: 2.8325696131587033 Accumlated Loss: 3.0600864307284357\n",
    "[Step 210/400] Immediate Loss: 3.203767530620098 Accumlated Loss: 2.957834710299969\n",
    "[Step 220/400] Immediate Loss: 3.0082357767224295 Accumlated Loss: 2.8931357885897158\n",
    "[Step 230/400] Immediate Loss: 2.871949333548545 Accumlated Loss: 2.8120275142788884\n",
    "[Step 240/400] Immediate Loss: 2.770469197630882 Accumlated Loss: 2.7996084527373313\n",
    "[Step 250/400] Immediate Loss: 2.776975691318512 Accumlated Loss: 2.781975454688072\n",
    "[Step 260/400] Immediate Loss: 2.6096868035197263 Accumlated Loss: 2.736122457921505\n",
    "[Step 270/400] Immediate Loss: 2.834006843566894 Accumlated Loss: 2.80602259850502\n",
    "[Step 280/400] Immediate Loss: 2.7753792360425003 Accumlated Loss: 2.663630239576101\n",
    "[Step 290/400] Immediate Loss: 2.6723285016417506 Accumlated Loss: 2.7857212778031823\n",
    "[Step 300/400] Immediate Loss: 2.6797246611118317 Accumlated Loss: 2.7780214585959917\n",
    "[Step 310/400] Immediate Loss: 2.743236700296403 Accumlated Loss: 2.8762407574951654\n",
    "[Step 320/400] Immediate Loss: 2.9069129949808135 Accumlated Loss: 2.730512857496738\n",
    "[Step 330/400] Immediate Loss: 2.5359984785318375 Accumlated Loss: 2.706855779349804\n",
    "[Step 340/400] Immediate Loss: 2.8012781810760488 Accumlated Loss: 2.6973362797200684\n",
    "[Step 350/400] Immediate Loss: 2.5512332478165627 Accumlated Loss: 2.7136866383254525\n",
    "[Step 360/400] Immediate Loss: 2.987985840439795 Accumlated Loss: 2.7060461809039116\n",
    "[Step 370/400] Immediate Loss: 2.8415988487005244 Accumlated Loss: 2.8244434046149256\n",
    "[Step 380/400] Immediate Loss: 2.663631010353566 Accumlated Loss: 2.5774431353211398\n",
    "[Step 390/400] Immediate Loss: 2.791434529423713 Accumlated Loss: 2.6088461354076866\n",
    "[Step 400/400] Immediate Loss: 2.85814619064331 Accumlated Loss: 2.6991474525332446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-arkansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guide 1\n",
    "[Step 10/200] Immediate Loss: 7.559717512130738 Accumlated Loss: 18.33279416501522\n",
    "[Step 20/200] Immediate Loss: 6.153506239354612 Accumlated Loss: 7.117524638921023\n",
    "[Step 30/200] Immediate Loss: 5.541966640651227 Accumlated Loss: 5.684932113975287\n",
    "[Step 40/200] Immediate Loss: 5.148693958520891 Accumlated Loss: 5.38080223414302\n",
    "[Step 50/200] Immediate Loss: 4.858222780823709 Accumlated Loss: 5.372910097420216\n",
    "[Step 60/200] Immediate Loss: 4.3179991856217415 Accumlated Loss: 5.156085780709981\n",
    "[Step 70/200] Immediate Loss: 5.332767331004142 Accumlated Loss: 5.199428522855043\n",
    "[Step 80/200] Immediate Loss: 4.723022387623786 Accumlated Loss: 4.88315895318985\n",
    "[Step 90/200] Immediate Loss: 4.767818248569966 Accumlated Loss: 4.737200892984867\n",
    "[Step 100/200] Immediate Loss: 4.6624362275004385 Accumlated Loss: 4.830253395944834\n",
    "[Step 110/200] Immediate Loss: 4.3004247260093695 Accumlated Loss: 4.749406197190285\n",
    "[Step 120/200] Immediate Loss: 4.410183604061603 Accumlated Loss: 4.58312035241723\n",
    "[Step 130/200] Immediate Loss: 4.632457412481308 Accumlated Loss: 4.5502141160368925\n",
    "[Step 140/200] Immediate Loss: 4.070656002163886 Accumlated Loss: 4.303979308575391\n",
    "[Step 150/200] Immediate Loss: 4.219085508882999 Accumlated Loss: 4.468279556453227\n",
    "[Step 160/200] Immediate Loss: 4.482489324808121 Accumlated Loss: 4.24668676546216\n",
    "[Step 170/200] Immediate Loss: 4.571949941813946 Accumlated Loss: 4.236999658554792\n",
    "[Step 180/200] Immediate Loss: 3.8510677894949903 Accumlated Loss: 4.2028233243823045\n",
    "[Step 190/200] Immediate Loss: 4.390659238100055 Accumlated Loss: 4.3577096745073804\n",
    "[Step 200/200] Immediate Loss: 3.561375828087331 Accumlated Loss: 4.068374880343676"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guide 2\n",
    "[Step 10/200] Immediate Loss: 36.34582948565483 Accumlated Loss: 102.13199270904065\n",
    "[Step 20/200] Immediate Loss: 24.573131604790692 Accumlated Loss: 27.80764180260897\n",
    "[Step 30/200] Immediate Loss: 22.30637564986943 Accumlated Loss: 23.54905093795061\n",
    "[Step 40/200] Immediate Loss: 20.33765312969684 Accumlated Loss: 21.59236597812176\n",
    "[Step 50/200] Immediate Loss: 19.630492952466017 Accumlated Loss: 20.015027120113373\n",
    "[Step 60/200] Immediate Loss: 17.475191622376446 Accumlated Loss: 19.02552970287204\n",
    "[Step 70/200] Immediate Loss: 18.147958450019356 Accumlated Loss: 17.944058370858432\n",
    "[Step 80/200] Immediate Loss: 17.768492654562 Accumlated Loss: 17.357401265323166\n",
    "[Step 90/200] Immediate Loss: 17.098091254830358 Accumlated Loss: 17.070186663359404\n",
    "[Step 100/200] Immediate Loss: 16.87052688539028 Accumlated Loss: 17.304257960915567\n",
    "[Step 110/200] Immediate Loss: 17.376896535158156 Accumlated Loss: 17.405616234004498\n",
    "[Step 120/200] Immediate Loss: 15.72870985478163 Accumlated Loss: 16.184437985241416\n",
    "[Step 130/200] Immediate Loss: 16.505062356293205 Accumlated Loss: 16.479880607217552\n",
    "[Step 140/200] Immediate Loss: 14.654706366658216 Accumlated Loss: 16.586361252725123\n",
    "[Step 150/200] Immediate Loss: 17.06694069772959 Accumlated Loss: 16.37410228174925\n",
    "[Step 160/200] Immediate Loss: 16.8475823968649 Accumlated Loss: 16.51549323582649\n",
    "[Step 170/200] Immediate Loss: 16.110470311045646 Accumlated Loss: 16.22321096855402\n",
    "[Step 180/200] Immediate Loss: 18.03824138969183 Accumlated Loss: 16.213583804011346\n",
    "[Step 190/200] Immediate Loss: 16.448858000338078 Accumlated Loss: 16.252151167631148\n",
    "[Step 200/200] Immediate Loss: 16.104524869918823 Accumlated Loss: 16.338763901770115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guide 3\n",
    "[Step 10/200] Immediate Loss: 129.39596097201107 Accumlated Loss: 150.33133329963687\n",
    "[Step 20/200] Immediate Loss: 128.21112619131807 Accumlated Loss: 132.53726438537242\n",
    "[Step 30/200] Immediate Loss: 117.83395487815142 Accumlated Loss: 120.01952632072569\n",
    "[Step 40/200] Immediate Loss: 116.78343730509283 Accumlated Loss: 118.54713125389816\n",
    "[Step 50/200] Immediate Loss: 116.25210932254794 Accumlated Loss: 115.84209503382444\n",
    "[Step 60/200] Immediate Loss: 117.45410153686997 Accumlated Loss: 115.28587766537069\n",
    "[Step 70/200] Immediate Loss: 112.48804287433627 Accumlated Loss: 116.32100198805334\n",
    "[Step 80/200] Immediate Loss: 112.87405549257987 Accumlated Loss: 114.50239012616872\n",
    "[Step 90/200] Immediate Loss: 112.65670664697886 Accumlated Loss: 113.1228032810688\n",
    "[Step 100/200] Immediate Loss: 113.96976754337548 Accumlated Loss: 114.07826335078478\n",
    "[Step 110/200] Immediate Loss: 119.35383942961693 Accumlated Loss: 113.20602866864202\n",
    "[Step 120/200] Immediate Loss: 108.93617531657218 Accumlated Loss: 114.54492359772323\n",
    "[Step 130/200] Immediate Loss: 111.59455785870557 Accumlated Loss: 113.15705430191757\n",
    "[Step 140/200] Immediate Loss: 111.8253447008132 Accumlated Loss: 113.49238991543649\n",
    "[Step 150/200] Immediate Loss: 115.3344919478893 Accumlated Loss: 111.89611885094641\n",
    "[Step 160/200] Immediate Loss: 115.06518815487618 Accumlated Loss: 114.72096209207179\n",
    "[Step 170/200] Immediate Loss: 111.97390507310628 Accumlated Loss: 113.52600042599438\n",
    "[Step 180/200] Immediate Loss: 110.2163209789991 Accumlated Loss: 112.27746578404307\n",
    "[Step 190/200] Immediate Loss: 112.16051963776347 Accumlated Loss: 113.08217988958955\n",
    "[Step 200/200] Immediate Loss: 109.15450765848156 Accumlated Loss: 112.25836501994726"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-massage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guide rnn\n",
    "[Step 10/200] Immediate Loss: 9.640962693095211 Accumlated Loss: 30.67832644885778\n",
    "[Step 20/200] Immediate Loss: 8.272537976801395 Accumlated Loss: 9.594280518233777\n",
    "[Step 30/200] Immediate Loss: 8.173096621632574 Accumlated Loss: 8.33986207279563\n",
    "[Step 40/200] Immediate Loss: 8.519405342340468 Accumlated Loss: 7.8515220711529246\n",
    "[Step 50/200] Immediate Loss: 7.4075272080302215 Accumlated Loss: 7.428417345315219\n",
    "[Step 60/200] Immediate Loss: 7.175404669940471 Accumlated Loss: 7.056655042648316\n",
    "[Step 70/200] Immediate Loss: 6.88103425234556 Accumlated Loss: 7.183161941289901\n",
    "[Step 80/200] Immediate Loss: 7.21724983394146 Accumlated Loss: 6.844631317824125\n",
    "[Step 90/200] Immediate Loss: 6.871469007134438 Accumlated Loss: 7.018842077046632\n",
    "[Step 100/200] Immediate Loss: 6.306239696443082 Accumlated Loss: 6.959406811505557\n",
    "[Step 110/200] Immediate Loss: 7.416387225091456 Accumlated Loss: 6.738025695741176\n",
    "[Step 120/200] Immediate Loss: 6.731718974411488 Accumlated Loss: 7.003545281022788\n",
    "[Step 130/200] Immediate Loss: 6.534330331981184 Accumlated Loss: 6.728503110408784\n",
    "[Step 140/200] Immediate Loss: 6.389326887428762 Accumlated Loss: 6.637771991342307\n",
    "[Step 150/200] Immediate Loss: 6.565908052623272 Accumlated Loss: 6.575905843764544\n",
    "[Step 160/200] Immediate Loss: 6.7690025347471225 Accumlated Loss: 6.682427942067384\n",
    "[Step 170/200] Immediate Loss: 6.35611254155636 Accumlated Loss: 6.5264977272748945\n",
    "[Step 180/200] Immediate Loss: 6.129716929793357 Accumlated Loss: 6.537472707509994\n",
    "[Step 190/200] Immediate Loss: 6.21984536796808 Accumlated Loss: 6.584824181616307\n",
    "[Step 200/200] Immediate Loss: 6.228714998960494 Accumlated Loss: 6.4946992860734465"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guide made 0\n",
    "[Step 10/200] Immediate Loss: 160.48423355847603 Accumlated Loss: 182.85270177999138\n",
    "[Step 20/200] Immediate Loss: 136.2737018719316 Accumlated Loss: 149.25851898822188\n",
    "[Step 30/200] Immediate Loss: 139.77632999777796 Accumlated Loss: 143.84995370826127\n",
    "[Step 40/200] Immediate Loss: 133.49804007470604 Accumlated Loss: 139.32783275571464\n",
    "[Step 50/200] Immediate Loss: 136.53222409278152 Accumlated Loss: 139.32025609317424\n",
    "[Step 60/200] Immediate Loss: 141.1230897080898 Accumlated Loss: 136.13689444220066\n",
    "[Step 70/200] Immediate Loss: 135.14042346447707 Accumlated Loss: 134.6072166762948\n",
    "[Step 80/200] Immediate Loss: 140.2574947077036 Accumlated Loss: 134.44344882920382\n",
    "[Step 90/200] Immediate Loss: 133.19639854818587 Accumlated Loss: 132.08353916981815\n",
    "[Step 100/200] Immediate Loss: 125.4555533385277 Accumlated Loss: 130.69213940227033\n",
    "[Step 110/200] Immediate Loss: 139.7100527447462 Accumlated Loss: 132.99286480870845\n",
    "[Step 120/200] Immediate Loss: 132.32316820651295 Accumlated Loss: 134.52053939491512\n",
    "[Step 130/200] Immediate Loss: 128.39376810431477 Accumlated Loss: 132.10056481549145\n",
    "[Step 140/200] Immediate Loss: 129.62434692651033 Accumlated Loss: 132.78218272250888\n",
    "[Step 150/200] Immediate Loss: 126.35322604447606 Accumlated Loss: 133.0661242879331\n",
    "[Step 160/200] Immediate Loss: 134.50700048357257 Accumlated Loss: 132.68880042499308\n",
    "[Step 170/200] Immediate Loss: 125.88842075854538 Accumlated Loss: 133.48948980358242\n",
    "[Step 180/200] Immediate Loss: 128.79157200038426 Accumlated Loss: 131.80554548048974\n",
    "[Step 190/200] Immediate Loss: 134.20808095455172 Accumlated Loss: 132.16203881713747\n",
    "[Step 200/200] Immediate Loss: 129.1014901822805 Accumlated Loss: 131.55340434998274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guide made 1\n",
    "NaturalOrdering=False\n",
    "[Step 10/200] Immediate Loss: 14.019104228317742 Accumlated Loss: 53.64432947850229\n",
    "[Step 20/200] Immediate Loss: 9.699863998293878 Accumlated Loss: 11.032929550886156\n",
    "[Step 30/200] Immediate Loss: 7.148877637684345 Accumlated Loss: 8.264739600002766\n",
    "[Step 40/200] Immediate Loss: 7.238150359988211 Accumlated Loss: 6.981379968196154\n",
    "[Step 50/200] Immediate Loss: 6.0434359559416775 Accumlated Loss: 6.643292198151348\n",
    "[Step 60/200] Immediate Loss: 6.070013944506647 Accumlated Loss: 6.357983798265456\n",
    "[Step 70/200] Immediate Loss: 6.789834988117221 Accumlated Loss: 6.619088857740164\n",
    "[Step 80/200] Immediate Loss: 6.323514266908169 Accumlated Loss: 6.1634104528427125\n",
    "[Step 90/200] Immediate Loss: 6.665135125517846 Accumlated Loss: 6.185240255266428\n",
    "[Step 100/200] Immediate Loss: 6.078257759213448 Accumlated Loss: 6.409523689031601\n",
    "[Step 110/200] Immediate Loss: 6.695484466254712 Accumlated Loss: 6.648152809649706\n",
    "[Step 120/200] Immediate Loss: 5.736008805036541 Accumlated Loss: 5.941689832746982\n",
    "[Step 130/200] Immediate Loss: 6.426270787715911 Accumlated Loss: 5.963468790888786\n",
    "[Step 140/200] Immediate Loss: 5.827706450521947 Accumlated Loss: 5.8566648783683775\n",
    "[Step 150/200] Immediate Loss: 6.102512870728971 Accumlated Loss: 5.749399427473547\n",
    "[Step 160/200] Immediate Loss: 6.727250106334686 Accumlated Loss: 5.821995285272598\n",
    "[Step 170/200] Immediate Loss: 5.84557059943676 Accumlated Loss: 5.8491086800396435\n",
    "[Step 180/200] Immediate Loss: 5.364269751608374 Accumlated Loss: 5.701569260686635\n",
    "[Step 190/200] Immediate Loss: 6.074659641683102 Accumlated Loss: 5.867555259823797\n",
    "[Step 200/200] Immediate Loss: 5.423869117200372 Accumlated Loss: 5.6179648361206045\n",
    "        \n",
    "NaturalOrdering=True\n",
    "[Step 10/200] Immediate Loss: 16.073262527287007 Accumlated Loss: 100.58396736195684\n",
    "[Step 20/200] Immediate Loss: 8.754549311697483 Accumlated Loss: 10.962106433063745\n",
    "[Step 30/200] Immediate Loss: 7.634609906077385 Accumlated Loss: 8.177316520512104\n",
    "[Step 40/200] Immediate Loss: 6.4821303054690365 Accumlated Loss: 6.937638696998359\n",
    "[Step 50/200] Immediate Loss: 6.452378512024877 Accumlated Loss: 6.032111377894878\n",
    "[Step 60/200] Immediate Loss: 5.6379613992571835 Accumlated Loss: 5.908414689779281\n",
    "[Step 70/200] Immediate Loss: 5.733489564657215 Accumlated Loss: 5.464340224891902\n",
    "[Step 80/200] Immediate Loss: 5.209848814010621 Accumlated Loss: 5.376727032631636\n",
    "[Step 90/200] Immediate Loss: 5.956956093907354 Accumlated Loss: 5.560123661011456\n",
    "[Step 100/200] Immediate Loss: 5.223151791691779 Accumlated Loss: 5.326080230921507\n",
    "[Step 110/200] Immediate Loss: 4.814688653051852 Accumlated Loss: 5.205872291654348\n",
    "[Step 120/200] Immediate Loss: 6.60426853686571 Accumlated Loss: 5.5602356737554075\n",
    "[Step 130/200] Immediate Loss: 5.436120665967464 Accumlated Loss: 5.156234309822321\n",
    "[Step 140/200] Immediate Loss: 4.893183960914612 Accumlated Loss: 5.31840815603733\n",
    "[Step 150/200] Immediate Loss: 5.22995001167059 Accumlated Loss: 5.081223203510046\n",
    "[Step 160/200] Immediate Loss: 5.380219370126723 Accumlated Loss: 5.248564005374908\n",
    "[Step 170/200] Immediate Loss: 5.119603236019612 Accumlated Loss: 5.480683054149152\n",
    "[Step 180/200] Immediate Loss: 4.46067540407181 Accumlated Loss: 5.032706862658262\n",
    "[Step 190/200] Immediate Loss: 4.931696077585218 Accumlated Loss: 5.102018988072871\n",
    "[Step 200/200] Immediate Loss: 5.070164603292943 Accumlated Loss: 5.01442726829648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guide made 2\n",
    "[Step 10/200] Immediate Loss: 168.91788237839936 Accumlated Loss: 187.96732055580617\n",
    "[Step 20/200] Immediate Loss: 150.52404758065938 Accumlated Loss: 156.19228604671358\n",
    "[Step 30/200] Immediate Loss: 154.54525569885973 Accumlated Loss: 144.47615030825136\n",
    "[Step 40/200] Immediate Loss: 156.36513914287093 Accumlated Loss: 139.19961321538688\n",
    "[Step 50/200] Immediate Loss: 132.17285798609262 Accumlated Loss: 140.33948809969425\n",
    "[Step 60/200] Immediate Loss: 132.14123793751 Accumlated Loss: 136.13094830822945\n",
    "[Step 70/200] Immediate Loss: 133.3406092461944 Accumlated Loss: 137.81934999725223\n",
    "[Step 80/200] Immediate Loss: 134.44997272610664 Accumlated Loss: 133.3638133395314\n",
    "[Step 90/200] Immediate Loss: 132.55868464320898 Accumlated Loss: 134.1936486185789\n",
    "[Step 100/200] Immediate Loss: 130.04643192917106 Accumlated Loss: 134.29819335079193\n",
    "[Step 110/200] Immediate Loss: 143.4784502640367 Accumlated Loss: 135.69545271345973\n",
    "[Step 120/200] Immediate Loss: 131.35292657703167 Accumlated Loss: 133.35868326303367\n",
    "[Step 130/200] Immediate Loss: 127.99579790413381 Accumlated Loss: 133.96785312217474\n",
    "[Step 140/200] Immediate Loss: 135.98382707864045 Accumlated Loss: 132.0557901872396\n",
    "[Step 150/200] Immediate Loss: 135.17538750559083 Accumlated Loss: 134.44175980606673\n",
    "[Step 160/200] Immediate Loss: 138.24288726031784 Accumlated Loss: 132.41555623677374\n",
    "[Step 170/200] Immediate Loss: 129.82562937021257 Accumlated Loss: 134.04827706205845\n",
    "[Step 180/200] Immediate Loss: 130.17766936391587 Accumlated Loss: 133.5063454965353\n",
    "[Step 190/200] Immediate Loss: 132.20994168728592 Accumlated Loss: 130.9864975514114\n",
    "[Step 200/200] Immediate Loss: 131.49415226608517 Accumlated Loss: 131.03741551262144\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guide made 3\n",
    "[Step 10/200] Immediate Loss: 13.010656988024715 Accumlated Loss: 99.06157932758333\n",
    "[Step 20/200] Immediate Loss: 7.5046216446161305 Accumlated Loss: 10.03270694538951\n",
    "[Step 30/200] Immediate Loss: 6.117533451020719 Accumlated Loss: 6.660886912047863\n",
    "[Step 40/200] Immediate Loss: 6.316120563745499 Accumlated Loss: 5.829992985129356\n",
    "[Step 50/200] Immediate Loss: 5.437958541512489 Accumlated Loss: 5.892800804883243\n",
    "[Step 60/200] Immediate Loss: 5.355252393484118 Accumlated Loss: 5.788893213301898\n",
    "[Step 70/200] Immediate Loss: 5.657892731130121 Accumlated Loss: 5.508381419748067\n",
    "[Step 80/200] Immediate Loss: 5.633600185513498 Accumlated Loss: 5.513742947548628\n",
    "[Step 90/200] Immediate Loss: 5.984356532394888 Accumlated Loss: 5.526806058973074\n",
    "[Step 100/200] Immediate Loss: 5.155731140077111 Accumlated Loss: 5.439844154447316\n",
    "[Step 110/200] Immediate Loss: 5.017589632570741 Accumlated Loss: 5.296441371321677\n",
    "[Step 120/200] Immediate Loss: 5.336543061435223 Accumlated Loss: 5.129204294204713\n",
    "[Step 130/200] Immediate Loss: 5.345724000930784 Accumlated Loss: 5.303808857530355\n",
    "[Step 140/200] Immediate Loss: 5.036817224025729 Accumlated Loss: 5.107035076290369\n",
    "[Step 150/200] Immediate Loss: 5.201788758933546 Accumlated Loss: 5.104901222437621\n",
    "[Step 160/200] Immediate Loss: 4.345631600022315 Accumlated Loss: 5.151430988103152\n",
    "[Step 170/200] Immediate Loss: 5.063906903862953 Accumlated Loss: 4.814677561849355\n",
    "[Step 180/200] Immediate Loss: 4.657406595349311 Accumlated Loss: 4.85194604074955\n",
    "[Step 190/200] Immediate Loss: 4.788816937506198 Accumlated Loss: 5.085158434122801\n",
    "[Step 200/200] Immediate Loss: 4.795215706825257 Accumlated Loss: 5.116604125469922"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MADE(3, [32, 32], 7, True)\n",
    "x = torch.rand(3)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-geology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-genome",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-snowboard",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-helen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-father",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-sixth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-athens",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-essay",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
