{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "secret-episode",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hid_orderings [0, 0, 0, 0, 0, 0, 0, 0] size 8\n",
      "num hid layers: 1\n",
      "expanded_input_ordering [0] size 1\n",
      "expanded_output_ordering [0, 0] size 2\n",
      "hid_orderings [2, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 2, 0, 0, 2] size 16\n",
      "num hid layers: 1\n",
      "expanded_input_ordering [0, 1] size 2\n",
      "expanded_output_ordering [0, 0, 2, 2] size 4\n",
      "hid_orderings [0, 6, 4, 1, 5, 5, 2, 5, 1, 4, 3, 5, 2, 2, 5, 3, 4, 4, 4, 6, 3, 4, 2, 0, 5, 1, 5, 6, 4, 1, 2, 2] size 32\n",
      "num hid layers: 1\n",
      "expanded_input_ordering [0, 1, 2] size 3\n",
      "expanded_output_ordering [2, 2, 1, 1, 3, 3] size 6\n",
      "hid_orderings [2, 14, 17, 10, 5, 2, 19, 0, 10, 7, 16, 22, 28, 19, 30, 29, 19, 7, 14, 5, 30, 13, 23, 1, 4, 5, 7, 10, 8, 13, 13, 3, 1, 11, 2, 13, 0, 16, 2, 21, 12, 30, 8, 14, 23, 10, 10, 19, 30, 27, 12, 22, 21, 11, 7, 2, 27, 1, 16, 16, 4, 20, 28, 24, 12, 3, 30, 16, 19, 24, 11, 1, 4, 20, 10, 27, 22, 30, 20, 9, 24, 0, 16, 27, 15, 4, 15, 16, 11, 27, 10, 8, 15, 6, 9, 12, 29, 2, 6, 6, 30, 27, 6, 17, 2, 9, 10, 16, 28, 25, 16, 24, 15, 22, 24, 7, 25, 17, 15, 12, 6, 15, 3, 19, 27, 24, 18, 15] size 128\n",
      "num hid layers: 1\n",
      "expanded_input_ordering [0, 1, 2, 3, 4] size 5\n",
      "expanded_output_ordering [4, 4, 0, 0, 10, 10, 6, 6] size 8\n",
      "hid_orderings [21, 3, 1, 26, 11, 20, 13, 11, 23, 10, 23, 25, 5, 30, 1, 12, 15, 25, 5, 17, 11, 28, 3, 8, 25, 9, 29, 1, 12, 3, 17, 29, 10, 20, 8, 13, 17, 7, 21, 15, 28, 8, 29, 5, 0, 20, 22, 14, 1, 30, 4, 26, 27, 6, 26, 14, 27, 27, 24, 8, 17, 30, 9, 11, 21, 1, 7, 5, 12, 13, 18, 15, 5, 1, 16, 23, 28, 26, 9, 26, 14, 8, 2, 13, 16, 8, 6, 29, 2, 29, 12, 24, 3, 17, 23, 13, 11, 15, 6, 6, 7, 1, 28, 2, 28, 0, 20, 16, 1, 29, 22, 27, 13, 9, 5, 3, 20, 25, 6, 0, 11, 4, 6, 6, 20, 13, 17, 12] size 128\n",
      "num hid layers: 1\n",
      "expanded_input_ordering [0, 1, 2, 3, 4] size 5\n",
      "expanded_output_ordering [10, 10, 4, 4, 6, 6] size 6\n",
      "hid_orderings [2, 1, 0, 0, 2, 2, 0, 1, 2, 1, 0, 1, 0, 0, 2, 0] size 16\n",
      "num hid layers: 1\n",
      "expanded_input_ordering [0, 1] size 2\n",
      "expanded_output_ordering [2, 2] size 2\n",
      "number of levels: 6\n",
      "input_levels [['r'], ['z2', 'r'], ['z2', 'y4', 'z1'], ['y2', 'x8', 'z1', 'y4', 'y3'], ['y2', 'x6', 'x4', 'y3', 'y1'], ['x2', 'y1']]\n",
      "out_levels [['z2'], ['y4', 'z1'], ['y2', 'x8', 'y3'], ['x6', 'x4', 'x7', 'y1'], ['x5', 'x2', 'x3'], ['x1']]\n",
      "[Step 10/1000] Immediate Loss: 9.157310142517089 Accumlated Loss: 15.049692971229552 Duration: 0.35099339485168457\n",
      "[Step 20/1000] Immediate Loss: 5.85079418182373 Accumlated Loss: 6.951483745574951 Duration: 0.348111629486084\n",
      "[Step 30/1000] Immediate Loss: 5.0251405143737795 Accumlated Loss: 5.265295015335083 Duration: 0.41347718238830566\n",
      "[Step 40/1000] Immediate Loss: 4.455887260437011 Accumlated Loss: 4.758720594406128 Duration: 0.35964298248291016\n",
      "[Step 50/1000] Immediate Loss: 4.326854467391968 Accumlated Loss: 4.419370892524719 Duration: 0.3862590789794922\n",
      "[Step 60/1000] Immediate Loss: 4.218307285308837 Accumlated Loss: 4.209368563652038 Duration: 0.35480189323425293\n",
      "[Step 70/1000] Immediate Loss: 3.8374552536010738 Accumlated Loss: 3.978692611694336 Duration: 0.409717321395874\n",
      "[Step 80/1000] Immediate Loss: 3.751860332489014 Accumlated Loss: 3.820096109390259 Duration: 0.35500216484069824\n",
      "[Step 90/1000] Immediate Loss: 3.7082661437988285 Accumlated Loss: 3.729127117156982 Duration: 0.39754223823547363\n",
      "[Step 100/1000] Immediate Loss: 3.5890631103515624 Accumlated Loss: 3.6896474103927615 Duration: 0.3591599464416504\n",
      "[Step 110/1000] Immediate Loss: 3.474279747009277 Accumlated Loss: 3.586595432281494 Duration: 0.34798717498779297\n",
      "[Step 120/1000] Immediate Loss: 3.6556751251220705 Accumlated Loss: 3.5458038244247434 Duration: 0.38567328453063965\n",
      "[Step 130/1000] Immediate Loss: 3.4842983436584474 Accumlated Loss: 3.5041985340118402 Duration: 0.35711145401000977\n",
      "[Step 140/1000] Immediate Loss: 3.3172739982604984 Accumlated Loss: 3.470182712554932 Duration: 0.39105868339538574\n",
      "[Step 150/1000] Immediate Loss: 3.4829155921936037 Accumlated Loss: 3.4747222614288327 Duration: 0.3634517192840576\n",
      "[Step 160/1000] Immediate Loss: 3.342771110534668 Accumlated Loss: 3.420569646835328 Duration: 0.3930521011352539\n",
      "[Step 170/1000] Immediate Loss: 3.3771132946014406 Accumlated Loss: 3.3877188377380376 Duration: 0.3480498790740967\n",
      "[Step 180/1000] Immediate Loss: 3.387749195098877 Accumlated Loss: 3.410190097808838 Duration: 0.42006444931030273\n",
      "[Step 190/1000] Immediate Loss: 3.393511924743653 Accumlated Loss: 3.387726984024048 Duration: 0.35900235176086426\n",
      "[Step 200/1000] Immediate Loss: 3.293798923492432 Accumlated Loss: 3.370459241867066 Duration: 0.3927791118621826\n",
      "[Step 210/1000] Immediate Loss: 3.2775422096252442 Accumlated Loss: 3.3615310697555545 Duration: 0.356921911239624\n",
      "[Step 220/1000] Immediate Loss: 3.3820526409149165 Accumlated Loss: 3.3239399595260624 Duration: 0.3690330982208252\n",
      "[Step 230/1000] Immediate Loss: 3.253921890258789 Accumlated Loss: 3.351316024780273 Duration: 0.40285444259643555\n",
      "[Step 240/1000] Immediate Loss: 3.3662335586547854 Accumlated Loss: 3.3590779170989986 Duration: 0.3788950443267822\n",
      "[Step 250/1000] Immediate Loss: 3.3439441299438477 Accumlated Loss: 3.3786073141098028 Duration: 0.3920586109161377\n",
      "[Step 260/1000] Immediate Loss: 3.354782133102417 Accumlated Loss: 3.3513808755874637 Duration: 0.35993361473083496\n",
      "[Step 270/1000] Immediate Loss: 3.3372807884216313 Accumlated Loss: 3.338691305160523 Duration: 0.39347100257873535\n",
      "[Step 280/1000] Immediate Loss: 3.310175018310547 Accumlated Loss: 3.2986322603225706 Duration: 0.4072144031524658\n",
      "[Step 290/1000] Immediate Loss: 3.333268260955811 Accumlated Loss: 3.3248736991882324 Duration: 0.41123247146606445\n",
      "[Step 300/1000] Immediate Loss: 3.3544892120361323 Accumlated Loss: 3.326479442596436 Duration: 0.4039759635925293\n",
      "[Step 310/1000] Immediate Loss: 3.346428356170654 Accumlated Loss: 3.305378651618958 Duration: 0.42134761810302734\n",
      "[Step 320/1000] Immediate Loss: 3.403144226074219 Accumlated Loss: 3.311773896217346 Duration: 0.36122655868530273\n",
      "[Step 330/1000] Immediate Loss: 3.2763449478149416 Accumlated Loss: 3.3266497364044185 Duration: 0.3582150936126709\n",
      "[Step 340/1000] Immediate Loss: 3.266353530883789 Accumlated Loss: 3.333148896217346 Duration: 0.42594099044799805\n",
      "[Step 350/1000] Immediate Loss: 3.279413528442383 Accumlated Loss: 3.2995057687759397 Duration: 0.3749394416809082\n",
      "[Step 360/1000] Immediate Loss: 3.213767147064209 Accumlated Loss: 3.308307558059693 Duration: 0.3870062828063965\n",
      "[Step 370/1000] Immediate Loss: 3.37831901550293 Accumlated Loss: 3.3509619054794317 Duration: 0.3554391860961914\n",
      "[Step 380/1000] Immediate Loss: 3.2749361038208002 Accumlated Loss: 3.320907657623291 Duration: 0.43279170989990234\n",
      "[Step 390/1000] Immediate Loss: 3.2810496330261225 Accumlated Loss: 3.328427023887634 Duration: 0.35605382919311523\n",
      "[Step 400/1000] Immediate Loss: 3.278467082977295 Accumlated Loss: 3.317427125930786 Duration: 0.38791894912719727\n",
      "[Step 410/1000] Immediate Loss: 3.3440698432922367 Accumlated Loss: 3.326253881454468 Duration: 0.3630564212799072\n",
      "[Step 420/1000] Immediate Loss: 3.3230349349975588 Accumlated Loss: 3.3114357585906977 Duration: 0.4106309413909912\n",
      "[Step 430/1000] Immediate Loss: 3.313874320983887 Accumlated Loss: 3.316153571128845 Duration: 0.39663028717041016\n",
      "[Step 440/1000] Immediate Loss: 3.3600057220458988 Accumlated Loss: 3.301804824829101 Duration: 0.3836090564727783\n",
      "[Step 450/1000] Immediate Loss: 3.286161613464355 Accumlated Loss: 3.3144787635803223 Duration: 0.40917134284973145\n",
      "[Step 460/1000] Immediate Loss: 3.3025293159484863 Accumlated Loss: 3.2997208251953123 Duration: 0.38283514976501465\n",
      "[Step 470/1000] Immediate Loss: 3.2771983528137203 Accumlated Loss: 3.291448328971863 Duration: 0.4188551902770996\n",
      "[Step 480/1000] Immediate Loss: 3.287361907958984 Accumlated Loss: 3.3076985902786253 Duration: 0.38308286666870117\n",
      "[Step 490/1000] Immediate Loss: 3.350705604553223 Accumlated Loss: 3.3191831941604613 Duration: 0.4051980972290039\n",
      "[Step 500/1000] Immediate Loss: 3.269703922271729 Accumlated Loss: 3.2964485731124875 Duration: 0.36760973930358887\n",
      "[Step 510/1000] Immediate Loss: 3.309275913238525 Accumlated Loss: 3.3095148010253905 Duration: 0.3980100154876709\n",
      "[Step 520/1000] Immediate Loss: 3.280012950897217 Accumlated Loss: 3.3023848466873167 Duration: 0.35692524909973145\n",
      "[Step 530/1000] Immediate Loss: 3.2583084297180176 Accumlated Loss: 3.3132524414062496 Duration: 0.40805792808532715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 540/1000] Immediate Loss: 3.249091358184814 Accumlated Loss: 3.3023037862777707 Duration: 0.3657839298248291\n",
      "[Step 550/1000] Immediate Loss: 3.3021725273132323 Accumlated Loss: 3.2992365531921384 Duration: 0.3607676029205322\n",
      "[Step 560/1000] Immediate Loss: 3.2874881362915036 Accumlated Loss: 3.307847092628479 Duration: 0.40618443489074707\n",
      "[Step 570/1000] Immediate Loss: 3.2956336784362787 Accumlated Loss: 3.313754090309143 Duration: 0.36110687255859375\n",
      "[Step 580/1000] Immediate Loss: 3.308529033660889 Accumlated Loss: 3.307110284805298 Duration: 0.4040539264678955\n",
      "[Step 590/1000] Immediate Loss: 3.266728000640869 Accumlated Loss: 3.3071045789718623 Duration: 0.36758947372436523\n",
      "[Step 600/1000] Immediate Loss: 3.3427077102661134 Accumlated Loss: 3.319442165374756 Duration: 0.3978755474090576\n",
      "[Step 610/1000] Immediate Loss: 3.277811832427979 Accumlated Loss: 3.3092873697280885 Duration: 0.368602991104126\n",
      "[Step 620/1000] Immediate Loss: 3.3398999786376953 Accumlated Loss: 3.3137764816284183 Duration: 0.388408899307251\n",
      "[Step 630/1000] Immediate Loss: 3.3554999160766603 Accumlated Loss: 3.300262495994568 Duration: 0.370255708694458\n",
      "[Step 640/1000] Immediate Loss: 3.32398515701294 Accumlated Loss: 3.3128390302658084 Duration: 0.40645265579223633\n",
      "[Step 650/1000] Immediate Loss: 3.3210130500793458 Accumlated Loss: 3.298892056465149 Duration: 0.35892391204833984\n",
      "[Step 660/1000] Immediate Loss: 3.2175397109985355 Accumlated Loss: 3.2880573740005494 Duration: 0.3479282855987549\n",
      "[Step 670/1000] Immediate Loss: 3.351421165466309 Accumlated Loss: 3.3231223983764653 Duration: 0.43328332901000977\n",
      "[Step 680/1000] Immediate Loss: 3.2576738166809083 Accumlated Loss: 3.3032870035171507 Duration: 0.35775184631347656\n",
      "[Step 690/1000] Immediate Loss: 3.272449645996094 Accumlated Loss: 3.309213233947754 Duration: 0.3954291343688965\n",
      "[Step 700/1000] Immediate Loss: 3.270857238769531 Accumlated Loss: 3.2929638547897335 Duration: 0.36011576652526855\n",
      "[Step 710/1000] Immediate Loss: 3.2790110588073733 Accumlated Loss: 3.315713575363159 Duration: 0.42752623558044434\n",
      "[Step 720/1000] Immediate Loss: 3.299638538360596 Accumlated Loss: 3.3011173324584955 Duration: 0.366910457611084\n",
      "[Step 730/1000] Immediate Loss: 3.2682429313659664 Accumlated Loss: 3.306383940696717 Duration: 0.39806294441223145\n",
      "[Step 740/1000] Immediate Loss: 3.2332065010070803 Accumlated Loss: 3.303490586280823 Duration: 0.37712717056274414\n",
      "[Step 750/1000] Immediate Loss: 3.310677337646484 Accumlated Loss: 3.3010164909362794 Duration: 0.41501855850219727\n",
      "[Step 760/1000] Immediate Loss: 3.266561450958252 Accumlated Loss: 3.300159492492676 Duration: 0.3900594711303711\n",
      "[Step 770/1000] Immediate Loss: 3.2882065200805664 Accumlated Loss: 3.2848971471786497 Duration: 0.3660550117492676\n",
      "[Step 780/1000] Immediate Loss: 3.322351398468018 Accumlated Loss: 3.3257057313919067 Duration: 0.42407703399658203\n",
      "[Step 790/1000] Immediate Loss: 3.3259357833862304 Accumlated Loss: 3.3010821838378903 Duration: 0.3699007034301758\n",
      "[Step 800/1000] Immediate Loss: 3.3515318870544433 Accumlated Loss: 3.3092310037612913 Duration: 0.39723944664001465\n",
      "[Step 810/1000] Immediate Loss: 3.367356910705566 Accumlated Loss: 3.315722069740296 Duration: 0.35505223274230957\n",
      "[Step 820/1000] Immediate Loss: 3.2537025356292726 Accumlated Loss: 3.301430205345154 Duration: 0.39604759216308594\n",
      "[Step 830/1000] Immediate Loss: 3.3313663482666014 Accumlated Loss: 3.3168755073547365 Duration: 0.35784435272216797\n",
      "[Step 840/1000] Immediate Loss: 3.374281883239746 Accumlated Loss: 3.3063914518356325 Duration: 0.40817952156066895\n",
      "[Step 850/1000] Immediate Loss: 3.2705587673187257 Accumlated Loss: 3.2939389181137084 Duration: 0.35893774032592773\n",
      "[Step 860/1000] Immediate Loss: 3.2662834167480472 Accumlated Loss: 3.323611269950867 Duration: 0.4060525894165039\n",
      "[Step 870/1000] Immediate Loss: 3.299567708969116 Accumlated Loss: 3.2936978788375857 Duration: 0.4847078323364258\n",
      "[Step 880/1000] Immediate Loss: 3.3145069885253906 Accumlated Loss: 3.3072527265548706 Duration: 0.4159691333770752\n",
      "[Step 890/1000] Immediate Loss: 3.288979301452637 Accumlated Loss: 3.311298152923584 Duration: 0.4421226978302002\n",
      "[Step 900/1000] Immediate Loss: 3.331672554016113 Accumlated Loss: 3.2962228116989136 Duration: 0.37487053871154785\n",
      "[Step 910/1000] Immediate Loss: 3.349849491119385 Accumlated Loss: 3.3247341480255126 Duration: 0.40424036979675293\n",
      "[Step 920/1000] Immediate Loss: 3.3153474044799807 Accumlated Loss: 3.298115752220154 Duration: 0.36104893684387207\n",
      "[Step 930/1000] Immediate Loss: 3.347795543670654 Accumlated Loss: 3.29688000869751 Duration: 0.405078649520874\n",
      "[Step 940/1000] Immediate Loss: 3.3006763458251953 Accumlated Loss: 3.3067226610183713 Duration: 0.3592236042022705\n",
      "[Step 950/1000] Immediate Loss: 3.3473744583129887 Accumlated Loss: 3.31758397769928 Duration: 0.39299511909484863\n",
      "[Step 960/1000] Immediate Loss: 3.2901748847961425 Accumlated Loss: 3.3202925367355345 Duration: 0.3634631633758545\n",
      "[Step 970/1000] Immediate Loss: 3.2751164627075195 Accumlated Loss: 3.2988116960525513 Duration: 0.40302443504333496\n",
      "[Step 980/1000] Immediate Loss: 3.306729278564453 Accumlated Loss: 3.3053677740097047 Duration: 0.3580911159515381\n",
      "[Step 990/1000] Immediate Loss: 3.3456609630584717 Accumlated Loss: 3.323902424812317 Duration: 0.3670032024383545\n",
      "[Step 1000/1000] Immediate Loss: 3.2466304969787596 Accumlated Loss: 3.308351089477539 Duration: 0.5125195980072021\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pyro\n",
    "import time\n",
    "import pyro.contrib.examples.polyphonic_data_loader as poly\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "from made import MADE\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal, AutoNormal, AutoMultivariateNormal\n",
    "from gmade import GMADE\n",
    "random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# NN used for p(x | y)\n",
    "class simpleNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden=32, out_size=1, t=\"normal\", out_non_linear=None):\n",
    "        super().__init__()\n",
    "        self.t = t\n",
    "        self.out_non_linear = out_non_linear\n",
    "        self.hiddeen_layer = nn.Linear(input_size, hidden)\n",
    "        if t == \"normal\":\n",
    "            self.loc_layer = nn.Linear(hidden, out_size)\n",
    "            self.std_layer = nn.Linear(hidden, out_size)\n",
    "            self.softplus = nn.Softplus()\n",
    "        elif t == \"bern\":\n",
    "            self.prob_layer = nn.Linear(hidden, out_size)\n",
    "        elif t == \"mlp\":\n",
    "            self.out_layer = nn.Linear(hidden, out_size)\n",
    "        \n",
    "    def forward(self, x_list):\n",
    "        for i in range(len(x_list)):\n",
    "            if x_list[i].dim() == 0:\n",
    "                x_list[i] = torch.unsqueeze(x_list[i], dim=0)\n",
    "        \n",
    "        input_x = torch.cat(x_list, -1)\n",
    "        \n",
    "        hid = F.relu(self.hiddeen_layer(input_x))\n",
    "        # return loc, std\n",
    "        if self.t == \"normal\":\n",
    "            return self.loc_layer(hid), self.softplus(self.std_layer(hid))\n",
    "        elif self.t == \"bern\":\n",
    "            return torch.sigmoid(self.prob_layer(hid))\n",
    "        else:\n",
    "            if self.out_non_linear == \"tanh\":\n",
    "                return torch.tanh(self.out_layer(hid))\n",
    "            else:\n",
    "                return self.out_layer(hid)\n",
    "\n",
    "class Experiment(nn.Module):\n",
    "    def __init__(self, USE_CUDA):\n",
    "        super().__init__()\n",
    "        \n",
    "        # guide 1\n",
    "        \n",
    "        self.x1_net_1 = simpleNN()\n",
    "        self.x2_net_1 = simpleNN()\n",
    "        self.x3_net_1 = simpleNN()\n",
    "        self.x4_net_1 = simpleNN()\n",
    "        self.x5_net_1 = simpleNN()\n",
    "        self.x6_net_1 = simpleNN()\n",
    "        self.x7_net_1 = simpleNN()\n",
    "        self.x8_net_1 = simpleNN()\n",
    "        self.y1_net_1 = simpleNN()\n",
    "        self.y2_net_1 = simpleNN()\n",
    "        self.y3_net_1 = simpleNN()\n",
    "        self.y4_net_1 = simpleNN()\n",
    "        self.z1_net_1 = simpleNN()\n",
    "        self.z2_net_1 = simpleNN()\n",
    "        \n",
    "        # guide 2\n",
    "        self.x1_net_2 = simpleNN(2)\n",
    "        self.x2_net_2 = simpleNN(2)\n",
    "        self.x3_net_2 = simpleNN(2)\n",
    "        self.x4_net_2 = simpleNN(2)\n",
    "        self.x5_net_2 = simpleNN(2)\n",
    "        self.x6_net_2 = simpleNN(2)\n",
    "        self.x7_net_2 = simpleNN(2)\n",
    "        self.x8_net_2 = simpleNN(2)\n",
    "        self.y1_net_2 = simpleNN(2)\n",
    "        self.y2_net_2 = simpleNN(2)\n",
    "        self.y3_net_2 = simpleNN(2)\n",
    "        self.y4_net_2 = simpleNN(2)\n",
    "        self.z1_net_2 = simpleNN()\n",
    "        self.z2_net_2 = simpleNN()\n",
    "        \n",
    "        # guide 3\n",
    "        self.x1_net_3 = simpleNN(7)\n",
    "        self.x2_net_3 = simpleNN(7)\n",
    "        self.x3_net_3 = simpleNN(7)\n",
    "        self.x4_net_3 = simpleNN(7)\n",
    "        self.x5_net_3 = simpleNN(7)\n",
    "        self.x6_net_3 = simpleNN(7)\n",
    "        self.x7_net_3 = simpleNN(7)\n",
    "        self.x8_net_3 = simpleNN(7)\n",
    "        self.y1_net_3 = simpleNN(3)\n",
    "        self.y2_net_3 = simpleNN(3)\n",
    "        self.y3_net_3 = simpleNN(3)\n",
    "        self.y4_net_3 = simpleNN(3)\n",
    "        self.z1_net_3 = simpleNN()\n",
    "        self.z2_net_3 = simpleNN()\n",
    "\n",
    "        # guide 4\n",
    "        self.x1_net_4 = simpleNN(2)\n",
    "        self.x2_net_4 = simpleNN()\n",
    "        self.x3_net_4 = simpleNN(2)\n",
    "        self.x4_net_4 = simpleNN()\n",
    "        self.x5_net_4 = simpleNN(2)\n",
    "        self.x6_net_4 = simpleNN()\n",
    "        self.x7_net_4 = simpleNN(2)\n",
    "        self.x8_net_4 = simpleNN()\n",
    "        self.y1_net_4 = simpleNN(2)\n",
    "        self.y2_net_4 = simpleNN()\n",
    "        self.y3_net_4 = simpleNN(2)\n",
    "        self.y4_net_4 = simpleNN()\n",
    "        self.z1_net_4 = simpleNN(2)\n",
    "        self.z2_net_4 = simpleNN()\n",
    "        \n",
    "        # guide 5\n",
    "        self.x1_net_5 = simpleNN(14)\n",
    "        self.x2_net_5 = simpleNN(13)\n",
    "        self.x3_net_5 = simpleNN(12)\n",
    "        self.x4_net_5 = simpleNN(11)\n",
    "        self.x5_net_5 = simpleNN(10)\n",
    "        self.x6_net_5 = simpleNN(9)\n",
    "        self.x7_net_5 = simpleNN(8)\n",
    "        self.x8_net_5 = simpleNN(7)\n",
    "        self.y1_net_5 = simpleNN(6)\n",
    "        self.y2_net_5 = simpleNN(5)\n",
    "        self.y3_net_5 = simpleNN(4)\n",
    "        self.y4_net_5 = simpleNN(3)\n",
    "        self.z1_net_5 = simpleNN(2)\n",
    "        self.z2_net_5 = simpleNN()\n",
    "        \n",
    "        # guide 6\n",
    "        self.x1_net_6 = simpleNN()\n",
    "        self.x2_net_6 = simpleNN()\n",
    "        self.x3_net_6 = simpleNN()\n",
    "        self.x4_net_6 = simpleNN()\n",
    "        self.x5_net_6 = simpleNN()\n",
    "        self.x6_net_6 = simpleNN()\n",
    "        self.x7_net_6 = simpleNN()\n",
    "        self.x8_net_6 = simpleNN()\n",
    "        self.y1_net_6 = simpleNN(2)\n",
    "        self.y2_net_6 = simpleNN(2)\n",
    "        self.y3_net_6 = simpleNN(2)\n",
    "        self.y4_net_6 = simpleNN(2)\n",
    "        self.z1_net_6 = simpleNN(2)\n",
    "        self.z2_net_6 = simpleNN(2)\n",
    "        \n",
    "        # guide 7\n",
    "        self.x1_net_7 = simpleNN()\n",
    "        self.x2_net_7 = simpleNN(2)\n",
    "        self.x3_net_7 = simpleNN()\n",
    "        self.x4_net_7 = simpleNN(2)\n",
    "        self.x5_net_7 = simpleNN()\n",
    "        self.x6_net_7 = simpleNN(2)\n",
    "        self.x7_net_7 = simpleNN()\n",
    "        self.x8_net_7 = simpleNN(2)\n",
    "        self.y1_net_7 = simpleNN(2)\n",
    "        self.y2_net_7 = simpleNN(3)\n",
    "        self.y3_net_7 = simpleNN(2)\n",
    "        self.y4_net_7 = simpleNN(3)\n",
    "        self.z1_net_7 = simpleNN(2)\n",
    "        self.z2_net_7 = simpleNN(3)\n",
    "        \n",
    "        \n",
    "        # guide full_made\n",
    "        input_dim_dict = {\n",
    "            \"r\" : 1\n",
    "        }\n",
    "        var_dim_dict = {\n",
    "            \"x1\" : 1,\n",
    "            \"x2\" : 1,\n",
    "            \"x3\" : 1,\n",
    "            \"x4\" : 1,\n",
    "            \"x5\" : 1,\n",
    "            \"x6\" : 1,\n",
    "            \"x7\" : 1,\n",
    "            \"x8\" : 1,\n",
    "            \"y1\" : 1,\n",
    "            \"y2\" : 1,\n",
    "            \"y3\" : 1,\n",
    "            \"y4\" : 1,\n",
    "            \"z1\" : 1,\n",
    "            \"z2\" : 1\n",
    "        }\n",
    "        dependency_dict = {\n",
    "            \"z2\" : [\"r\"],\n",
    "            \"z1\" : [\"r\", \"z2\"],\n",
    "            \"y4\" : [\"z2\"],\n",
    "            \"y3\" : [\"z2\", \"y4\"],\n",
    "            \"y2\" : [\"z1\"],\n",
    "            \"y1\" : [\"z1\", \"y2\"],\n",
    "            \"x8\" : [\"y4\"],\n",
    "            \"x7\" : [\"y4\", \"x8\"],\n",
    "            \"x6\" : [\"y3\"],\n",
    "            \"x5\" : [\"y3\", \"x6\"],\n",
    "            \"x4\" : [\"y2\"],\n",
    "            \"x3\" : [\"y2\", \"x4\"],\n",
    "            \"x2\" : [\"y1\"],\n",
    "            \"x1\" : [\"y1\", \"x2\"]\n",
    "        }\n",
    "        to_event_dict = {\n",
    "            \"x1\" : 1,\n",
    "            \"x2\" : 1,\n",
    "            \"x3\" : 1,\n",
    "            \"x4\" : 1,\n",
    "            \"x5\" : 1,\n",
    "            \"x6\" : 1,\n",
    "            \"x7\" : 1,\n",
    "            \"x8\" : 1,\n",
    "            \"y1\" : 1,\n",
    "            \"y2\" : 1,\n",
    "            \"y3\" : 1,\n",
    "            \"y4\" : 1,\n",
    "            \"z1\" : 1,\n",
    "            \"z2\" : 1\n",
    "        }\n",
    "        \n",
    "        self.full_made = GMADE(input_dim_dict, dependency_dict, var_dim_dict, to_event_dict=to_event_dict)\n",
    "        if USE_CUDA:\n",
    "            self.cuda()\n",
    "    \n",
    "    # a tree model\n",
    "    def model(self, obs):\n",
    "        pyro.module(\"model\", self)\n",
    "        with pyro.plate(\"data\", obs.shape[0]):\n",
    "            x1 = pyro.sample(\"x1\", dist.Normal(torch.zeros(obs.shape[0], 1).cuda(), torch.ones(obs.shape[0], 1).cuda()).to_event(1))\n",
    "            x2 = pyro.sample(\"x2\", dist.Normal(torch.zeros(obs.shape[0], 1).cuda(), torch.ones(obs.shape[0], 1).cuda()).to_event(1))\n",
    "            x3 = pyro.sample(\"x3\", dist.Normal(torch.zeros(obs.shape[0], 1).cuda(), torch.ones(obs.shape[0], 1).cuda()).to_event(1))\n",
    "            x4 = pyro.sample(\"x4\", dist.Normal(torch.zeros(obs.shape[0], 1).cuda(), torch.ones(obs.shape[0], 1).cuda()).to_event(1))\n",
    "            x5 = pyro.sample(\"x5\", dist.Normal(torch.zeros(obs.shape[0], 1).cuda(), torch.ones(obs.shape[0], 1).cuda()).to_event(1))\n",
    "            x6 = pyro.sample(\"x6\", dist.Normal(torch.zeros(obs.shape[0], 1).cuda(), torch.ones(obs.shape[0], 1).cuda()).to_event(1))\n",
    "            x7 = pyro.sample(\"x7\", dist.Normal(torch.zeros(obs.shape[0], 1).cuda(), torch.ones(obs.shape[0], 1).cuda()).to_event(1))\n",
    "            x8 = pyro.sample(\"x8\", dist.Normal(torch.zeros(obs.shape[0], 1).cuda(), torch.ones(obs.shape[0], 1).cuda()).to_event(1))\n",
    "            y1 = pyro.sample(\"y1\", dist.Normal(x1+x2, torch.ones(obs.shape[0], 1).cuda()).to_event(1))\n",
    "            y2 = pyro.sample(\"y2\", dist.Normal(x3+x4, torch.ones(obs.shape[0], 1).cuda()).to_event(1))\n",
    "            y3 = pyro.sample(\"y3\", dist.Normal(x5+x6, torch.ones(obs.shape[0], 1).cuda()).to_event(1))\n",
    "            y4 = pyro.sample(\"y4\", dist.Normal(x7+x8, torch.ones(obs.shape[0], 1).cuda()).to_event(1))\n",
    "            z1 = pyro.sample(\"z1\", dist.Normal(y1+y2, torch.ones(obs.shape[0], 1).cuda()).to_event(1))\n",
    "            z2 = pyro.sample(\"z2\", dist.Normal(y3+y4, torch.ones(obs.shape[0], 1).cuda()).to_event(1))\n",
    "            pyro.sample(\"obs\", dist.Normal(z1+z2, 1.0).to_event(1), obs=obs)\n",
    "        \n",
    "    # guide 1 basically inverse the arrows in the model\n",
    "    def guide_1(self, obs):\n",
    "        pyro.module(\"model\", self)\n",
    "        z2_mean, z2_std = self.z2_net_1([obs])\n",
    "        z2 = pyro.sample(\"z2\", dist.Normal(z2_mean, z2_std))\n",
    "        z1_mean, z1_std = self.z1_net_1([obs])\n",
    "        z1 = pyro.sample(\"z1\", dist.Normal(z1_mean, z1_std))\n",
    "        y4_mean, y4_std = self.y4_net_1([z2])\n",
    "        y4 = pyro.sample(\"y4\", dist.Normal(y4_mean, y4_std))\n",
    "        y3_mean, y3_std = self.y3_net_1([z2])\n",
    "        y3 = pyro.sample(\"y3\", dist.Normal(y3_mean, y3_std))\n",
    "        y2_mean, y2_std = self.y2_net_1([z1])\n",
    "        y2 = pyro.sample(\"y2\", dist.Normal(y2_mean, y2_std))\n",
    "        y1_mean, y1_std = self.y1_net_1([z1])\n",
    "        y1 = pyro.sample(\"y1\", dist.Normal(y1_mean, y1_std))\n",
    "        x8_mean, x8_std = self.x8_net_1([y4])\n",
    "        x8 = pyro.sample(\"x8\", dist.Normal(x8_mean, x8_std))\n",
    "        x7_mean, x7_std = self.x7_net_1([y4])\n",
    "        x7 = pyro.sample(\"x7\", dist.Normal(x7_mean, x7_std))\n",
    "        x6_mean, x6_std = self.x6_net_1([y3])\n",
    "        x6 = pyro.sample(\"x6\", dist.Normal(x6_mean, x6_std))\n",
    "        x5_mean, x5_std = self.x5_net_1([y3])\n",
    "        x5 = pyro.sample(\"x5\", dist.Normal(x5_mean, x5_std))\n",
    "        x4_mean, x4_std = self.x4_net_1([y2])\n",
    "        x4 = pyro.sample(\"x4\", dist.Normal(x4_mean, x4_std))\n",
    "        x3_mean, x3_std = self.x3_net_1([y2])\n",
    "        x3 = pyro.sample(\"x3\", dist.Normal(x3_mean, x3_std))\n",
    "        x2_mean, x2_std = self.x2_net_1([y1])\n",
    "        x2 = pyro.sample(\"x2\", dist.Normal(x2_mean, x2_std))\n",
    "        x1_mean, x1_std = self.x1_net_1([y1])\n",
    "        x1 = pyro.sample(\"x1\", dist.Normal(x1_mean, x1_std))\n",
    "    \n",
    "    # guide 2 inverse the arrows in the model and add obs as dependency for each RV \n",
    "    def guide_2(self, obs):\n",
    "        pyro.module(\"model\", self)\n",
    "        z2_mean, z2_std = self.z2_net_2([obs])\n",
    "        z2 = pyro.sample(\"z2\", dist.Normal(z2_mean, z2_std))\n",
    "        z1_mean, z1_std = self.z1_net_2([obs])\n",
    "        z1 = pyro.sample(\"z1\", dist.Normal(z1_mean, z1_std))\n",
    "        y4_mean, y4_std = self.y4_net_2([z2, obs])\n",
    "        y4 = pyro.sample(\"y4\", dist.Normal(y4_mean, y4_std))\n",
    "        y3_mean, y3_std = self.y3_net_2([z2, obs])\n",
    "        y3 = pyro.sample(\"y3\", dist.Normal(y3_mean, y3_std))\n",
    "        y2_mean, y2_std = self.y2_net_2([z1, obs])\n",
    "        y2 = pyro.sample(\"y2\", dist.Normal(y2_mean, y2_std))\n",
    "        y1_mean, y1_std = self.y1_net_2([z1, obs])\n",
    "        y1 = pyro.sample(\"y1\", dist.Normal(y1_mean, y1_std))\n",
    "        x8_mean, x8_std = self.x8_net_2([y4, obs])\n",
    "        x8 = pyro.sample(\"x8\", dist.Normal(x8_mean, x8_std))\n",
    "        x7_mean, x7_std = self.x7_net_2([y4, obs])\n",
    "        x7 = pyro.sample(\"x7\", dist.Normal(x7_mean, x7_std))\n",
    "        x6_mean, x6_std = self.x6_net_2([y3, obs])\n",
    "        x6 = pyro.sample(\"x6\", dist.Normal(x6_mean, x6_std))\n",
    "        x5_mean, x5_std = self.x5_net_2([y3, obs])\n",
    "        x5 = pyro.sample(\"x5\", dist.Normal(x5_mean, x5_std))\n",
    "        x4_mean, x4_std = self.x4_net_2([y2, obs])\n",
    "        x4 = pyro.sample(\"x4\", dist.Normal(x4_mean, x4_std))\n",
    "        x3_mean, x3_std = self.x3_net_2([y2, obs])\n",
    "        x3 = pyro.sample(\"x3\", dist.Normal(x3_mean, x3_std))\n",
    "        x2_mean, x2_std = self.x2_net_2([y1, obs])\n",
    "        x2 = pyro.sample(\"x2\", dist.Normal(x2_mean, x2_std))\n",
    "        x1_mean, x1_std = self.x1_net_2([y1, obs])\n",
    "        x1 = pyro.sample(\"x1\", dist.Normal(x1_mean, x1_std))\n",
    "    \n",
    "    # guide 3 inverse the arrows and each RV depends on all RVs in previous levels\n",
    "    # i.e. x depends on all y + z + obs\n",
    "    def guide_3(self, obs):\n",
    "        pyro.module(\"model\", self)\n",
    "        z2_mean, z2_std = self.z2_net_3([obs])\n",
    "        z2 = pyro.sample(\"z2\", dist.Normal(z2_mean, z2_std))\n",
    "        z1_mean, z1_std = self.z1_net_3([obs])\n",
    "        z1 = pyro.sample(\"z1\", dist.Normal(z1_mean, z1_std))\n",
    "        y4_mean, y4_std = self.y4_net_3([z1, z2, obs])\n",
    "        y4 = pyro.sample(\"y4\", dist.Normal(y4_mean, y4_std))\n",
    "        y3_mean, y3_std = self.y3_net_3([z1, z2, obs])\n",
    "        y3 = pyro.sample(\"y3\", dist.Normal(y3_mean, y3_std))\n",
    "        y2_mean, y2_std = self.y2_net_3([z1, z2, obs])\n",
    "        y2 = pyro.sample(\"y2\", dist.Normal(y2_mean, y2_std))\n",
    "        y1_mean, y1_std = self.y1_net_3([z1, z2, obs])\n",
    "        y1 = pyro.sample(\"y1\", dist.Normal(y1_mean, y1_std))\n",
    "        x8_mean, x8_std = self.x8_net_3([y1, y2, y3, y4, z1, z2, obs])\n",
    "        x8 = pyro.sample(\"x8\", dist.Normal(x8_mean, x8_std))\n",
    "        x7_mean, x7_std = self.x7_net_3([y1, y2, y3, y4, z1, z2, obs])\n",
    "        x7 = pyro.sample(\"x7\", dist.Normal(x7_mean, x7_std))\n",
    "        x6_mean, x6_std = self.x6_net_3([y1, y2, y3, y4, z1, z2, obs])\n",
    "        x6 = pyro.sample(\"x6\", dist.Normal(x6_mean, x6_std))\n",
    "        x5_mean, x5_std = self.x5_net_3([y1, y2, y3, y4, z1, z2, obs])\n",
    "        x5 = pyro.sample(\"x5\", dist.Normal(x5_mean, x5_std))\n",
    "        x4_mean, x4_std = self.x4_net_3([y1, y2, y3, y4, z1, z2, obs])\n",
    "        x4 = pyro.sample(\"x4\", dist.Normal(x4_mean, x4_std))\n",
    "        x3_mean, x3_std = self.x3_net_3([y1, y2, y3, y4, z1, z2, obs])\n",
    "        x3 = pyro.sample(\"x3\", dist.Normal(x3_mean, x3_std))\n",
    "        x2_mean, x2_std = self.x2_net_3([y1, y2, y3, y4, z1, z2, obs])\n",
    "        x2 = pyro.sample(\"x2\", dist.Normal(x2_mean, x2_std))\n",
    "        x1_mean, x1_std = self.x1_net_3([y1, y2, y3, y4, z1, z2, obs])\n",
    "        x1 = pyro.sample(\"x1\", dist.Normal(x1_mean, x1_std))\n",
    "    \n",
    "    # guide 4 inverse the arrows and each RV depends on its previously sampled sibling (if any) at the same level\n",
    "    # i.e. x1 depends on y1 and x2, y1 depends on z1 and y2, y3 depends on z2 and y4\n",
    "    # this should be minimum failthful\n",
    "    def guide_4(self, obs):\n",
    "        pyro.module(\"model\", self)\n",
    "        with pyro.plate(\"data\", obs.shape[0]):\n",
    "            z2_mean, z2_std = self.z2_net_4([obs])\n",
    "            z2 = pyro.sample(\"z2\", dist.Normal(z2_mean, z2_std).to_event(1))\n",
    "            z1_mean, z1_std = self.z1_net_4([z2, obs])\n",
    "            z1 = pyro.sample(\"z1\", dist.Normal(z1_mean, z1_std).to_event(1))\n",
    "            y4_mean, y4_std = self.y4_net_4([z2])\n",
    "            y4 = pyro.sample(\"y4\", dist.Normal(y4_mean, y4_std).to_event(1))\n",
    "            y3_mean, y3_std = self.y3_net_4([y4, z2])\n",
    "            y3 = pyro.sample(\"y3\", dist.Normal(y3_mean, y3_std).to_event(1))\n",
    "            y2_mean, y2_std = self.y2_net_4([z1])\n",
    "            y2 = pyro.sample(\"y2\", dist.Normal(y2_mean, y2_std).to_event(1))\n",
    "            y1_mean, y1_std = self.y1_net_4([y2, z1])\n",
    "            y1 = pyro.sample(\"y1\", dist.Normal(y1_mean, y1_std).to_event(1))\n",
    "            x8_mean, x8_std = self.x8_net_4([y4])\n",
    "            x8 = pyro.sample(\"x8\", dist.Normal(x8_mean, x8_std).to_event(1))\n",
    "            x7_mean, x7_std = self.x7_net_4([x8, y4])\n",
    "            x7 = pyro.sample(\"x7\", dist.Normal(x7_mean, x7_std).to_event(1))\n",
    "            x6_mean, x6_std = self.x6_net_4([y3])\n",
    "            x6 = pyro.sample(\"x6\", dist.Normal(x6_mean, x6_std).to_event(1))\n",
    "            x5_mean, x5_std = self.x5_net_4([x6, y3])\n",
    "            x5 = pyro.sample(\"x5\", dist.Normal(x5_mean, x5_std).to_event(1))\n",
    "            x4_mean, x4_std = self.x4_net_4([y2])\n",
    "            x4 = pyro.sample(\"x4\", dist.Normal(x4_mean, x4_std).to_event(1))\n",
    "            x3_mean, x3_std = self.x3_net_4([x4, y2])\n",
    "            x3 = pyro.sample(\"x3\", dist.Normal(x3_mean, x3_std).to_event(1))\n",
    "            x2_mean, x2_std = self.x2_net_4([y1])\n",
    "            x2 = pyro.sample(\"x2\", dist.Normal(x2_mean, x2_std).to_event(1))\n",
    "            x1_mean, x1_std = self.x1_net_4([x2, y1])\n",
    "            x1 = pyro.sample(\"x1\", dist.Normal(x1_mean, x1_std).to_event(1))\n",
    "    \n",
    "    # guide 5 inverse the arrows and each RV depends on all previously sampled RV\n",
    "    # fully-connected\n",
    "    def guide_5(self, obs):\n",
    "        pyro.module(\"model\", self)\n",
    "        z2_mean, z2_std = self.z2_net_5([obs])\n",
    "        z2 = pyro.sample(\"z2\", dist.Normal(z2_mean, z2_std))\n",
    "        z1_mean, z1_std = self.z1_net_5([z2, obs])\n",
    "        z1 = pyro.sample(\"z1\", dist.Normal(z1_mean, z1_std))\n",
    "        y4_mean, y4_std = self.y4_net_5([z1, z2, obs])\n",
    "        y4 = pyro.sample(\"y4\", dist.Normal(y4_mean, y4_std))\n",
    "        y3_mean, y3_std = self.y3_net_5([y4, z1, z2, obs])\n",
    "        y3 = pyro.sample(\"y3\", dist.Normal(y3_mean, y3_std))\n",
    "        y2_mean, y2_std = self.y2_net_5([y3, y4, z1, z2, obs])\n",
    "        y2 = pyro.sample(\"y2\", dist.Normal(y2_mean, y2_std))\n",
    "        y1_mean, y1_std = self.y1_net_5([y2, y3, y4, z1, z2, obs])\n",
    "        y1 = pyro.sample(\"y1\", dist.Normal(y1_mean, y1_std))\n",
    "        x8_mean, x8_std = self.x8_net_5([y1, y2, y3, y4, z1, z2, obs])\n",
    "        x8 = pyro.sample(\"x8\", dist.Normal(x8_mean, x8_std))\n",
    "        x7_mean, x7_std = self.x7_net_5([x8, y1, y2, y3, y4, z1, z2, obs])\n",
    "        x7 = pyro.sample(\"x7\", dist.Normal(x7_mean, x7_std))\n",
    "        x6_mean, x6_std = self.x6_net_5([x7, x8, y1, y2, y3, y4, z1, z2, obs])\n",
    "        x6 = pyro.sample(\"x6\", dist.Normal(x6_mean, x6_std))\n",
    "        x5_mean, x5_std = self.x5_net_5([x6, x7, x8, y1, y2, y3, y4, z1, z2, obs])\n",
    "        x5 = pyro.sample(\"x5\", dist.Normal(x5_mean, x5_std))\n",
    "        x4_mean, x4_std = self.x4_net_5([x5, x6, x7, x8, y1, y2, y3, y4, z1, z2, obs])\n",
    "        x4 = pyro.sample(\"x4\", dist.Normal(x4_mean, x4_std))\n",
    "        x3_mean, x3_std = self.x3_net_5([x4, x5, x6, x7, x8, y1, y2, y3, y4, z1, z2, obs])\n",
    "        x3 = pyro.sample(\"x3\", dist.Normal(x3_mean, x3_std))\n",
    "        x2_mean, x2_std = self.x2_net_5([x3, x4, x5, x6, x7, x8, y1, y2, y3, y4, z1, z2, obs])\n",
    "        x2 = pyro.sample(\"x2\", dist.Normal(x2_mean, x2_std))\n",
    "        x1_mean, x1_std = self.x1_net_5([x2, x3, x4, x5, x6, x7, x8, y1, y2, y3, y4, z1, z2, obs])\n",
    "        x1 = pyro.sample(\"x1\", dist.Normal(x1_mean, x1_std))\n",
    "        \n",
    "    # guide 6 the order to sample RV is the same as the model but given obs as dependency for xs\n",
    "    def guide_6(self, obs):\n",
    "        pyro.module(\"model\", self)\n",
    "\n",
    "        x1_mean, x1_std = self.x1_net_6([obs])\n",
    "        x1 = pyro.sample(\"x1\", dist.Normal(x1_mean, x1_std))\n",
    "        x2_mean, x2_std = self.x2_net_6([obs])\n",
    "        x2 = pyro.sample(\"x2\", dist.Normal(x2_mean, x2_std))\n",
    "        x3_mean, x3_std = self.x3_net_6([obs])\n",
    "        x3 = pyro.sample(\"x3\", dist.Normal(x3_mean, x3_std))\n",
    "        x4_mean, x4_std = self.x4_net_6([obs])\n",
    "        x4 = pyro.sample(\"x4\", dist.Normal(x4_mean, x4_std))        \n",
    "        x5_mean, x5_std = self.x5_net_6([obs])\n",
    "        x5 = pyro.sample(\"x5\", dist.Normal(x5_mean, x5_std))\n",
    "        x6_mean, x6_std = self.x6_net_6([obs])\n",
    "        x6 = pyro.sample(\"x6\", dist.Normal(x6_mean, x6_std))        \n",
    "        x7_mean, x7_std = self.x7_net_6([obs])\n",
    "        x7 = pyro.sample(\"x7\", dist.Normal(x7_mean, x7_std))\n",
    "        x8_mean, x8_std = self.x8_net_6([obs])\n",
    "        x8 = pyro.sample(\"x8\", dist.Normal(x8_mean, x8_std))\n",
    "        y1_mean, y1_std = self.y1_net_6([x1, x2])\n",
    "        y1 = pyro.sample(\"y1\", dist.Normal(y1_mean, y1_std))        \n",
    "        y2_mean, y2_std = self.y2_net_6([x3, x4])\n",
    "        y2 = pyro.sample(\"y2\", dist.Normal(y2_mean, y2_std))        \n",
    "        y3_mean, y3_std = self.y3_net_6([x5, x6])\n",
    "        y3 = pyro.sample(\"y3\", dist.Normal(y3_mean, y3_std))        \n",
    "        y4_mean, y4_std = self.y4_net_6([x7, x8])\n",
    "        y4 = pyro.sample(\"y4\", dist.Normal(y4_mean, y4_std))        \n",
    "        z1_mean, z1_std = self.z1_net_6([y1, y2])\n",
    "        z1 = pyro.sample(\"z1\", dist.Normal(z1_mean, z1_std))        \n",
    "        z2_mean, z2_std = self.z2_net_6([y3, y4])\n",
    "        z2 = pyro.sample(\"z2\", dist.Normal(z2_mean, z2_std))\n",
    "    \n",
    "    # guide 7 is similar to guide 6 but each RV dependent on its subling\n",
    "    # the inverse of guide 4\n",
    "    def guide_7(self, obs):\n",
    "        pyro.module(\"model\", self)\n",
    "        \n",
    "        x1_mean, x1_std = self.x1_net_7([obs])\n",
    "        x1 = pyro.sample(\"x1\", dist.Normal(x1_mean, x1_std))\n",
    "        x2_mean, x2_std = self.x2_net_7([x1, obs])\n",
    "        x2 = pyro.sample(\"x2\", dist.Normal(x2_mean, x2_std))\n",
    "        x3_mean, x3_std = self.x3_net_7([obs])\n",
    "        x3 = pyro.sample(\"x3\", dist.Normal(x3_mean, x3_std))\n",
    "        x4_mean, x4_std = self.x4_net_7([x3, obs])\n",
    "        x4 = pyro.sample(\"x4\", dist.Normal(x4_mean, x4_std))        \n",
    "        x5_mean, x5_std = self.x5_net_7([obs])\n",
    "        x5 = pyro.sample(\"x5\", dist.Normal(x5_mean, x5_std))\n",
    "        x6_mean, x6_std = self.x6_net_7([x5, obs])\n",
    "        x6 = pyro.sample(\"x6\", dist.Normal(x6_mean, x6_std))        \n",
    "        x7_mean, x7_std = self.x7_net_7([obs])\n",
    "        x7 = pyro.sample(\"x7\", dist.Normal(x7_mean, x7_std))\n",
    "        x8_mean, x8_std = self.x8_net_7([x7, obs])\n",
    "        x8 = pyro.sample(\"x8\", dist.Normal(x8_mean, x8_std))\n",
    "        y1_mean, y1_std = self.y1_net_7([x1, x2])\n",
    "        y1 = pyro.sample(\"y1\", dist.Normal(y1_mean, y1_std))        \n",
    "        y2_mean, y2_std = self.y2_net_7([x3, x4, y1])\n",
    "        y2 = pyro.sample(\"y2\", dist.Normal(y2_mean, y2_std))        \n",
    "        y3_mean, y3_std = self.y3_net_7([x5, x6])\n",
    "        y3 = pyro.sample(\"y3\", dist.Normal(y3_mean, y3_std))        \n",
    "        y4_mean, y4_std = self.y4_net_7([x7, x8, y3])\n",
    "        y4 = pyro.sample(\"y4\", dist.Normal(y4_mean, y4_std))        \n",
    "        z1_mean, z1_std = self.z1_net_7([y1, y2])\n",
    "        z1 = pyro.sample(\"z1\", dist.Normal(z1_mean, z1_std))        \n",
    "        z2_mean, z2_std = self.z2_net_7([y3, y4, z1])\n",
    "        z2 = pyro.sample(\"z2\", dist.Normal(z2_mean, z2_std))\n",
    "    \n",
    "    def guide_full_made(self, obs):\n",
    "        pyro.module(\"model\", self)\n",
    "        with pyro.plate(\"data\", obs.shape[0]):\n",
    "            input_made = {\n",
    "                    \"r\" : obs\n",
    "            }\n",
    "            output_dict = self.full_made(input_made, suffix=\"\")\n",
    "        \n",
    "def generate_data():\n",
    "    x_len = 8\n",
    "    xs = torch.randn(x_len)\n",
    "    ys = []\n",
    "    i = 0\n",
    "    while i < len(xs):\n",
    "        y = dist.Normal(xs[i] + xs[i+1], 2).sample()\n",
    "        ys.append(y)\n",
    "        i +=2\n",
    "        \n",
    "    zs = []\n",
    "    i = 0\n",
    "    while i < len(ys):\n",
    "        z = dist.Normal(ys[i] + ys[i+1], 1.5).sample()\n",
    "        zs.append(z)\n",
    "        i +=2\n",
    "        \n",
    "        \n",
    "    obs = dist.Normal(zs[0] + zs[1], 1).sample()\n",
    "    return obs\n",
    "    \n",
    "USE_CUDA = True\n",
    "data = []\n",
    "num_data = 100\n",
    "for _ in range(num_data):\n",
    "    data.append(generate_data().unsqueeze(0))\n",
    "data = torch.stack(data)\n",
    "batches = []\n",
    "for i in range(5):\n",
    "    batches.append(data[int(i * num_data / 5): int((i + 1) * num_data / 5)])\n",
    "experiment = Experiment(USE_CUDA)\n",
    "adam_params = {\"lr\": 0.001, \"betas\": (0.95, 0.999)}\n",
    "optimizer = Adam(adam_params)\n",
    "guide = experiment.guide_4 # guide_1\n",
    "\n",
    "# pyro auto guide\n",
    "#guide = AutoNormal(experiment.model)\n",
    "#guide = AutoMultivariateNormal(experiment.model)\n",
    "#guide = AutoDiagonalNormal(experiment.model)\n",
    "\n",
    "svi = SVI(experiment.model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "\n",
    "n_steps = 1000\n",
    "log_interval = 10\n",
    "# do gradient steps\n",
    "loss = 0\n",
    "loss_track = []\n",
    "for step in range(1, n_steps + 1):\n",
    "    imme_loss = 0\n",
    "    start = time.time()\n",
    "    for obs in batches:\n",
    "        if USE_CUDA:\n",
    "            obs = obs.cuda()\n",
    "            #print(obs.shape)\n",
    "        imme_loss += svi.step(obs) / num_data\n",
    "        \n",
    "    loss_track.append(imme_loss)\n",
    "    loss += imme_loss / log_interval\n",
    "    \n",
    "    if step % log_interval == 0:\n",
    "        print(\"[Step {}/{}] Immediate Loss: {} Accumlated Loss: {} Duration: {}\".format(step, n_steps, imme_loss, loss, time.time() - start))\n",
    "        loss = 0\n",
    "    \n",
    "plt.plot(loss_track)\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "olive-reynolds",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-54276383af52>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-54276383af52>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    [Step 10/100] Immediate Loss: 5.6792811819911035 Accumlated Loss: 8.161501753419639\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# guide 1\n",
    "[Step 10/100] Immediate Loss: 5.6792811819911035 Accumlated Loss: 8.161501753419639\n",
    "[Step 20/100] Immediate Loss: 5.6724499085545546 Accumlated Loss: 5.7964787363111965\n",
    "[Step 30/100] Immediate Loss: 5.761731808781624 Accumlated Loss: 5.581517954558135\n",
    "[Step 40/100] Immediate Loss: 6.112146431803706 Accumlated Loss: 5.703019751578569\n",
    "[Step 50/100] Immediate Loss: 5.990561635196209 Accumlated Loss: 5.61033443725109\n",
    "[Step 60/100] Immediate Loss: 5.79047949910164 Accumlated Loss: 5.709876654744148\n",
    "[Step 70/100] Immediate Loss: 5.865784194469453 Accumlated Loss: 5.6813742448389535\n",
    "[Step 80/100] Immediate Loss: 5.5839250811934455 Accumlated Loss: 5.665409379035234\n",
    "[Step 90/100] Immediate Loss: 5.802031826376916 Accumlated Loss: 5.689023856550455\n",
    "[Step 100/100] Immediate Loss: 5.711089228987696 Accumlated Loss: 5.733322246074676"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guide 2\n",
    "[Step 10/100] Immediate Loss: 6.268227699697018 Accumlated Loss: 9.823156678080556\n",
    "[Step 20/100] Immediate Loss: 6.012787947654725 Accumlated Loss: 6.139076143831014\n",
    "[Step 30/100] Immediate Loss: 6.102070910632606 Accumlated Loss: 5.927030056893825\n",
    "[Step 40/100] Immediate Loss: 6.45776209264994 Accumlated Loss: 5.981230340063572\n",
    "[Step 50/100] Immediate Loss: 6.256524928808213 Accumlated Loss: 5.893774724751712\n",
    "[Step 60/100] Immediate Loss: 6.069752818644045 Accumlated Loss: 5.9795956534147265\n",
    "[Step 70/100] Immediate Loss: 6.092835750877859 Accumlated Loss: 5.928910363674163\n",
    "[Step 80/100] Immediate Loss: 5.949914609193803 Accumlated Loss: 5.921238815844059\n",
    "[Step 90/100] Immediate Loss: 6.110138435661791 Accumlated Loss: 5.9243319559395315\n",
    "[Step 100/100] Immediate Loss: 5.967664820253849 Accumlated Loss: 5.956207058787346"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guide 3\n",
    "[Step 10/100] Immediate Loss: 6.570058162808415 Accumlated Loss: 9.397841033637526\n",
    "[Step 20/100] Immediate Loss: 6.147684178054332 Accumlated Loss: 6.235418024212122\n",
    "[Step 30/100] Immediate Loss: 6.111919716298578 Accumlated Loss: 5.938203674733639\n",
    "[Step 40/100] Immediate Loss: 6.531079505085945 Accumlated Loss: 5.937320747315884\n",
    "[Step 50/100] Immediate Loss: 6.133750829696654 Accumlated Loss: 5.836949352592229\n",
    "[Step 60/100] Immediate Loss: 5.995875943303109 Accumlated Loss: 5.909800890862941\n",
    "[Step 70/100] Immediate Loss: 6.01717264592648 Accumlated Loss: 5.870649518340828\n",
    "[Step 80/100] Immediate Loss: 5.837504681944848 Accumlated Loss: 5.835405748039484\n",
    "[Step 90/100] Immediate Loss: 6.031392835974693 Accumlated Loss: 5.827427438616752\n",
    "[Step 100/100] Immediate Loss: 5.949472520351413 Accumlated Loss: 5.865983818709852"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-theology",
   "metadata": {},
   "outputs": [],
   "source": [
    "### guide 4 ### best\n",
    "[Step 10/100] Immediate Loss: 4.377868088781835 Accumlated Loss: 6.545501601159573\n",
    "[Step 20/100] Immediate Loss: 3.575750644803046 Accumlated Loss: 4.03719574072957\n",
    "[Step 30/100] Immediate Loss: 3.5968227416276943 Accumlated Loss: 3.657659743905067\n",
    "[Step 40/100] Immediate Loss: 3.5601295506954207 Accumlated Loss: 3.6178785299956795\n",
    "[Step 50/100] Immediate Loss: 3.4762910395860662 Accumlated Loss: 3.543176322728395\n",
    "[Step 60/100] Immediate Loss: 3.52601896584034 Accumlated Loss: 3.533123803049326\n",
    "[Step 70/100] Immediate Loss: 3.449901377856731 Accumlated Loss: 3.498376666098833\n",
    "[Step 80/100] Immediate Loss: 3.472165140509606 Accumlated Loss: 3.527985264599323\n",
    "[Step 90/100] Immediate Loss: 3.535960964262485 Accumlated Loss: 3.4987394436597814\n",
    "[Step 100/100] Immediate Loss: 3.4975102710723887 Accumlated Loss: 3.5023412067890174\n",
    "\n",
    "# gpu\n",
    "[Step 10/1000] Immediate Loss: 9.157310142517089 Accumlated Loss: 15.049692971229552 Duration: 0.35099339485168457\n",
    "[Step 20/1000] Immediate Loss: 5.85079418182373 Accumlated Loss: 6.951483745574951 Duration: 0.348111629486084\n",
    "[Step 30/1000] Immediate Loss: 5.0251405143737795 Accumlated Loss: 5.265295015335083 Duration: 0.41347718238830566\n",
    "[Step 40/1000] Immediate Loss: 4.455887260437011 Accumlated Loss: 4.758720594406128 Duration: 0.35964298248291016\n",
    "[Step 50/1000] Immediate Loss: 4.326854467391968 Accumlated Loss: 4.419370892524719 Duration: 0.3862590789794922\n",
    "[Step 60/1000] Immediate Loss: 4.218307285308837 Accumlated Loss: 4.209368563652038 Duration: 0.35480189323425293\n",
    "[Step 70/1000] Immediate Loss: 3.8374552536010738 Accumlated Loss: 3.978692611694336 Duration: 0.409717321395874\n",
    "[Step 80/1000] Immediate Loss: 3.751860332489014 Accumlated Loss: 3.820096109390259 Duration: 0.35500216484069824\n",
    "[Step 90/1000] Immediate Loss: 3.7082661437988285 Accumlated Loss: 3.729127117156982 Duration: 0.39754223823547363\n",
    "[Step 100/1000] Immediate Loss: 3.5890631103515624 Accumlated Loss: 3.6896474103927615 Duration: 0.3591599464416504\n",
    "[Step 110/1000] Immediate Loss: 3.474279747009277 Accumlated Loss: 3.586595432281494 Duration: 0.34798717498779297\n",
    "[Step 120/1000] Immediate Loss: 3.6556751251220705 Accumlated Loss: 3.5458038244247434 Duration: 0.38567328453063965\n",
    "[Step 130/1000] Immediate Loss: 3.4842983436584474 Accumlated Loss: 3.5041985340118402 Duration: 0.35711145401000977\n",
    "[Step 140/1000] Immediate Loss: 3.3172739982604984 Accumlated Loss: 3.470182712554932 Duration: 0.39105868339538574\n",
    "[Step 150/1000] Immediate Loss: 3.4829155921936037 Accumlated Loss: 3.4747222614288327 Duration: 0.3634517192840576\n",
    "[Step 160/1000] Immediate Loss: 3.342771110534668 Accumlated Loss: 3.420569646835328 Duration: 0.3930521011352539\n",
    "[Step 170/1000] Immediate Loss: 3.3771132946014406 Accumlated Loss: 3.3877188377380376 Duration: 0.3480498790740967\n",
    "[Step 180/1000] Immediate Loss: 3.387749195098877 Accumlated Loss: 3.410190097808838 Duration: 0.42006444931030273\n",
    "[Step 190/1000] Immediate Loss: 3.393511924743653 Accumlated Loss: 3.387726984024048 Duration: 0.35900235176086426\n",
    "[Step 200/1000] Immediate Loss: 3.293798923492432 Accumlated Loss: 3.370459241867066 Duration: 0.3927791118621826\n",
    "[Step 210/1000] Immediate Loss: 3.2775422096252442 Accumlated Loss: 3.3615310697555545 Duration: 0.356921911239624\n",
    "[Step 220/1000] Immediate Loss: 3.3820526409149165 Accumlated Loss: 3.3239399595260624 Duration: 0.3690330982208252\n",
    "[Step 230/1000] Immediate Loss: 3.253921890258789 Accumlated Loss: 3.351316024780273 Duration: 0.40285444259643555\n",
    "[Step 240/1000] Immediate Loss: 3.3662335586547854 Accumlated Loss: 3.3590779170989986 Duration: 0.3788950443267822\n",
    "[Step 250/1000] Immediate Loss: 3.3439441299438477 Accumlated Loss: 3.3786073141098028 Duration: 0.3920586109161377\n",
    "[Step 260/1000] Immediate Loss: 3.354782133102417 Accumlated Loss: 3.3513808755874637 Duration: 0.35993361473083496\n",
    "[Step 270/1000] Immediate Loss: 3.3372807884216313 Accumlated Loss: 3.338691305160523 Duration: 0.39347100257873535\n",
    "[Step 280/1000] Immediate Loss: 3.310175018310547 Accumlated Loss: 3.2986322603225706 Duration: 0.4072144031524658\n",
    "[Step 290/1000] Immediate Loss: 3.333268260955811 Accumlated Loss: 3.3248736991882324 Duration: 0.41123247146606445\n",
    "[Step 300/1000] Immediate Loss: 3.3544892120361323 Accumlated Loss: 3.326479442596436 Duration: 0.4039759635925293\n",
    "[Step 310/1000] Immediate Loss: 3.346428356170654 Accumlated Loss: 3.305378651618958 Duration: 0.42134761810302734\n",
    "[Step 320/1000] Immediate Loss: 3.403144226074219 Accumlated Loss: 3.311773896217346 Duration: 0.36122655868530273\n",
    "[Step 330/1000] Immediate Loss: 3.2763449478149416 Accumlated Loss: 3.3266497364044185 Duration: 0.3582150936126709\n",
    "[Step 340/1000] Immediate Loss: 3.266353530883789 Accumlated Loss: 3.333148896217346 Duration: 0.42594099044799805\n",
    "[Step 350/1000] Immediate Loss: 3.279413528442383 Accumlated Loss: 3.2995057687759397 Duration: 0.3749394416809082\n",
    "[Step 360/1000] Immediate Loss: 3.213767147064209 Accumlated Loss: 3.308307558059693 Duration: 0.3870062828063965\n",
    "[Step 370/1000] Immediate Loss: 3.37831901550293 Accumlated Loss: 3.3509619054794317 Duration: 0.3554391860961914\n",
    "[Step 380/1000] Immediate Loss: 3.2749361038208002 Accumlated Loss: 3.320907657623291 Duration: 0.43279170989990234\n",
    "[Step 390/1000] Immediate Loss: 3.2810496330261225 Accumlated Loss: 3.328427023887634 Duration: 0.35605382919311523\n",
    "[Step 400/1000] Immediate Loss: 3.278467082977295 Accumlated Loss: 3.317427125930786 Duration: 0.38791894912719727\n",
    "[Step 410/1000] Immediate Loss: 3.3440698432922367 Accumlated Loss: 3.326253881454468 Duration: 0.3630564212799072\n",
    "[Step 420/1000] Immediate Loss: 3.3230349349975588 Accumlated Loss: 3.3114357585906977 Duration: 0.4106309413909912\n",
    "[Step 430/1000] Immediate Loss: 3.313874320983887 Accumlated Loss: 3.316153571128845 Duration: 0.39663028717041016\n",
    "[Step 440/1000] Immediate Loss: 3.3600057220458988 Accumlated Loss: 3.301804824829101 Duration: 0.3836090564727783\n",
    "[Step 450/1000] Immediate Loss: 3.286161613464355 Accumlated Loss: 3.3144787635803223 Duration: 0.40917134284973145\n",
    "[Step 460/1000] Immediate Loss: 3.3025293159484863 Accumlated Loss: 3.2997208251953123 Duration: 0.38283514976501465\n",
    "[Step 470/1000] Immediate Loss: 3.2771983528137203 Accumlated Loss: 3.291448328971863 Duration: 0.4188551902770996\n",
    "[Step 480/1000] Immediate Loss: 3.287361907958984 Accumlated Loss: 3.3076985902786253 Duration: 0.38308286666870117\n",
    "[Step 490/1000] Immediate Loss: 3.350705604553223 Accumlated Loss: 3.3191831941604613 Duration: 0.4051980972290039\n",
    "[Step 500/1000] Immediate Loss: 3.269703922271729 Accumlated Loss: 3.2964485731124875 Duration: 0.36760973930358887\n",
    "[Step 510/1000] Immediate Loss: 3.309275913238525 Accumlated Loss: 3.3095148010253905 Duration: 0.3980100154876709\n",
    "[Step 520/1000] Immediate Loss: 3.280012950897217 Accumlated Loss: 3.3023848466873167 Duration: 0.35692524909973145\n",
    "[Step 530/1000] Immediate Loss: 3.2583084297180176 Accumlated Loss: 3.3132524414062496 Duration: 0.40805792808532715\n",
    "[Step 540/1000] Immediate Loss: 3.249091358184814 Accumlated Loss: 3.3023037862777707 Duration: 0.3657839298248291\n",
    "[Step 550/1000] Immediate Loss: 3.3021725273132323 Accumlated Loss: 3.2992365531921384 Duration: 0.3607676029205322\n",
    "[Step 560/1000] Immediate Loss: 3.2874881362915036 Accumlated Loss: 3.307847092628479 Duration: 0.40618443489074707\n",
    "[Step 570/1000] Immediate Loss: 3.2956336784362787 Accumlated Loss: 3.313754090309143 Duration: 0.36110687255859375\n",
    "[Step 580/1000] Immediate Loss: 3.308529033660889 Accumlated Loss: 3.307110284805298 Duration: 0.4040539264678955\n",
    "[Step 590/1000] Immediate Loss: 3.266728000640869 Accumlated Loss: 3.3071045789718623 Duration: 0.36758947372436523\n",
    "[Step 600/1000] Immediate Loss: 3.3427077102661134 Accumlated Loss: 3.319442165374756 Duration: 0.3978755474090576\n",
    "[Step 610/1000] Immediate Loss: 3.277811832427979 Accumlated Loss: 3.3092873697280885 Duration: 0.368602991104126\n",
    "[Step 620/1000] Immediate Loss: 3.3398999786376953 Accumlated Loss: 3.3137764816284183 Duration: 0.388408899307251\n",
    "[Step 630/1000] Immediate Loss: 3.3554999160766603 Accumlated Loss: 3.300262495994568 Duration: 0.370255708694458\n",
    "[Step 640/1000] Immediate Loss: 3.32398515701294 Accumlated Loss: 3.3128390302658084 Duration: 0.40645265579223633\n",
    "[Step 650/1000] Immediate Loss: 3.3210130500793458 Accumlated Loss: 3.298892056465149 Duration: 0.35892391204833984\n",
    "[Step 660/1000] Immediate Loss: 3.2175397109985355 Accumlated Loss: 3.2880573740005494 Duration: 0.3479282855987549\n",
    "[Step 670/1000] Immediate Loss: 3.351421165466309 Accumlated Loss: 3.3231223983764653 Duration: 0.43328332901000977\n",
    "[Step 680/1000] Immediate Loss: 3.2576738166809083 Accumlated Loss: 3.3032870035171507 Duration: 0.35775184631347656\n",
    "[Step 690/1000] Immediate Loss: 3.272449645996094 Accumlated Loss: 3.309213233947754 Duration: 0.3954291343688965\n",
    "[Step 700/1000] Immediate Loss: 3.270857238769531 Accumlated Loss: 3.2929638547897335 Duration: 0.36011576652526855\n",
    "[Step 710/1000] Immediate Loss: 3.2790110588073733 Accumlated Loss: 3.315713575363159 Duration: 0.42752623558044434\n",
    "[Step 720/1000] Immediate Loss: 3.299638538360596 Accumlated Loss: 3.3011173324584955 Duration: 0.366910457611084\n",
    "[Step 730/1000] Immediate Loss: 3.2682429313659664 Accumlated Loss: 3.306383940696717 Duration: 0.39806294441223145\n",
    "[Step 740/1000] Immediate Loss: 3.2332065010070803 Accumlated Loss: 3.303490586280823 Duration: 0.37712717056274414\n",
    "[Step 750/1000] Immediate Loss: 3.310677337646484 Accumlated Loss: 3.3010164909362794 Duration: 0.41501855850219727\n",
    "[Step 760/1000] Immediate Loss: 3.266561450958252 Accumlated Loss: 3.300159492492676 Duration: 0.3900594711303711\n",
    "[Step 770/1000] Immediate Loss: 3.2882065200805664 Accumlated Loss: 3.2848971471786497 Duration: 0.3660550117492676\n",
    "[Step 780/1000] Immediate Loss: 3.322351398468018 Accumlated Loss: 3.3257057313919067 Duration: 0.42407703399658203\n",
    "[Step 790/1000] Immediate Loss: 3.3259357833862304 Accumlated Loss: 3.3010821838378903 Duration: 0.3699007034301758\n",
    "[Step 800/1000] Immediate Loss: 3.3515318870544433 Accumlated Loss: 3.3092310037612913 Duration: 0.39723944664001465\n",
    "[Step 810/1000] Immediate Loss: 3.367356910705566 Accumlated Loss: 3.315722069740296 Duration: 0.35505223274230957\n",
    "[Step 820/1000] Immediate Loss: 3.2537025356292726 Accumlated Loss: 3.301430205345154 Duration: 0.39604759216308594\n",
    "[Step 830/1000] Immediate Loss: 3.3313663482666014 Accumlated Loss: 3.3168755073547365 Duration: 0.35784435272216797\n",
    "[Step 840/1000] Immediate Loss: 3.374281883239746 Accumlated Loss: 3.3063914518356325 Duration: 0.40817952156066895\n",
    "[Step 850/1000] Immediate Loss: 3.2705587673187257 Accumlated Loss: 3.2939389181137084 Duration: 0.35893774032592773\n",
    "[Step 860/1000] Immediate Loss: 3.2662834167480472 Accumlated Loss: 3.323611269950867 Duration: 0.4060525894165039\n",
    "[Step 870/1000] Immediate Loss: 3.299567708969116 Accumlated Loss: 3.2936978788375857 Duration: 0.4847078323364258\n",
    "[Step 880/1000] Immediate Loss: 3.3145069885253906 Accumlated Loss: 3.3072527265548706 Duration: 0.4159691333770752\n",
    "[Step 890/1000] Immediate Loss: 3.288979301452637 Accumlated Loss: 3.311298152923584 Duration: 0.4421226978302002\n",
    "[Step 900/1000] Immediate Loss: 3.331672554016113 Accumlated Loss: 3.2962228116989136 Duration: 0.37487053871154785\n",
    "[Step 910/1000] Immediate Loss: 3.349849491119385 Accumlated Loss: 3.3247341480255126 Duration: 0.40424036979675293\n",
    "[Step 920/1000] Immediate Loss: 3.3153474044799807 Accumlated Loss: 3.298115752220154 Duration: 0.36104893684387207\n",
    "[Step 930/1000] Immediate Loss: 3.347795543670654 Accumlated Loss: 3.29688000869751 Duration: 0.405078649520874\n",
    "[Step 940/1000] Immediate Loss: 3.3006763458251953 Accumlated Loss: 3.3067226610183713 Duration: 0.3592236042022705\n",
    "[Step 950/1000] Immediate Loss: 3.3473744583129887 Accumlated Loss: 3.31758397769928 Duration: 0.39299511909484863\n",
    "[Step 960/1000] Immediate Loss: 3.2901748847961425 Accumlated Loss: 3.3202925367355345 Duration: 0.3634631633758545\n",
    "[Step 970/1000] Immediate Loss: 3.2751164627075195 Accumlated Loss: 3.2988116960525513 Duration: 0.40302443504333496\n",
    "[Step 980/1000] Immediate Loss: 3.306729278564453 Accumlated Loss: 3.3053677740097047 Duration: 0.3580911159515381\n",
    "[Step 990/1000] Immediate Loss: 3.3456609630584717 Accumlated Loss: 3.323902424812317 Duration: 0.3670032024383545\n",
    "[Step 1000/1000] Immediate Loss: 3.2466304969787596 Accumlated Loss: 3.308351089477539 Duration: 0.5125195980072021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "### gmade ###\n",
    "[Step 10/100] Immediate Loss: 6.091697503328321 Accumlated Loss: 11.86150044506788\n",
    "[Step 20/100] Immediate Loss: 4.033280954957009 Accumlated Loss: 4.759111630380153\n",
    "[Step 30/100] Immediate Loss: 3.729229698777201 Accumlated Loss: 3.9092313872575764\n",
    "[Step 40/100] Immediate Loss: 3.6316562518477444 Accumlated Loss: 3.7232759437263008\n",
    "[Step 50/100] Immediate Loss: 3.5912016203999513 Accumlated Loss: 3.5814507492780683\n",
    "[Step 60/100] Immediate Loss: 3.409006274640559 Accumlated Loss: 3.4948545873165124\n",
    "[Step 70/100] Immediate Loss: 3.545635481178759 Accumlated Loss: 3.461448216050863\n",
    "[Step 80/100] Immediate Loss: 3.5296939200162885 Accumlated Loss: 3.425615511506796\n",
    "[Step 90/100] Immediate Loss: 3.399731188714504 Accumlated Loss: 3.419999708920717\n",
    "[Step 100/100] Immediate Loss: 3.2696896982193002 Accumlated Loss: 3.3859521727561956\n",
    "\n",
    "# gpu\n",
    "[Step 10/1000] Immediate Loss: 28.645474891662598 Accumlated Loss: 32.46923538351059 Duration: 0.3118765354156494\n",
    "[Step 20/1000] Immediate Loss: 20.140543217658998 Accumlated Loss: 24.561776025891305 Duration: 0.2937607765197754\n",
    "[Step 30/1000] Immediate Loss: 16.37481050491333 Accumlated Loss: 19.07333705496788 Duration: 0.33535194396972656\n",
    "[Step 40/1000] Immediate Loss: 13.368425827026368 Accumlated Loss: 15.38671475982666 Duration: 0.29599928855895996\n",
    "[Step 50/1000] Immediate Loss: 11.016235466003419 Accumlated Loss: 12.108031059265137 Duration: 0.3342249393463135\n",
    "[Step 60/1000] Immediate Loss: 8.497132759094239 Accumlated Loss: 9.264692452430726 Duration: 0.2967672348022461\n",
    "[Step 70/1000] Immediate Loss: 6.468829097747803 Accumlated Loss: 7.156724786758423 Duration: 0.32948970794677734\n",
    "[Step 80/1000] Immediate Loss: 5.5583078575134275 Accumlated Loss: 5.902008189201355 Duration: 0.29119873046875\n",
    "[Step 90/1000] Immediate Loss: 4.963556032180787 Accumlated Loss: 5.233694943428039 Duration: 0.293048620223999\n",
    "[Step 100/1000] Immediate Loss: 4.868153781890869 Accumlated Loss: 4.961175120353699 Duration: 0.3373074531555176\n",
    "[Step 110/1000] Immediate Loss: 4.747272834777832 Accumlated Loss: 4.717371043205261 Duration: 0.29387354850769043\n",
    "[Step 120/1000] Immediate Loss: 4.77202657699585 Accumlated Loss: 4.654214656829835 Duration: 0.32922959327697754\n",
    "[Step 130/1000] Immediate Loss: 4.455893688201904 Accumlated Loss: 4.513361931800842 Duration: 0.3361806869506836\n",
    "[Step 140/1000] Immediate Loss: 4.603406753540039 Accumlated Loss: 4.418698398590088 Duration: 0.3919994831085205\n",
    "[Step 150/1000] Immediate Loss: 4.205460004806518 Accumlated Loss: 4.3214278402328485 Duration: 0.30796360969543457\n",
    "[Step 160/1000] Immediate Loss: 4.27139030456543 Accumlated Loss: 4.299418837547302 Duration: 0.3729991912841797\n",
    "[Step 170/1000] Immediate Loss: 4.136210622787476 Accumlated Loss: 4.180789088249207 Duration: 0.3121047019958496\n",
    "[Step 180/1000] Immediate Loss: 4.182266654968262 Accumlated Loss: 4.158940057754516 Duration: 0.34903693199157715\n",
    "[Step 190/1000] Immediate Loss: 4.09123836517334 Accumlated Loss: 4.077604190826416 Duration: 0.3060595989227295\n",
    "[Step 200/1000] Immediate Loss: 3.8515351581573487 Accumlated Loss: 3.9907477827072144 Duration: 0.3020462989807129\n",
    "[Step 210/1000] Immediate Loss: 3.7652649116516117 Accumlated Loss: 3.938784395217896 Duration: 0.33886003494262695\n",
    "[Step 220/1000] Immediate Loss: 3.9395818519592285 Accumlated Loss: 3.8765062685012817 Duration: 0.30915260314941406\n",
    "[Step 230/1000] Immediate Loss: 3.8621268272399902 Accumlated Loss: 3.8862210855484007 Duration: 0.3399207592010498\n",
    "[Step 240/1000] Immediate Loss: 3.8003242492675784 Accumlated Loss: 3.870174863815307 Duration: 0.29604506492614746\n",
    "[Step 250/1000] Immediate Loss: 3.6445272636413573 Accumlated Loss: 3.779785706520081 Duration: 0.3441014289855957\n",
    "[Step 260/1000] Immediate Loss: 3.7637029457092286 Accumlated Loss: 3.727572990417481 Duration: 0.2986483573913574\n",
    "[Step 270/1000] Immediate Loss: 3.6157450866699223 Accumlated Loss: 3.674282705307007 Duration: 0.33304715156555176\n",
    "[Step 280/1000] Immediate Loss: 3.5875861167907717 Accumlated Loss: 3.649505294799805 Duration: 0.3106265068054199\n",
    "[Step 290/1000] Immediate Loss: 3.4999898147583 Accumlated Loss: 3.5941883020401004 Duration: 0.3620624542236328\n",
    "[Step 300/1000] Immediate Loss: 3.567591018676758 Accumlated Loss: 3.583481592178345 Duration: 0.29308223724365234\n",
    "[Step 310/1000] Immediate Loss: 3.5853019905090333 Accumlated Loss: 3.5462128639221193 Duration: 0.30051088333129883\n",
    "[Step 320/1000] Immediate Loss: 3.5454703330993653 Accumlated Loss: 3.5123749027252193 Duration: 0.347367525100708\n",
    "[Step 330/1000] Immediate Loss: 3.5508033180236818 Accumlated Loss: 3.5439950218200686 Duration: 0.3043985366821289\n",
    "[Step 340/1000] Immediate Loss: 3.493808183670044 Accumlated Loss: 3.516240322113037 Duration: 0.33291149139404297\n",
    "[Step 350/1000] Immediate Loss: 3.5204606819152833 Accumlated Loss: 3.4875489130020143 Duration: 0.29442644119262695\n",
    "[Step 360/1000] Immediate Loss: 3.487391319274902 Accumlated Loss: 3.4969412460327147 Duration: 0.33286619186401367\n",
    "[Step 370/1000] Immediate Loss: 3.4867223739624023 Accumlated Loss: 3.47367154598236 Duration: 0.2952587604522705\n",
    "[Step 380/1000] Immediate Loss: 3.404178600311279 Accumlated Loss: 3.4313887786865243 Duration: 0.34160327911376953\n",
    "[Step 390/1000] Immediate Loss: 3.4566253662109374 Accumlated Loss: 3.4635467624664305 Duration: 0.30103230476379395\n",
    "[Step 400/1000] Immediate Loss: 3.4031524658203125 Accumlated Loss: 3.4423334703445434 Duration: 0.3343679904937744\n",
    "[Step 410/1000] Immediate Loss: 3.437589797973633 Accumlated Loss: 3.445955638885498 Duration: 0.2934551239013672\n",
    "[Step 420/1000] Immediate Loss: 3.4305254745483396 Accumlated Loss: 3.467087458610535 Duration: 0.2950420379638672\n",
    "[Step 430/1000] Immediate Loss: 3.4124325561523436 Accumlated Loss: 3.3959003705978392 Duration: 0.3310115337371826\n",
    "[Step 440/1000] Immediate Loss: 3.4083506584167482 Accumlated Loss: 3.393881471633911 Duration: 0.29466772079467773\n",
    "[Step 450/1000] Immediate Loss: 3.4347219371795656 Accumlated Loss: 3.39964142036438 Duration: 0.3309810161590576\n",
    "[Step 460/1000] Immediate Loss: 3.3265656852722167 Accumlated Loss: 3.3675982246398926 Duration: 0.296555757522583\n",
    "[Step 470/1000] Immediate Loss: 3.4610227775573725 Accumlated Loss: 3.400850156784058 Duration: 0.33401060104370117\n",
    "[Step 480/1000] Immediate Loss: 3.37685188293457 Accumlated Loss: 3.399056836128235 Duration: 0.29609203338623047\n",
    "[Step 490/1000] Immediate Loss: 3.4419313430786134 Accumlated Loss: 3.3897217569351197 Duration: 0.33090662956237793\n",
    "[Step 500/1000] Immediate Loss: 3.4230207633972167 Accumlated Loss: 3.4003475246429447 Duration: 0.30504751205444336\n",
    "[Step 510/1000] Immediate Loss: 3.395289058685303 Accumlated Loss: 3.3760693769454955 Duration: 0.3329806327819824\n",
    "[Step 520/1000] Immediate Loss: 3.3966053771972655 Accumlated Loss: 3.3817456083297732 Duration: 0.2982442378997803\n",
    "[Step 530/1000] Immediate Loss: 3.3084476661682127 Accumlated Loss: 3.353052099227906 Duration: 0.2963106632232666\n",
    "[Step 540/1000] Immediate Loss: 3.4057489967346193 Accumlated Loss: 3.339466228485108 Duration: 0.3337278366088867\n",
    "[Step 550/1000] Immediate Loss: 3.3545660018920898 Accumlated Loss: 3.377721154212952 Duration: 0.29999327659606934\n",
    "[Step 560/1000] Immediate Loss: 3.3552959823608397 Accumlated Loss: 3.343118812561035 Duration: 0.33011841773986816\n",
    "[Step 570/1000] Immediate Loss: 3.3844991874694825 Accumlated Loss: 3.3770285129547117 Duration: 0.30004382133483887\n",
    "[Step 580/1000] Immediate Loss: 3.429487190246582 Accumlated Loss: 3.349548743247986 Duration: 0.3329911231994629\n",
    "[Step 590/1000] Immediate Loss: 3.2413233947753906 Accumlated Loss: 3.3280337438583376 Duration: 0.3239881992340088\n",
    "[Step 600/1000] Immediate Loss: 3.3326664733886715 Accumlated Loss: 3.3676852083206175 Duration: 0.36179113388061523\n",
    "[Step 610/1000] Immediate Loss: 3.395525283813477 Accumlated Loss: 3.3513045186996466 Duration: 0.31592345237731934\n",
    "[Step 620/1000] Immediate Loss: 3.268419303894043 Accumlated Loss: 3.3398091859817503 Duration: 0.3591301441192627\n",
    "[Step 630/1000] Immediate Loss: 3.3351130962371824 Accumlated Loss: 3.3343322324752807 Duration: 0.3169856071472168\n",
    "[Step 640/1000] Immediate Loss: 3.3750020599365236 Accumlated Loss: 3.3320983238220214 Duration: 0.32368016242980957\n",
    "[Step 650/1000] Immediate Loss: 3.3008910751342775 Accumlated Loss: 3.3469146718978884 Duration: 0.3561289310455322\n",
    "[Step 660/1000] Immediate Loss: 3.3249346923828123 Accumlated Loss: 3.3422493209838864 Duration: 0.3186657428741455\n",
    "[Step 670/1000] Immediate Loss: 3.4729016685485834 Accumlated Loss: 3.3565485239028927 Duration: 0.3653910160064697\n",
    "[Step 680/1000] Immediate Loss: 3.325834655761719 Accumlated Loss: 3.3186508769989014 Duration: 0.3211078643798828\n",
    "[Step 690/1000] Immediate Loss: 3.3627269935607913 Accumlated Loss: 3.3128015460968014 Duration: 0.36841797828674316\n",
    "[Step 700/1000] Immediate Loss: 3.2416428565979007 Accumlated Loss: 3.3147759170532227 Duration: 0.3240361213684082\n",
    "[Step 710/1000] Immediate Loss: 3.239976062774658 Accumlated Loss: 3.306152648925781 Duration: 0.35913729667663574\n",
    "[Step 720/1000] Immediate Loss: 3.331307716369629 Accumlated Loss: 3.342268701553345 Duration: 0.3161740303039551\n",
    "[Step 730/1000] Immediate Loss: 3.3111047172546386 Accumlated Loss: 3.3177096233367918 Duration: 0.3901174068450928\n",
    "[Step 740/1000] Immediate Loss: 3.361885032653808 Accumlated Loss: 3.3334077072143558 Duration: 0.32385969161987305\n",
    "[Step 750/1000] Immediate Loss: 3.29816819190979 Accumlated Loss: 3.3066765327453616 Duration: 0.31812286376953125\n",
    "[Step 760/1000] Immediate Loss: 3.29841796875 Accumlated Loss: 3.310022190093994 Duration: 0.34715795516967773\n",
    "[Step 770/1000] Immediate Loss: 3.281107082366943 Accumlated Loss: 3.2933409452438354 Duration: 0.33165979385375977\n",
    "[Step 780/1000] Immediate Loss: 3.3082717132568358 Accumlated Loss: 3.3196176013946532 Duration: 0.35816287994384766\n",
    "[Step 790/1000] Immediate Loss: 3.3504421806335447 Accumlated Loss: 3.32738166809082 Duration: 0.31704187393188477\n",
    "[Step 800/1000] Immediate Loss: 3.2881481647491455 Accumlated Loss: 3.308452456474304 Duration: 0.38905787467956543\n",
    "[Step 810/1000] Immediate Loss: 3.284136838912964 Accumlated Loss: 3.314943544387817 Duration: 0.322110652923584\n",
    "[Step 820/1000] Immediate Loss: 3.305176105499267 Accumlated Loss: 3.3037649545669554 Duration: 0.3619554042816162\n",
    "[Step 830/1000] Immediate Loss: 3.393955955505371 Accumlated Loss: 3.3367141866683956 Duration: 0.29704999923706055\n",
    "[Step 840/1000] Immediate Loss: 3.2559880638122563 Accumlated Loss: 3.30268783569336 Duration: 0.33498573303222656\n",
    "[Step 850/1000] Immediate Loss: 3.2791462993621825 Accumlated Loss: 3.3085220441818235 Duration: 0.29592418670654297\n",
    "[Step 860/1000] Immediate Loss: 3.281724548339844 Accumlated Loss: 3.322854890823364 Duration: 0.3008260726928711\n",
    "[Step 870/1000] Immediate Loss: 3.3175796890258793 Accumlated Loss: 3.301014678001404 Duration: 0.3460514545440674\n",
    "[Step 880/1000] Immediate Loss: 3.326539611816406 Accumlated Loss: 3.2972715778350827 Duration: 0.2960472106933594\n",
    "[Step 890/1000] Immediate Loss: 3.2491430473327636 Accumlated Loss: 3.3055799140930175 Duration: 0.33775854110717773\n",
    "[Step 900/1000] Immediate Loss: 3.3498123550415038 Accumlated Loss: 3.3090501632690428 Duration: 0.31011414527893066\n",
    "[Step 910/1000] Immediate Loss: 3.3080376243591307 Accumlated Loss: 3.3173300933837893 Duration: 0.33286476135253906\n",
    "[Step 920/1000] Immediate Loss: 3.3105238723754886 Accumlated Loss: 3.3006394176483154 Duration: 0.2990453243255615\n",
    "[Step 930/1000] Immediate Loss: 3.348530197143555 Accumlated Loss: 3.3052138490676874 Duration: 0.3319988250732422\n",
    "[Step 940/1000] Immediate Loss: 3.2989852142333986 Accumlated Loss: 3.3112360324859615 Duration: 0.29785895347595215\n",
    "[Step 950/1000] Immediate Loss: 3.321433353424072 Accumlated Loss: 3.3080471029281613 Duration: 0.3328404426574707\n",
    "[Step 960/1000] Immediate Loss: 3.2815662574768067 Accumlated Loss: 3.304311319351197 Duration: 0.29705333709716797\n",
    "[Step 970/1000] Immediate Loss: 3.318554363250732 Accumlated Loss: 3.295420694351196 Duration: 0.295973539352417\n",
    "[Step 980/1000] Immediate Loss: 3.3074472808837894 Accumlated Loss: 3.3034933023452764 Duration: 0.32967591285705566\n",
    "[Step 990/1000] Immediate Loss: 3.2701464557647704 Accumlated Loss: 3.3209096412658683 Duration: 0.2958359718322754\n",
    "[Step 1000/1000] Immediate Loss: 3.313778133392334 Accumlated Loss: 3.2882585115432743 Duration: 0.3350646495819092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guide 5\n",
    "[Step 10/100] Immediate Loss: 5.436824112832545 Accumlated Loss: 7.459522048622368\n",
    "[Step 20/100] Immediate Loss: 4.522601895034313 Accumlated Loss: 4.697016129463912\n",
    "[Step 30/100] Immediate Loss: 4.035572609305381 Accumlated Loss: 4.087585103064775\n",
    "[Step 40/100] Immediate Loss: 3.9674540817737576 Accumlated Loss: 3.922967136412859\n",
    "[Step 50/100] Immediate Loss: 3.819277352690697 Accumlated Loss: 3.8670592607259744\n",
    "[Step 60/100] Immediate Loss: 3.8970872554183003 Accumlated Loss: 3.8228334636390207\n",
    "[Step 70/100] Immediate Loss: 3.6495595994591716 Accumlated Loss: 3.728844232827424\n",
    "[Step 80/100] Immediate Loss: 3.6761886674165725 Accumlated Loss: 3.7416907757222653\n",
    "[Step 90/100] Immediate Loss: 3.6880584159493455 Accumlated Loss: 3.6743881301581856\n",
    "[Step 100/100] Immediate Loss: 3.7141958668828003 Accumlated Loss: 3.684455200225115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guide 6\n",
    "[Step 10/100] Immediate Loss: 8.01269422084093 Accumlated Loss: 15.49425634944439\n",
    "[Step 20/100] Immediate Loss: 6.9067663696408275 Accumlated Loss: 7.9501663804352285\n",
    "[Step 30/100] Immediate Loss: 8.029220328330993 Accumlated Loss: 8.010903750896453\n",
    "[Step 40/100] Immediate Loss: 8.31351687759161 Accumlated Loss: 8.1114898557961\n",
    "[Step 50/100] Immediate Loss: 8.786913882493973 Accumlated Loss: 7.755306751757858\n",
    "[Step 60/100] Immediate Loss: 7.783528136909008 Accumlated Loss: 7.7925652467012405\n",
    "[Step 70/100] Immediate Loss: 8.006914387047292 Accumlated Loss: 7.681327867358923\n",
    "[Step 80/100] Immediate Loss: 8.374721429347991 Accumlated Loss: 7.807788733184337\n",
    "[Step 90/100] Immediate Loss: 7.715019945800301 Accumlated Loss: 7.808754653304815\n",
    "[Step 100/100] Immediate Loss: 8.256794970929624 Accumlated Loss: 7.728429790437221"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-analyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guide 7\n",
    "[Step 10/100] Immediate Loss: 7.946083300113675 Accumlated Loss: 15.880016953587534\n",
    "[Step 20/100] Immediate Loss: 6.707426010072232 Accumlated Loss: 7.429752558678389\n",
    "[Step 30/100] Immediate Loss: 6.904689866006373 Accumlated Loss: 7.198620594471693\n",
    "[Step 40/100] Immediate Loss: 7.047841712534428 Accumlated Loss: 7.0875664424300195\n",
    "[Step 50/100] Immediate Loss: 7.573134193122387 Accumlated Loss: 6.855033908337354\n",
    "[Step 60/100] Immediate Loss: 7.103918403685091 Accumlated Loss: 6.874631712168455\n",
    "[Step 70/100] Immediate Loss: 6.944367031455041 Accumlated Loss: 6.938687078088522\n",
    "[Step 80/100] Immediate Loss: 7.446044615507124 Accumlated Loss: 7.044109266996383\n",
    "[Step 90/100] Immediate Loss: 6.661473557949067 Accumlated Loss: 6.902258327007294\n",
    "[Step 100/100] Immediate Loss: 7.03185636729002 Accumlated Loss: 6.8993669908344755\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoNormal \n",
    "[Step 10/100] Immediate Loss: 29.661400873661044 Accumlated Loss: 34.381083221018315\n",
    "[Step 20/100] Immediate Loss: 21.721210796236996 Accumlated Loss: 24.125531654298307\n",
    "[Step 30/100] Immediate Loss: 20.717992586493505 Accumlated Loss: 20.629015110522506\n",
    "[Step 40/100] Immediate Loss: 20.31778262376786 Accumlated Loss: 20.401564517676828\n",
    "[Step 50/100] Immediate Loss: 21.517062507569797 Accumlated Loss: 20.607605726897724\n",
    "[Step 60/100] Immediate Loss: 20.558893247246754 Accumlated Loss: 20.390989627033466\n",
    "[Step 70/100] Immediate Loss: 20.57022605895995 Accumlated Loss: 20.47847183179855\n",
    "[Step 80/100] Immediate Loss: 19.823867598176 Accumlated Loss: 20.282766120791436\n",
    "[Step 90/100] Immediate Loss: 20.220103700757026 Accumlated Loss: 20.263966775059696\n",
    "[Step 100/100] Immediate Loss: 20.07956038057804 Accumlated Loss: 20.539366428911684"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoMultivariateNormal\n",
    "[Step 10/100] Immediate Loss: 29.280232212543496 Accumlated Loss: 34.96955828201771\n",
    "[Step 20/100] Immediate Loss: 20.823702557683003 Accumlated Loss: 23.777785731434825\n",
    "[Step 30/100] Immediate Loss: 17.56564211547375 Accumlated Loss: 18.975887567698955\n",
    "[Step 40/100] Immediate Loss: 18.09688677370549 Accumlated Loss: 18.158192227423196\n",
    "[Step 50/100] Immediate Loss: 17.358041861653323 Accumlated Loss: 17.932833659708496\n",
    "[Step 60/100] Immediate Loss: 17.660284592509267 Accumlated Loss: 17.789189765751363\n",
    "[Step 70/100] Immediate Loss: 16.889609196186065 Accumlated Loss: 17.35965327256918\n",
    "[Step 80/100] Immediate Loss: 18.56209426760674 Accumlated Loss: 17.78802405971289\n",
    "[Step 90/100] Immediate Loss: 17.714577720761305 Accumlated Loss: 17.483879809975623\n",
    "[Step 100/100] Immediate Loss: 16.55987268984317 Accumlated Loss: 17.230655351817603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoDiagonalNormal\n",
    "[Step 10/100] Immediate Loss: 29.594689139127723 Accumlated Loss: 35.09126758885384\n",
    "[Step 20/100] Immediate Loss: 22.305745403170594 Accumlated Loss: 24.653422838687895\n",
    "[Step 30/100] Immediate Loss: 19.765058637857443 Accumlated Loss: 20.882928909182546\n",
    "[Step 40/100] Immediate Loss: 20.646754955649378 Accumlated Loss: 20.50682894957065\n",
    "[Step 50/100] Immediate Loss: 20.2246539914608 Accumlated Loss: 20.506671326756482\n",
    "[Step 60/100] Immediate Loss: 21.094068167805677 Accumlated Loss: 20.755982849299905\n",
    "[Step 70/100] Immediate Loss: 20.336222262978545 Accumlated Loss: 20.427405071496963\n",
    "[Step 80/100] Immediate Loss: 21.357484986782072 Accumlated Loss: 20.727729279518126\n",
    "[Step 90/100] Immediate Loss: 21.060880010724066 Accumlated Loss: 20.713297606468206\n",
    "[Step 100/100] Immediate Loss: 20.076601029038425 Accumlated Loss: 20.360575951337818"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
