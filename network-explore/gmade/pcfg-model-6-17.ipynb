{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "optional-directory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(2.6941), tensor(3.9621), tensor(7.0929), tensor(4.9392), tensor(3.0060), tensor(9.0831), tensor(2.4896), tensor(3.2253), tensor(3.1595), tensor(3.1650), tensor(3.0045), tensor(4.2084), tensor(4.9492), tensor(0.2180), tensor(6.6041), tensor(2.8698), tensor(2.5025), tensor(5.6181), tensor(4.7291), tensor(3.3583), tensor(2.9780), tensor(3.4487), tensor(4.1549), tensor(4.6353), tensor(4.8174), tensor(2.3706), tensor(3.5672), tensor(3.5153), tensor(3.8304), tensor(8.4062), tensor(5.8252), tensor(6.0036), tensor(5.7325), tensor(2.3821), tensor(3.0960), tensor(4.2701), tensor(4.9106), tensor(3.6987), tensor(3.4630), tensor(2.7172), tensor(12.0344), tensor(2.7754), tensor(2.7467), tensor(3.2961), tensor(4.4028), tensor(2.1755), tensor(3.3162), tensor(3.5215), tensor(2.9892), tensor(2.5443), tensor(3.7411), tensor(4.3880), tensor(3.1752), tensor(3.0859), tensor(3.5000), tensor(4.3876), tensor(7.4913), tensor(2.8243), tensor(5.0054), tensor(4.0988), tensor(3.3010), tensor(4.6071), tensor(4.8085), tensor(2.0548), tensor(5.1801), tensor(3.3294), tensor(2.1954), tensor(4.0426), tensor(2.1471), tensor(5.1169), tensor(2.6197), tensor(2.5377), tensor(8.8469), tensor(4.1961), tensor(2.4460), tensor(2.5633), tensor(2.8962), tensor(4.6489), tensor(7.9131), tensor(3.8911), tensor(3.9866), tensor(6.9598), tensor(4.3034), tensor(1.7263), tensor(3.7826), tensor(4.2066), tensor(3.1125), tensor(5.6243), tensor(4.4282), tensor(7.9733), tensor(5.9792), tensor(6.5543), tensor(1.8312), tensor(7.9447), tensor(4.7770), tensor(4.2990), tensor(8.7907), tensor(3.7301), tensor(7.6739), tensor(5.8326)]\n",
      "hid_orderings [1, 0, 2, 2, 1, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1] size 16\n",
      "num hid layers: 1\n",
      "expanded_input_ordering [0, 1, 1, 1] size 4\n",
      "expanded_output_ordering [2] size 1\n",
      "number of levels: 1\n",
      "input_levels [['obs', 'h']]\n",
      "out_levels [['a']]\n",
      "hid_orderings [0, 5, 0, 6, 4, 5, 4, 5, 1, 0, 4, 5, 3, 6, 1, 4, 3, 1, 1, 1, 5, 3, 2, 3, 2, 1, 5, 6, 4, 1, 3, 3] size 32\n",
      "num hid layers: 1\n",
      "expanded_input_ordering [0, 1, 1, 1, 2] size 5\n",
      "expanded_output_ordering [6, 6] size 2\n",
      "number of levels: 1\n",
      "input_levels [['obs', 'h', 'a']]\n",
      "out_levels [['b']]\n",
      "[Step 10/200] Immediate Loss: 16.228585069775583 Accumlated Loss: 29.89437417051196\n",
      "[Step 20/200] Immediate Loss: 10.570998136997227 Accumlated Loss: 10.193143275767568\n",
      "[Step 30/200] Immediate Loss: 8.87859395009931 Accumlated Loss: 9.46797249697859\n",
      "[Step 40/200] Immediate Loss: 9.205837236614897 Accumlated Loss: 9.285295881732369\n",
      "[Step 50/200] Immediate Loss: 8.60184272795916 Accumlated Loss: 8.67534855375439\n",
      "[Step 60/200] Immediate Loss: 8.145570023544137 Accumlated Loss: 8.732611868691631\n",
      "[Step 70/200] Immediate Loss: 8.276699676327409 Accumlated Loss: 8.756063909554852\n",
      "[Step 80/200] Immediate Loss: 8.71482197921723 Accumlated Loss: 8.600668620096519\n",
      "[Step 90/200] Immediate Loss: 8.696509092785416 Accumlated Loss: 8.637636805855669\n",
      "[Step 100/200] Immediate Loss: 8.27016553352587 Accumlated Loss: 8.62791279060859\n",
      "[Step 110/200] Immediate Loss: 9.115066520506508 Accumlated Loss: 8.492126013375353\n",
      "[Step 120/200] Immediate Loss: 9.163098026737567 Accumlated Loss: 8.575263834489508\n",
      "[Step 130/200] Immediate Loss: 7.50167819555849 Accumlated Loss: 8.340214180591516\n",
      "[Step 140/200] Immediate Loss: 7.467777451891453 Accumlated Loss: 8.127204266864808\n",
      "[Step 150/200] Immediate Loss: 8.799595964476467 Accumlated Loss: 8.267007992542814\n",
      "[Step 160/200] Immediate Loss: 8.11897164894268 Accumlated Loss: 8.414821279149036\n",
      "[Step 170/200] Immediate Loss: 8.533070401169823 Accumlated Loss: 8.351124089608433\n",
      "[Step 180/200] Immediate Loss: 8.593473738376055 Accumlated Loss: 8.891616586138845\n",
      "[Step 190/200] Immediate Loss: 8.315683883382007 Accumlated Loss: 8.50586731727724\n",
      "[Step 200/200] Immediate Loss: 7.760830519634764 Accumlated Loss: 8.487844280509629\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAqFklEQVR4nO3deXxU9b3/8ddnsq9sWQwEBJRFUTajxbVatFWrQou2treWtla639r2thfbu3S7t1bban+1tUVrpVrXFq/WDRFFcQMCgoCELUBYQjaykT2Z7++POQkZEiAgk8lJ3s/HI4+ZOXMm88l3Tt7zne/3nDnmnENERPwnEO0CRETkxCjARUR8SgEuIuJTCnAREZ9SgIuI+JQCXETEpxTgIiI+pQCXfsnMdprZ5dGuQySSFOAiIj6lAJcBw8wSzOxuM9vn/dxtZgnefRlm9qyZVZnZATNbbmYB775/N7O9ZlZrZpvNbGZ0/xKRkNhoFyDSi34EzACmAg54GvgP4D+B7wF7gExv3RmAM7MJwDeBc51z+8xsNBDTu2WLdE89cBlI/gX4qXOu1DlXBvwEuMm7rwXIAU51zrU455a70BcFtQEJwJlmFuec2+mc2x6V6kUOowCXgWQ4sKvT7V3eMoA7gW3AS2ZWaGbzAZxz24BbgR8DpWb2mJkNR6QPUIDLQLIPOLXT7VHeMpxztc657znnxgLXAt9tH+t2zj3inLvIe6wDftm7ZYt0TwEu/VmcmSW2/wCPAv9hZplmlgH8F/AwgJldY2anm5kBNYSGTtrMbIKZfcSb7GwEGrz7RKJOAS792fOEArf9JxHIB94D1gNrgJ97644DXgYOAm8Df3DOLSM0/n07UA7sB7KAH/baXyByFKYTOoiI+JN64CIiPqUAFxHxKQW4iIhPKcBFRHyqVw+lz8jIcKNHj+7NpxQR8b3Vq1eXO+cyD1/eqwE+evRo8vPze/MpRUR8z8x2dbdcQygiIj6lABcR8SkFuIiITynARUR8SgEuIuJTCnAREZ/qUYCb2XfMbKOZbTCzR72v5xxqZkvMbKt3OSTSxYqIyCHHDHAzGwH8K5DnnDuL0PkAbwTmA0udc+OApd7tiFi6qYQ/LNsWqV8vIuJLPR1CiQWSzCwWSCZ0FpNZwELv/oXA7JNenWfZ5jLue70wUr9eRMSXjhngzrm9wK+AIqAYqHbOvQRkO+eKvXWKCX3RfRdmNs/M8s0sv6ys7ISKjAkYbUF9b7mISGc9GUIZQqi3PYbQCWBTzOxzPX0C59wC51yecy4vM7PLofw9K9IM5beISLieDKFcDuxwzpU551qARcAFQImZ5QB4l6WRKjImgHrgIiKH6UmAFwEzzCzZO+HrTGAT8Aww11tnLvB0ZEqEQMBo06nfRETCHPPbCJ1zK8zs74ROANsKvAssAFKBJ8zsZkIhf0OkiowxI6geuIhImB59naxz7r+B/z5scROh3njExagHLiLShS+OxAyY4Rw4hbiISAdfBHhMwABNZIqIdOarAG9VgIuIdPBVgAc1hCIi0sEfAW4aQhEROZwvAjzQ3gMPRrkQEZE+xBcBHhPKb+1KKCLSiT8CXHuhiIh04YsAD2gSU0SkC18EuCYxRUS68kWABzSEIiLShS8CvL0HriEUEZFD/BHg6oGLiHThiwDXJKaISFe+CPBDk5hRLkREpA/xR4B7VWoIRUTkEF8EeECTmCIiXfgiwDWJKSLS1TED3MwmmNnaTj81ZnarmQ01syVmttW7HBKxItsDXD1wEZEOxwxw59xm59xU59xU4BygHngKmA8sdc6NA5Z6tyOiYz9w9cBFRDoc7xDKTGC7c24XMAtY6C1fCMw+iXWF0RCKiEhXxxvgNwKPeteznXPFAN5lVncPMLN5ZpZvZvllZWUnVqRpCEVE5HA9DnAziweuA548nidwzi1wzuU55/IyMzOPtz6g0ynVtB+4iEiH4+mBXwWscc6VeLdLzCwHwLssPdnFtTt0UmMluIhIu+MJ8M9waPgE4Blgrnd9LvD0ySrqcDqpsYhIVz0KcDNLBq4AFnVafDtwhZlt9e67/eSXF6JD6UVEuortyUrOuXpg2GHLKgjtlRJxAR1KLyLSha+OxNQQiojIIf4IcJ1STUSkC18EuL4PXESkK18EuHrgIiJd+SPAdSi9iEgXvghwDaGIiHTliwDXfuAiIl35IsA79gNXD1xEpIMvAlzfBy4i0pU/AlyTmCIiXfgiwDWJKSLSlS8CXPuBi4h05Y8A10mNRUS68EWABzSJKSLShS8C/NAkZpQLERHpQ3wR4F5+awhFRKQTXwS4mREwDaGIiHTmiwAHiA0EaFWAi4h06Ok5MQeb2d/NrMDMNpnZ+WY21MyWmNlW73JIRAsNaD9wEZHOetoD/y3wonNuIjAF2ATMB5Y658YBS73bERNjpv3ARUQ6OWaAm1k6cAnwZwDnXLNzrgqYBSz0VlsIzI5MiSGBgAJcRKSznvTAxwJlwF/M7F0zu9/MUoBs51wxgHeZ1d2DzWyemeWbWX5ZWdkJFxoTMA2hiIh00pMAjwWmA/c656YBdRzHcIlzboFzLs85l5eZmXmCZWoIRUTkcD0J8D3AHufcCu/23wkFeomZ5QB4l6WRKTEkoB64iEiYYwa4c24/sNvMJniLZgLvA88Ac71lc4GnI1KhRz1wEZFwsT1c71vA38wsHigEvkgo/J8ws5uBIuCGyJQYEhMwHUovItJJjwLcObcWyOvmrpkntZqj0H7gIiLhfHMkpoZQRETC+SbAAwHTl1mJiHTimwCPMdOXWYmIdOKfANeRmCIiYXwT4AHTfuAiIp35JsDVAxcRCeebAA9NYka7ChGRvsM3AR6jM/KIiITxT4BrCEVEJIxvAjxg2g9cRKQz3wR4bIx64CIinfkmwAM6lF5EJIxvAlxn5BERCeefAFcPXEQkjG8CXCc1FhEJ55sAj9Gh9CIiYfwT4OqBi4iE8U2Ah05qHO0qRET6jh6dUs3MdgK1QBvQ6pzLM7OhwOPAaGAn8CnnXGVkygwdSq8euIjIIcfTA7/MOTfVOdd+bsz5wFLn3DhgqXc7YjSJKSIS7oMMocwCFnrXFwKzP3A1R6FJTBGRcD0NcAe8ZGarzWyetyzbOVcM4F1mdfdAM5tnZvlmll9WVnbChWoSU0QkXI/GwIELnXP7zCwLWGJmBT19AufcAmABQF5e3gkncEBHYoqIhOlRD9w5t8+7LAWeAs4DSswsB8C7LI1UkaAjMUVEDnfMADezFDNLa78OfBTYADwDzPVWmws8HakiQUMoIiKH68kQSjbwlJm1r/+Ic+5FM1sFPGFmNwNFwA2RK7P9pMaRfAYREX85ZoA75wqBKd0srwBmRqKo7sQEtB+4iEhnvjoSU2fkERE5xDcBHmOmkxqLiHTinwBXD1xEJIxvAjxghnOoFy4i4vFNgMcGDEC9cBERj28CPNAe4OqBi4gAPgrwGC/AdTi9iEiIfwLc1AMXEenMNwHePoQSDEa5EBGRPsI3AR4Tym9NYoqIePwT4JrEFBEJ45sAD2gSU0QkjG8CXJOYIiLhfBPg2g9cRCScbwK8vQeuIRQRkRD/BLh64CIiYXwT4JrEFBEJ55sAPzSJGeVCRET6iB4HuJnFmNm7Zvasd3uomS0xs63e5ZDIlRk6pRpoCEVEpN3x9MC/DWzqdHs+sNQ5Nw5Y6t2OmIAmMUVEwvQowM0sF/g4cH+nxbOAhd71hcDsk1rZYTSJKSISrqc98LuBHwCdR6CznXPFAN5lVncPNLN5ZpZvZvllZWUnXqhO6CAiEuaYAW5m1wClzrnVJ/IEzrkFzrk851xeZmbmifwKoNN+4OqBi4gAENuDdS4ErjOzq4FEIN3MHgZKzCzHOVdsZjlAaSQL1RCKiEi4Y/bAnXO3OedynXOjgRuBV5xznwOeAeZ6q80Fno5YlRyaxFSAi4iEfJD9wG8HrjCzrcAV3u2IiY3RGLiISGc9GULp4JxbBizzrlcAM09+Sd1TD1xEJJx/jsTUofQiImH8E+A6lF5EJIxvAjygQ+lFRML4JsA1hCIiEs4/Aa5JTBGRML4JcH0fuIhION8EuHrgIiLh/BPgOpReRCSMbwJcQygiIuF8E+DaD1xEJJxvArxjP3D1wEVEAB8FuL4PXEQknH8CXJOYIiJhfBPgcd5p6X+zZAtfeSg/ytWIiESfbwI8JSGWO+ZMZnLuIBZvLKGyrjnaJYmIRJVvAhzgU+eO5OaLxgBQWF4X5WpERKLLVwEOMDYzFYDCsoNRrkREJLp8F+C5Q5KIDRg71AMXkQHumAFuZolmttLM1pnZRjP7ibd8qJktMbOt3uWQyJcbmswcNSxZAS4iA15PeuBNwEecc1OAqcCVZjYDmA8sdc6NA5Z6t3vF2IwUCssU4CIysB0zwF1I+4BznPfjgFnAQm/5QmB2JArsztjMVHZU1OmgHhEZ0Ho0Bm5mMWa2FigFljjnVgDZzrliAO8yK2JVHmZMRgrNrUH2VjX01lOKiPQ5PQpw51ybc24qkAucZ2Zn9fQJzGyemeWbWX5ZWdkJlhlubEYKgMbBRWRAO669UJxzVcAy4EqgxMxyALzL0iM8ZoFzLs85l5eZmfnBqvWMyQwFuHYlFJGBrCd7oWSa2WDvehJwOVAAPAPM9VabCzwdoRq7yExN4JT0RN7YVtFbTyki0ufE9mCdHGChmcUQCvwnnHPPmtnbwBNmdjNQBNwQwTrDmBnXTsnhwbd2UlXfzODk+N56ahGRPuOYAe6cew+Y1s3yCmBmJIrqiVlTR3Df8h28sGE/nzlvVLTKEBGJGt8didlu0vB0xmam8ET+bjbsrdbXzIrIgOPbADcz5kzP5d2iKq753Rv89uUt0S5JRKRX+TbAAb5yyVie+voFDB+UyHbtUigiA4yvAzw2JsC0UUPIHZpMWU1TtMsREelVvg7wdllpCZTWNka7DBGRXtUvAjw7PZGSmiaczlgvIgNIvwjwrLQEGlraONjUGu1SRER6Tf8I8PQEAEprNQ4uIgNHvwjw7LREAEo1kSkiA0i/CPBDPXBNZIrIwNEvAjxTPXARGYD6RYCnJ8aSGBdQD1xEBpR+EeBmRlZaaFdCEZGBol8EOOhgHhEZePpPgKcnaDdCERlQ+k+ApyVqElNEBpT+E+DpCRxsaqW+WUdjisjA0H8CXLsSisgA05OTGo80s1fNbJOZbTSzb3vLh5rZEjPb6l0OiXy5R5atw+lFZIDpSQ+8Ffiec+4MYAbwDTM7E5gPLHXOjQOWerejpr0HXlKjPVFEZGA4ZoA754qdc2u867XAJmAEMAtY6K22EJgdoRp7JCtNPXARGViOawzczEYTOkP9CiDbOVcMoZAHsk56dcdhcHIc8TE6GlNEBo4eB7iZpQL/AG51ztUcx+PmmVm+meWXlZWdSI09fR4y0xI0iSkiA0aPAtzM4giF99+cc4u8xSVmluPdnwOUdvdY59wC51yecy4vMzPzZNR8RKGDedQDF5GBoSd7oRjwZ2CTc+43ne56BpjrXZ8LPH3yyzs+2TqYR0QGkJ70wC8EbgI+YmZrvZ+rgduBK8xsK3CFdzuqstITtBeKiAwYscdawTn3BmBHuHvmyS3ng8lKS6CmsZXGljYS42KiXY6ISET1myMxAbLSQ/uCl2lXQhEZAPpXgKfp1GoiMnD0swBvPxpTPXAR6f/6V4C3fx+KJjJFZADoVwE+NDme2IB1HE7/3SfW8vtXt0W5KhGRyOhXAR4IGKcMSmTt7irW76lm0Zq9LN1UEu2yREQiol8FOMAXLhjNW9sruPXxdwHYXdkQ5YpERCKj3wX4588fzdiMFLaX1ZGWEEtZbRONLW3RLktEetG63VV8+k9v9/v//X4X4PGxAX466yzGZqTw9ctOB2CPeuEiA8o7hRWs2HGAogP10S4lovpdgANcNC6DV/7tUs4dHTpJ0J7K/v0iiki4irpmAMr7+UF9/TLA2+UOSQY0Di4y0JQfDAV32UEFuG9lpSUQHxtQD1xkgKk4GOqB9/ev1ejXAR4IGLmDk9hzQD1wkYGkoi4U3OVekPdX/TrAAUYMSVIPXGSAUQ+8nxg5NJndlQ3ctWQLD765I9rliEiEOec6ArxcY+D+ljskiQN1zfx26Vb+8tbOaJcjIhFW29RKc1sQUID73khvT5Tk+Bh2VdRT3dAS5YpEJJLae98JsQENofjdpRMy+c7l4/nlnMkAvL+vJsoViUgkHfAmMMdnp1FR10ww6KJcUeT0+wBPS4zj25ePY8bYYQBs3Fcd5Yq6t+T9EvZX62twRT6o9j1PJp6SRlvQUVnff/dE6clZ6R8ws1Iz29Bp2VAzW2JmW73LIZEt84PLTEsgOz2BjT3sga/edYD7lxfiXOjdu9UbU4uE4uoGbvlrPne8WBCx5xhI2l8zGZjah1AmnJIG9O9dCXvSA38QuPKwZfOBpc65ccBS73afd9bwQWzYG+qBN7a0cdeSLTy9di+VdeEv8CsFJXz2vhX8/LlNPL12H794fhPn3/4KVRF6J39xw34AXnq/xBdfvlNS09hnQ/LFDfuZ8pOX9GlmAKvwJi4PBXj0xsH/57n3+Z/n3o/Y7z9mgDvnXgcOHLZ4FrDQu74QmH1yy4qMScPT2V52kIbmNn7+3Pv8dulWvv3YWs75+RKuv/ctdh+op6q+ma8+vIbx2WlMyR3E/EXv8afXCymrbeKpd/eG/b5XN5dy4e2vsHRTCdtKa/nNki1U1x//JOkL6/eTFBfDwaZWlm0uO1l/bkRs2FvN+b9Y2vGmE2n7qxu5c3EBn73vnY4336N5aeN+ahpbeWRlUS9UJ5HWFnQ8sqKIg02tPX5MRV0z6YmxDB+cBERvX3DnHE+u3sM/1uyNWIfnRMfAs51zxQDeZdaRVjSzeWaWb2b5ZWXRDadJIwYRdHDzwlU8/E4RX75oDP/3jQv51kfG8d7eah54cwfLt5bT3Brkp7MmcecNUwgG4eJxGZw9YhCPrizCOUdLW5DGljb+8/82sLcqNPxx1W+X8/+WbuULD6486sZWXd/CnYsL+PrfVvP8+mJ2H6hn1a4DfPniMQxLieef7+074mOLqxs4UNf9p4C2oGP9nuoTmrBpCzquu+cNfvH8JuDoQxAPv7OLoIPn1hcf9/OciG8+soY/vlbIqp0H+MubO4+6rnOOtwsrAHhkRRHNrZEb9pLutQUdD765g71VJ+fo51cKSvnhU+t57DjekMsPNpGRmkBGakLH7aP524pd/GrxZgB+8cImPnf/ipMSuIXldVTVt3CgrpldFZE5mDA2Ir+1E+fcAmABQF5eXlQ/d188LoMbzx3Jy5tKmTZqMD+4ciLxsQGmjhzM+r3VvLyphJqGVgYnxzE5dzAxAePV719KZmoCi9bsYf6i9Vzzuzco2F/LxFPS2FPZwP2fz+PZ9/aREBvDOaOHcNui9Xzqj2/zs9mTAAPgzJx0kuJjKCw7yPV/fJvK+maGpcTz/Pr9mIFz8PHJOVTVt/DQO7t4bfNi5l0yln+dOa6j9rqmVmbd8yYjhybz96+ej5l13PdKQQk/e3YTO8rruOXiMfzo42cCULC/hje2lnOgrpm1u6swgzuun8IIr2fSbvHG/by3p5rCsjq+ffk4bn4wn8S4APd+7hz2VDbw3HvFFJYf5KYZp/LMun0EDF7bXEZza5D42O77AC1tQe5asoWlm0q5dGImN184hqz00EmnnXPsrKhnTEbKUV+vdburyN9VyX9ecyYFxTW8uGE/jS1n8dqWMmaMHcagpLiw9YsO1FNc3cjlZ2Tx8qZS/rluH3POye3RtlFd30JxTQPjs9Job9rObXwkf35jB+OyUrlkfCZV9c0cbGpl+KAkAoEjP7a4uoHZv3+ToIPs9ARyBiXxg49NYFx2Wo9qPdlW7TzAyCHJnDIo8QP/rjsWF/Cn1wpZtauS3392+nE91jnXpc2fWRfq0Ly2pYwvXzy2R7+n4mAzw1LjSU+MJf4YuxK2BR13v7yVstomcockcf/yHbQFHat3VZI3emiPa29tC/L02n28urmUWy8fz+lZqazZVdlx/5qiSkYfY3s/ESca4CVmluOcKzazHKD0ZBYVKcnxsdw+Z3K3G8oVZ2bzSkEpZbX7uPyMbGK8f8D2sLt2ynB+8UIBxdWNfGLaCJ5fX8y1U4Zz+ZnZXH5mdsfvGZIcz22L1jPn3rc7lsXFGLdcPJZXCkoJOsc/v3kRZ+Sk8/b2Cl4pCDXdhOw0vjXzdLLTE1hTVMVvlmwhKS6GC04fxqnDUljw2nZKa5sorW0if1cleacOoTXoKDpQzzf+9i65Q5L46JnZ3Ld8B7ExAQrLDrJ4Y+h0cgGDiaeks7uynk/+4U1uu+oMLhqXQUZqAs45/rBsG2mJsdQ2tvK9J9Z19GI/8Ye32FpSS5tzJMfF8My6fTgH37zsdO55dRtvF1bQFgwyY+wwkuMPbUo7y+v49mPvsm5PNVNyB/Hn5Tt4bXMZT339QpLiY/jzGzv4+XObuOvTU7jqrByeX18cCt+qRmqbWrhpxmjOP20Yf3lzB6kJsXwqL5d3i6p4cvUevvP4Wl7YsJ/PfmgU//uJs4FQILa3I8APrpzI7gMNfP/v69hUXMP8qyayvayOnz/3PvOvmsik4YPCXvu6plY+veBtCvbXMjg5jpbWICOGJPHwlz9EVloirW1BYmNCb1TOOf7r6Y2cnpVKdnoCP3v2fQYlxbHo6xdw0/0r2FfdyNCUeB784rlMzh1Mc2uQ+uZWUhNiO37HrxZvobKuhdnThlNW28Q7hRXMX7S+yxtzd3aU1/HAGzuYMnIw15+TS0tbkBizjjeMhuY2FrxeyA15uR1DCEezeON+vvrwanLSE3nyaxd0eXPvrLGljXte2caaokriYgJ8dFI215+TS0JsqHPy2KrdLHi9kIzUBBZv2M/+6sZjvim0t+2b28r54VPrue/zeYz33sjqm1t5+f0SYgPGyh0HaGxpwwwSYmOO+jsr6poYm5GKmTF8UCKvFJTy1Q+fRk1jC6kJsQzzeuYAq3dVUlbbRGzAmL9oPcnxMcSY8dA7u5g2aghNrW1h23Z3mluDfHrB27xbVEVMwHiloJS7Pj2VNUWVpCfGEnTwblEVn5zes87E8bCefFQws9HAs865s7zbdwIVzrnbzWw+MNQ594Nj/Z68vDyXn5//AUuOjNKaRs7736UA3Hn9ZG7IG9llnf3VjaQkxJCWGEdjSxuxAev4p+ysuqGF594rJistgaBzvLBhf8f4+V++eC6XTTjiiBMQ2qjnPbS6I5RS4mNoCTouHZ9J/q5KRg1NpvxgE7WNrQxKiqOmsYXFt17C4OQ4Pv2nd1i7u4qhKfF85ryRzL1gNBkpCQQCRsH+Gm5+MJ+9VQ0kxcXw8JfPo+hAPd95fB23f/Js/vjadnZW1DM2I4UbzxvJL14o4FPnjOT7V06guTXIF/6ykpSEWB69ZQZTf/oScTEBahtbmTM9l1/dMJnn1+/n7cJynlqzl9iYAL+cczZXnpXD61vKmPuXlXxi6gh+OvssPnzHq1TUNZMSH8OYzBQ27K3BjI43lPKDzYzPTmV7WR2fP/9U/vvaSbS2BTnvf5dyoK6ZgIXejFf+aCbJ8bHc8td8lrxfQlJcDKmJsaz84UxqGlq5/cUCHl1ZxC0Xj+Htwgo27K1haEo8nz//VFYUHqDsYBODk+IwC/0jf+fy8eypbCAxLsAT+XsYk5FCQlyAneV1/PFz5/ChscN4b08V193zJhA6eUjukCSKKupJiA3Q3Bbkex+dwMK3dpIQG+Az543i10u20NwaJHdIEvd8dnrHP/u8i8dy29VnAKFhqf/4vw385Qvn8uHxmbxfXMOWklpiAsaaXZWs2llJdUML1Q0tHcNz6YmxvHXbTL68cFXok9vNHyIzLYH/ee597lu+g7NGpPPgF8/jhfXFlB9sZkhyHB+fPJzMtEPhtWFvNdf/8S3GZKSyp7KejNQEHr1lBqcMSqS6oYXvP7mOMZkp3HZVqM77lxfy8+c2MSV3ENUNLeysqOfC04dxybhMfuntQXX12Tncevl4rrjrNb724dO48dxR5AxOpC3ouP2FAjLTErjp/FNJT4zjoXd28b/PbeK2qydy77LtFFc3Mmd6Lv9+1QR+8XwBcTHGE/l7+Pqlp/GHZdv57hXjuXfZdq4+O4f/+cRZJMZ1DfKSmkYu+9Uy5kzP5WezQ5/WbvlrPklxMVQ3tDB8UCKP3DKDNUWVpCbE8nZhBX9bUcRPr5vE/EXr+deZ46hpaOGRFUWMHJpESU0TP7z6DFITY2kLBpk1ZQQltY0s31LO9FMHc1pmKr9+aQv3vLqNX845m0vGZ/KVh1azo6yO9KQ4xmWn0toW2pXxuX+9+Kj/90djZqudc3ldlh8rwM3sUeBSIAMoAf4b+D/gCWAUUATc4Jw7fKKzi74c4ACzf/8ma3dXseKHM8lO/+AfJzt7fUsZtY2tfHxyTo/Wb2pt442t5TS1Blm6qZS1uytZ+KXz+Mfqvdz18hZGDE7izOHpLNtcyu8/O52PTjoFCPUmKw42M3JoUre9ubagY8Peam59fC1ltU3UNbcybeRgHp03g/uX7+DOxZv57Y1TmTV1BAebQj3HdsGgo7ktSGJcDF97eDUvbyrhvDFDeXNbBR+ZmMUrBaWkJcQy47Rh/OS6SWE9wLtf3sLdL29lxOAk9lY18LvPTOOHT63HOfjVDZP5yMRs4mMDNDS38afXt7N+TzWZaQl894rxHUMvP/3n+zy9di8/mTWJbz7yLndcP5nJuYO48u7lTBqezsZ9NVw7ZTi/+8y0jue9bdF6HvXGT2+7aiJ/fmMHpbVNTBqezqihyRQdqGfjvhr+65oz+dJFYzoet3RTCbf8NZ+stEQS4wLsq2rk9/8ynde2lPJk/h4+OX0Ez64r5smvnc+iNXtZ8Hoh/3nNmdx80RhWFFbwmfveIejgsgmZXHBaBg++tbNjXHhoSjyv/tulHUNAza1BZv5mGTUNrTjnqGk8NIeSEBvgQ2OHkZWWQHpiHFnpCYzJSOErD63m4nEZLN9aTsBgTEYKN+SN5I4XC5g2agird1USGzBaO82JxAaMcdlpzBg7lH/76ATm3PsWlfXNPPutiyk6UMfcB1YxJCWOL14whsdX7WZzSS0AD918HueNGcold7zKmIwUHpt3Ps45/rFmLz/4+zqCDj42KZufzTqr47X60oOrOjogYzJSGJYST743pJCeGMu1U4bz2KrdJMfHUNvYSsBgxthhrNp5gKkjB7NqZ2jdnEGJLP3eh5n20yU0tQYZlBRHdUMLU0YO5k+fO4fXt5SxfFs5H5mYyehhKfz4n++ztaSWp75+YcdeKG9sLef/Ld3KjNOGsfCtnRxsaqXNa5fk+BguPD2DBTedw9rdVUzOHcyO8jquvPt1Rg1NJiM1gZU7D0Xb5WdksXZ3VceuicNS4qmsb2bO9FzuvGEKEPoE+rG7X6epNcj3rhhPc1uQPyzbzvoff/SYvfkjOeEAP5n6eoAv3rifVTsO8B/XnBntUo6ovrmVJ1bt5hPTchmUHBf28f547D5Qz40L3mHS8HR+e+M0kuJjaGhu4+VNJXz87JyjjuFCaMy4tqmFjNQErv7tcgrL65h3yVjmXzmx28c653j4nV387LlNXHR6Bg984Vx2lteREBcgZ9CxP+pD6JNJQ0sbqQmxXHHX6wBkpMazfk81b87/CKt3VTIxJz1sGKCxpY0vPbiK3CFJ3HH9FCoONtHUGgx7czn8japdUUU9WekJNLa0MfeBlWwtPUjAjCvOzOauT0+lpS1IXEyAlrYgq3dV8qExQzveNP++eg91Ta3cNONUAgGjqr6Zv769i6Ep8Vw6IbPjZCPtXi0o5YE3dzByaDLnjBrC1FGDaQs6cockdftP/8k/vMmaoirGZ6fyk+vO4tuPvUtpbRNZaQks+e6H+fvqPazcUcHXLz2ds0cMorD8IE+9u5f1e2t4fUsZ2ekJlNQ0seCmczre/NftruLzD6ykuqGFYSnx3HnDZH7+7CaaWoNcMj6TR1cW8dcvnccl4zM76nhtSxm7Kur43IdODXvdC8sO8s91xQxOjuOvb+9kZ0U9v75hCqdnpfK7V7ayeGMJE7LTePwrM7jnlW2MGpbMJeMyuezXy3AOfjb7LKbkDiI5PobTs9L4/AMreWtbOU989XxKa5r47hNrcY6O7aHzjgN/uukcPub9TYfbsLeaX7+0mU9Oz+XZ9/axeGMJd396KrOnjQhbb29VA1lpCcSYsWxLKYOT43mnsII7XtzMqKHJ/HLOZArLD7K2qIqaxhbuvGEK6YmH5mR+/+o27ly8mcfmzaC+uZUvPZjPY/NmdBxQeLwU4NJFW9B1jPV/EDvK6ygoruGqs4/96aK0ppHUxNgT7om0e+69Yr735FoaW4J847LT+P7HJh5x3e7mPI7X/upGrr3nDcpqm3jklg9xwWkZH+j3fVAvbdzPvIdWdwzJOefYXlZHSkLMMd8Qn8zfzb//4z0+NukU7v3cOWH3HWxqpb6plcy0BMyM1bsq+fLCVVTWtzA5dxBPf+PC427LlrYglXXNHb1zgF0VdQxOju8yEf3jZzbS0NzG7XPODnueHeV17K9u5PzTQgFYsL+GHy5az8wzsvnKJWPZuK+GA/XNnJKeyBk56T2qq7UtyModB5gxdtgxOyztNuytZuTQ5C51Hy4YdOTvquTc0UOobmjhz2/sYM703BOeyFSAS78TDDrK65oYlpJwUt6IjmXD3mpeLSjlG5ed3uN/+EgqrWkMC8XjsaO8juGDE485IQihdt5X3cCgpDjSEo8eXBIZRwrwiO9GKBIpgYCRlXZy5yqO5qwRgzhrxKBjr9hLTjS8gWPuwtlZIGBdhnykb+j3X2YlItJfKcBFRHxKAS4i4lMKcBERn1KAi4j4lAJcRMSnFOAiIj6lABcR8alePRLTzMqAXSf48Ayg/CSWc7L01bqg79amuo5PX60L+m5t/a2uU51zmYcv7NUA/yDMLL+7Q0mjra/WBX23NtV1fPpqXdB3axsodWkIRUTEpxTgIiI+5acAXxDtAo6gr9YFfbc21XV8+mpd0HdrGxB1+WYMXEREwvmpBy4iIp0owEVEfMoXAW5mV5rZZjPbZmbzo1jHSDN71cw2mdlGM/u2t/zHZrbXzNZ6P1dHobadZrbee/58b9lQM1tiZlu9yyG9XNOETm2y1sxqzOzWaLWXmT1gZqVmtqHTsiO2kZnd5m1zm83sY71c151mVmBm75nZU2Y22Fs+2swaOrXdH3u5riO+dlFur8c71bTTzNZ6y3uzvY6UD5HbxpxzffoHiAG2A2OBeGAdcGaUaskBpnvX04AtwJnAj4F/i3I77QQyDlt2BzDfuz4f+GWUX8f9wKnRai/gEmA6sOFYbeS9ruuABGCMtw3G9GJdHwViveu/7FTX6M7rRaG9un3tot1eh93/a+C/otBeR8qHiG1jfuiBnwdsc84VOueagceAWdEoxDlX7Jxb412vBTYBI47+qKiaBSz0ri8EZkevFGYC251zJ3ok7gfmnHsdOHDY4iO10SzgMedck3NuB7CN0LbYK3U5515yzrWfav0dIDcSz328dR1FVNurnYXOhPwp4NFIPPfRHCUfIraN+SHARwC7O93eQx8ITTMbDUwDVniLvul93H2gt4cqPA54ycxWm9k8b1m2c64YQhsXkBWFutrdSPg/VbTbq92R2qgvbXdfAl7odHuMmb1rZq+Z2cVRqKe7166vtNfFQIlzbmunZb3eXoflQ8S2MT8EeHen/47qvo9mlgr8A7jVOVcD3AucBkwFigl9hOttFzrnpgNXAd8ws0uiUEO3zCweuA540lvUF9rrWPrEdmdmPwJagb95i4qBUc65acB3gUfMLL0XSzrSa9cn2gv4DOEdhV5vr27y4YirdrPsuNrMDwG+BxjZ6XYusC9KtWBmcYRenL855xYBOOdKnHNtzrkgcB8R+uh4NM65fd5lKfCUV0OJmeV4decApb1dl+cqYI1zrsSrMert1cmR2ijq252ZzQWuAf7FeYOm3sftCu/6akLjpuN7q6ajvHZ9ob1igU8Cj7cv6+326i4fiOA25ocAXwWMM7MxXk/uRuCZaBTija/9GdjknPtNp+U5nVb7BLDh8MdGuK4UM0trv05oAmwDoXaa6602F3i6N+vqJKxXFO32OsyR2ugZ4EYzSzCzMcA4YGVvFWVmVwL/DlznnKvvtDzTzGK862O9ugp7sa4jvXZRbS/P5UCBc25P+4LebK8j5QOR3MZ6Y3b2JMzuXk1oRnc78KMo1nERoY847wFrvZ+rgYeA9d7yZ4CcXq5rLKHZ7HXAxvY2AoYBS4Gt3uXQKLRZMlABDOq0LCrtRehNpBhoIdT7uflobQT8yNvmNgNX9XJd2wiNj7ZvZ3/01p3jvcbrgDXAtb1c1xFfu2i2l7f8QeCrh63bm+11pHyI2DamQ+lFRHzKD0MoIiLSDQW4iIhPKcBFRHxKAS4i4lMKcBERn1KAi4j4lAJcRMSn/j8hEtlzizxfKgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pyro\n",
    "import pyro.contrib.examples.polyphonic_data_loader as poly\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "from made import MADE\n",
    "from gmade import GMADE\n",
    "\n",
    "random.seed(234) # 123\n",
    "torch.manual_seed(234)\n",
    "\n",
    "# NN used for p(x | y)\n",
    "class simpleNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden=32, out_size=1, t=\"normal\", out_non_linear=None):\n",
    "        super().__init__()\n",
    "        self.t = t\n",
    "        self.out_non_linear = out_non_linear\n",
    "        self.hiddeen_layer = nn.Linear(input_size, hidden)\n",
    "        if t == \"normal\":\n",
    "            self.loc_layer = nn.Linear(hidden, out_size)\n",
    "            self.std_layer = nn.Linear(hidden, out_size)\n",
    "            self.softplus = nn.Softplus()\n",
    "        elif t == \"bern\":\n",
    "            self.prob_layer = nn.Linear(hidden, out_size)\n",
    "        elif t == \"mlp\":\n",
    "            self.out_layer = nn.Linear(hidden, out_size)\n",
    "        \n",
    "    def forward(self, x_list):\n",
    "        for i in range(len(x_list)):\n",
    "            if x_list[i].dim() == 0:\n",
    "                x_list[i] = torch.unsqueeze(x_list[i], dim=0)\n",
    "        input_x = torch.cat(x_list)\n",
    "        hid = F.relu(self.hiddeen_layer(input_x))\n",
    "        # return loc, std\n",
    "        if self.t == \"normal\":\n",
    "            return self.loc_layer(hid), self.softplus(self.std_layer(hid))\n",
    "        elif self.t == \"bern\":\n",
    "            return torch.sigmoid(self.prob_layer(hid))\n",
    "        else:\n",
    "            if self.out_non_linear == \"tanh\":\n",
    "                return torch.tanh(self.out_layer(hid))\n",
    "            else:\n",
    "                return self.out_layer(hid)\n",
    "        \n",
    "class simpleRNN(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=32, max_l=5, max_t=3):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_size + max_l + max_t, hidden_size=hidden_size, nonlinearity='relu',\n",
    "                          batch_first=True, num_layers=1)\n",
    "        self.h_0 = nn.Parameter(torch.zeros((1, 1, hidden_size)))\n",
    "        self.out_loc = nn.Linear(hidden_size, 1)\n",
    "        self.out_std = nn.Linear(hidden_size, 1)\n",
    "        self.max_l = max_l # max time steps\n",
    "        self.max_t = max_t # type of random variables, ex. y1 y2 next_x\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "    def forward(self, x, obs, l, t):\n",
    "        \"\"\"\n",
    "        x: x0\n",
    "        obs: R\n",
    "        l: length\n",
    "        t: type, y1, y2 or next_x\n",
    "        \"\"\"\n",
    "        length = l * 3\n",
    "        input_x = x.repeat((int(length), 1))\n",
    "        input_obs = obs.repeat((int(length), 1))\n",
    "        input_l = []\n",
    "        input_t = []\n",
    "        \n",
    "        for n in range(int(l)):\n",
    "            \n",
    "            for i in range(int(t)):\n",
    "                input_l.append(n)\n",
    "                input_t.append(i)\n",
    "        input_l = F.one_hot(torch.tensor(input_l), self.max_l) \n",
    "        input_t = F.one_hot(torch.tensor(input_t), self.max_t) \n",
    "        \n",
    "        input_ = torch.unsqueeze(torch.cat([input_x, input_obs, input_l, input_t], -1), 0)\n",
    "        \n",
    "        # the input is [x, obs, onehot(l), onehot(t)]\n",
    "        rnn_output, _ = self.rnn(input_, self.h_0)\n",
    "        rnn_output = torch.squeeze(rnn_output, 0)\n",
    "        out_loc = self.out_loc(F.relu(rnn_output))\n",
    "        out_std = self.softplus(self.out_std(F.relu(rnn_output)))\n",
    "        # the first outputs are y_1_1, y_2_1, next_x_1; y_1_2, y_2_2, next_x_2\n",
    "        return torch.squeeze(out_loc, 1), torch.squeeze(out_std, 1) # shape l * t\n",
    "\n",
    "class Experiment(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_size = 3\n",
    "        self.max_rec = 5\n",
    "        \n",
    "        # for guide_0\n",
    "        self.h_a_nn_0 = simpleNN(input_size=self.hidden_size, t=\"bern\")\n",
    "        self.h_b_nn_0 = simpleNN(input_size=self.hidden_size)\n",
    "        self.h_encoder_0 = simpleNN(input_size=1 + self.hidden_size, out_size=self.hidden_size, t=\"mlp\")\n",
    "        self.h0_0 = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "        \n",
    "        \n",
    "        # for guide_1\n",
    "        self.h_a_nn_1 = simpleNN(input_size=1 + self.hidden_size, t=\"bern\")\n",
    "        self.h_b_nn_1 = simpleNN(input_size=1 + self.hidden_size)\n",
    "        self.h_encoder_1 = simpleNN(input_size=1 + self.hidden_size, out_size=self.hidden_size, t=\"mlp\")\n",
    "        self.h0_1 = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "        \n",
    "        # for guide_2\n",
    "        self.h_a_nn_2 = simpleNN(input_size=1 + self.hidden_size, t=\"bern\")\n",
    "        self.h_b_nn_2 = simpleNN(input_size=2 + self.hidden_size)\n",
    "        self.h_encoder_2 = simpleNN(input_size=2 + self.hidden_size, out_size=self.hidden_size, t=\"mlp\")\n",
    "        self.h0_2 = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "        \n",
    "        # for guide_made_0\n",
    "        self.hidden_size_made_0 = 3\n",
    "        self.made_in_dim_0_1 = 1 + self.hidden_size_made_0\n",
    "        self.num_var_0_1 = 1\n",
    "        self.made_out_dim_0_1 = self.made_in_dim_0_1 * self.num_var_0_1\n",
    "        self.made_hidden_0_1 = [32, 32]\n",
    "        self.made_0_1 = MADE(self.made_in_dim_0_1, self.made_hidden_0_1, self.made_out_dim_0_1, num_masks=1, natural_ordering=True)\n",
    "        self.made_map_a_0 = nn.Linear(self.made_out_dim_0_1, 1)\n",
    "    \n",
    "        self.made_in_dim_0_2 = 2 + self.hidden_size_made_0\n",
    "        self.num_var_0_2 = 1\n",
    "        self.made_out_dim_0_2 = self.made_in_dim_0_2 * self.num_var_0_2\n",
    "        self.made_hidden_0_2 = [32, 32]\n",
    "        self.made_0_2 = MADE(self.made_in_dim_0_2, self.made_hidden_0_2, self.made_out_dim_0_2, num_masks=1, natural_ordering=True)\n",
    "        self.made_map_b_loc_0 = nn.Linear(self.made_out_dim_0_2, 1)\n",
    "        self.made_map_b_std_0 = nn.Linear(self.made_out_dim_0_2, 1)\n",
    "        self.h_encoder_made_0 = simpleNN(input_size=2 + self.hidden_size, out_size=self.hidden_size, t=\"mlp\")\n",
    "        self.h0_made_0 = nn.Parameter(torch.zeros(self.hidden_size_made_0))\n",
    "        \n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "        # for guide_gmade\n",
    "        input_dim_dict1 = {\n",
    "            \"obs\" : 1,\n",
    "            \"h\" : self.hidden_size,\n",
    "        }\n",
    "        var_dim_dict1 = {\n",
    "            \"a\" : 1,\n",
    "        }\n",
    "        \n",
    "        dependency_dict1 = {\n",
    "            \"a\" : [\"h\", \"obs\"],\n",
    "        }\n",
    "        \n",
    "        dist_type_dict1 = {\n",
    "            \"a\" : \"bern\"\n",
    "        }\n",
    "        \n",
    "        \n",
    "        self.gmade1 = GMADE(input_dim_dict1, dependency_dict1, var_dim_dict1, dist_type_dict1)\n",
    "        \n",
    "        input_dim_dict2 = {\n",
    "            \"obs\" : 1,\n",
    "            \"h\" : self.hidden_size,\n",
    "            \"a\" : 1\n",
    "        }\n",
    "        var_dim_dict2 = {\n",
    "            \"b\" : 1,\n",
    "        }\n",
    "        \n",
    "        dependency_dict2 = {\n",
    "            \"b\" : [\"h\", \"obs\", \"a\"],\n",
    "        }\n",
    "        self.gmade2 = GMADE(input_dim_dict2, dependency_dict2, var_dim_dict2)\n",
    "        \n",
    "        self.h_encoder_gmade = simpleNN(input_size=1 + self.hidden_size, out_size=self.hidden_size, t=\"mlp\")\n",
    "        self.h0_gmade = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "\n",
    "    \n",
    "    def g(self, x):\n",
    "        return torch.tanh(x) \n",
    "    \n",
    "    def model(self, obs):\n",
    "        pyro.module(\"model\", self)\n",
    "        def rec_model(prefix, l):\n",
    "            a = pyro.sample(\"a_{}\".format(prefix), dist.Bernoulli(0.5))\n",
    "            if a > 0 or l > self.max_rec:\n",
    "                b = pyro.sample(\"b_{}\".format(prefix), dist.Normal(0, 1))\n",
    "                return b\n",
    "            else:\n",
    "                c = rec_model(prefix+\"c\", l + 1)\n",
    "                d = rec_model(prefix+\"d\", l + 1)\n",
    "                return c + d\n",
    "        sig = torch.tensor(0.5)\n",
    "        x = rec_model(\"root\", 0)\n",
    "        pyro.sample(\"obs\", dist.Normal(x, sig), obs=float(obs))\n",
    "    \n",
    "    # guide uses simple and individual NN for each random variable\n",
    "    def guide_0(self, obs):\n",
    "        pyro.module(\"model\", self)\n",
    "        def rec_guide(prefix, h, l):\n",
    "            a_prob = self.h_a_nn_0([h])\n",
    "            a = pyro.sample(\"a_{}\".format(prefix), dist.Bernoulli(a_prob))\n",
    "            if a > 0 or l > self.max_rec:\n",
    "                b_loc, b_std = self.h_b_nn_0([h])\n",
    "                return pyro.sample(\"b_{}\".format(prefix), dist.Normal(b_loc, b_std))\n",
    "            else:\n",
    "                c = rec_guide(prefix+\"c\", h, l + 1)\n",
    "                h = self.h_encoder_0([c,h])\n",
    "                d = rec_guide(prefix+\"d\", h, l + 1)\n",
    "                return c + d \n",
    "        rec_guide(\"root\", self.h0_0, 0)\n",
    "                \n",
    "    \n",
    "    # guide uses simple and individual NN for each random variable, add obs as dependency\n",
    "    def guide_1(self, obs):\n",
    "        pyro.module(\"model\", self)\n",
    "        def rec_guide(prefix, h, obs, l):\n",
    "            a_prob = self.h_a_nn_1([h, obs])\n",
    "            a = pyro.sample(\"a_{}\".format(prefix), dist.Bernoulli(a_prob))\n",
    "            if a > 0 or l > self.max_rec:\n",
    "                b_loc, b_std = self.h_b_nn_1([h, obs])\n",
    "                return pyro.sample(\"b_{}\".format(prefix), dist.Normal(b_loc, b_std))\n",
    "            else:\n",
    "                c = rec_guide(prefix+\"c\", h, obs, l + 1)\n",
    "                h = self.h_encoder_1([c,h])\n",
    "                d = rec_guide(prefix+\"d\", h, obs, l + 1)\n",
    "                return c + d \n",
    "        rec_guide(\"root\", self.h0_1, obs, 0)\n",
    "        \n",
    "    # guide uses simple and individual NN for each random variable, more dependency considered\n",
    "    def guide_2(self, obs):\n",
    "        pyro.module(\"model\", self)\n",
    "        def rec_guide(prefix, h, obs, l):\n",
    "            a_prob = self.h_a_nn_2([h, obs])\n",
    "            a = pyro.sample(\"a_{}\".format(prefix), dist.Bernoulli(a_prob))\n",
    "            if a > 0 or l > self.max_rec:\n",
    "                #b_loc, b_std = self.h_b_nn_2([h, obs, a_prob])\n",
    "                b_loc, b_std = self.h_b_nn_2([h, obs, a])\n",
    "                #b_loc, b_std = self.h_b_nn_2([h, obs, torch.tensor(0)])\n",
    "                return pyro.sample(\"b_{}\".format(prefix), dist.Normal(b_loc, b_std))\n",
    "            else:\n",
    "                c = rec_guide(prefix+\"c\", h, obs, l + 1)\n",
    "                #h = self.h_encoder_2([c,h,a])\n",
    "                h = self.h_encoder_2([c,h,torch.tensor(0)]) \n",
    "                d = rec_guide(prefix+\"d\", h, obs, l + 1)\n",
    "                return c + d \n",
    "        rec_guide(\"root\", self.h0_2, obs, 0)\n",
    "    \n",
    "    # guide uses simple and individual NN for each random variable, more dependency considered\n",
    "    def guide_gmade(self, obs):\n",
    "        pyro.module(\"model\", self)\n",
    "        def rec_guide(prefix, h, obs, l):\n",
    "            input_made_1 = {\n",
    "                \"h\" : h,\n",
    "                \"obs\" : obs\n",
    "            }\n",
    "            output_a = self.gmade1(input_made_1, suffix=\"_{}\".format(prefix))\n",
    "            a = output_a[\"a\"]\n",
    "            if a > 0 or l > self.max_rec:\n",
    "                input_made_2 = {\n",
    "                    \"h\" : h,\n",
    "                    \"obs\" : obs,\n",
    "                    \"a\" : a\n",
    "                }\n",
    "                output_b = self.gmade2(input_made_2, suffix=\"_{}\".format(prefix))\n",
    "                return output_b[\"b\"]\n",
    "            else:\n",
    "                c = rec_guide(prefix+\"c\", h, obs, l + 1)\n",
    "                h = self.h_encoder_gmade([c,h,torch.tensor(0)]) \n",
    "                d = rec_guide(prefix+\"d\", h, obs, l + 1)\n",
    "                return c + d \n",
    "        rec_guide(\"root\", self.h0_gmade, obs, 0)\n",
    "        \n",
    "    def guide_made_0(self, obs):\n",
    "        pyro.module(\"model\", self)\n",
    "        def concat_input_made(x_list):\n",
    "            for i in range(len(x_list)):\n",
    "                if x_list[i].dim() == 0:\n",
    "                    x_list[i] = torch.unsqueeze(x_list[i], dim=0)\n",
    "            input_x = torch.cat(x_list)\n",
    "            return input_x\n",
    "        def rec_guide(prefix, h, obs, l):\n",
    "            made_1_in = concat_input_made([h, obs])\n",
    "            made_1_out = self.made_0_1(made_1_in)\n",
    "            a_prob = torch.sigmoid(self.made_map_a_0(made_1_out))\n",
    "            a = pyro.sample(\"a_{}\".format(prefix), dist.Bernoulli(a_prob))\n",
    "            if a > 0 or l > self.max_rec:\n",
    "                made_2_in = concat_input_made([h, obs, a])\n",
    "                made_2_out = self.made_0_2(made_2_in)\n",
    "                b_loc = self.made_map_b_loc_0(made_2_out)\n",
    "                b_std = self.softplus(self.made_map_b_std_0(made_2_out))\n",
    "                return pyro.sample(\"b_{}\".format(prefix), dist.Normal(b_loc, b_std))\n",
    "            else:\n",
    "                c = rec_guide(prefix+\"c\", h, obs, l + 1)\n",
    "                h = self.h_encoder_made_0([c,h,a])\n",
    "                d = rec_guide(prefix+\"d\", h, obs, l + 1)\n",
    "                return c + d \n",
    "        rec_guide(\"root\", self.h0_made_0, obs, 0)\n",
    "    \n",
    "def generate_data():\n",
    "    # the actual data generation has three latent variables (y_1, y_2, y_3)\n",
    "    x0 = random.random() / 50\n",
    "    base_std = 0.356\n",
    "    def f(x0):\n",
    "        a = dist.Bernoulli(x0).sample()\n",
    "        if a > 0:\n",
    "            noise_mean = random.random() / 5 \n",
    "            if random.random() < 0.5:\n",
    "                noise_mean *= -1\n",
    "            noise_std = random.random() / 5\n",
    "            return dist.Normal(2 * x0 + noise_mean, base_std + noise_std).sample()\n",
    "            #return dist.Normal(2 + noise_mean, base_std + noise_std).sample()\n",
    "        else:\n",
    "            s = 0\n",
    "            for _ in range(2):\n",
    "                s += f(max(x0 + random.random() / 10, 0.75))\n",
    "            return s\n",
    "        \n",
    "    return f(x0)\n",
    "    \n",
    "data = []\n",
    "num_data = 100 # 100\n",
    "for _ in range(num_data):\n",
    "    data.append(generate_data())\n",
    "\n",
    "print(data)\n",
    "experiment = Experiment()\n",
    "adam_params = {\"lr\": 0.001, \"betas\": (0.95, 0.999)}\n",
    "optimizer = Adam(adam_params)\n",
    "guide = experiment.guide_2 # guide_1\n",
    "\n",
    "svi = SVI(experiment.model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "n_steps = 200\n",
    "log_interval = 10\n",
    "# do gradient steps\n",
    "loss = 0\n",
    "loss_track = []\n",
    "for step in range(1, n_steps + 1):\n",
    "    imme_loss = 0\n",
    "    \n",
    "    for obs in data:\n",
    "        imme_loss += svi.step(obs) / num_data\n",
    "        \n",
    "    loss_track.append(imme_loss)\n",
    "    loss += imme_loss / log_interval\n",
    "    \n",
    "    if step % log_interval == 0:\n",
    "        print(\"[Step {}/{}] Immediate Loss: {} Accumlated Loss: {}\".format(step, n_steps, imme_loss, loss))\n",
    "        loss = 0\n",
    "    \n",
    "plt.plot(loss_track)\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-paradise",
   "metadata": {},
   "source": [
    "results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "based-reader",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-cf15846e9d35>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-cf15846e9d35>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    [Step 10/200] Immediate Loss: 19.031159826517108 Accumlated Loss: 30.669728822827345\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# guide_0\n",
    "[Step 10/200] Immediate Loss: 19.031159826517108 Accumlated Loss: 30.669728822827345\n",
    "[Step 20/200] Immediate Loss: 16.54116699755191 Accumlated Loss: 16.42493181198835\n",
    "[Step 30/200] Immediate Loss: 15.830906611680986 Accumlated Loss: 15.341987950056792\n",
    "[Step 40/200] Immediate Loss: 14.625833902955055 Accumlated Loss: 15.328452881455421\n",
    "[Step 50/200] Immediate Loss: 15.429139192998413 Accumlated Loss: 15.12139590618014\n",
    "[Step 60/200] Immediate Loss: 16.700843995809553 Accumlated Loss: 15.193378344476224\n",
    "[Step 70/200] Immediate Loss: 13.782704436480998 Accumlated Loss: 14.926948586285116\n",
    "[Step 80/200] Immediate Loss: 13.849593170583253 Accumlated Loss: 14.773630363225937\n",
    "[Step 90/200] Immediate Loss: 14.682365895658728 Accumlated Loss: 14.936238646164536\n",
    "[Step 100/200] Immediate Loss: 14.949490445554256 Accumlated Loss: 15.427314028546215\n",
    "[Step 110/200] Immediate Loss: 15.533587679564953 Accumlated Loss: 15.366392598420385\n",
    "[Step 120/200] Immediate Loss: 15.174833083748815 Accumlated Loss: 15.251150453075768\n",
    "[Step 130/200] Immediate Loss: 15.296379088461395 Accumlated Loss: 15.222727478057147\n",
    "[Step 140/200] Immediate Loss: 14.548529184162614 Accumlated Loss: 15.24323720142245\n",
    "[Step 150/200] Immediate Loss: 15.327036719620224 Accumlated Loss: 15.089846912264827\n",
    "[Step 160/200] Immediate Loss: 15.66180207520723 Accumlated Loss: 15.366233869403601\n",
    "[Step 170/200] Immediate Loss: 15.747280451953404 Accumlated Loss: 14.878797188669443\n",
    "[Step 180/200] Immediate Loss: 14.456126456558705 Accumlated Loss: 14.763787083625793\n",
    "[Step 190/200] Immediate Loss: 14.843872060179715 Accumlated Loss: 14.933787340179087\n",
    "[Step 200/200] Immediate Loss: 15.440625265836713 Accumlated Loss: 15.287833866387603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guide_1\n",
    "[Step 10/100] Immediate Loss: 17.930261366367343 Accumlated Loss: 27.829307303488253\n",
    "[Step 20/100] Immediate Loss: 9.592198074162006 Accumlated Loss: 12.552848016187548\n",
    "[Step 30/100] Immediate Loss: 9.336000512987372 Accumlated Loss: 9.432053346917034\n",
    "[Step 40/100] Immediate Loss: 8.680979787409306 Accumlated Loss: 9.163381985932588\n",
    "[Step 50/100] Immediate Loss: 9.146411037594085 Accumlated Loss: 8.818429456993936\n",
    "[Step 60/100] Immediate Loss: 8.943090667575602 Accumlated Loss: 9.075083977945148\n",
    "[Step 70/100] Immediate Loss: 8.938816332668068 Accumlated Loss: 8.664682637892664\n",
    "[Step 80/100] Immediate Loss: 8.656451758295297 Accumlated Loss: 8.717697938293217\n",
    "[Step 90/100] Immediate Loss: 9.12393444098532 Accumlated Loss: 8.998925273619593\n",
    "[Step 100/100] Immediate Loss: 8.29359666161239 Accumlated Loss: 8.46837840466201\n",
    "\n",
    "# 234\n",
    "\n",
    "[Step 10/100] Immediate Loss: 37.67193449005484 Accumlated Loss: 44.051792191930126\n",
    "[Step 20/100] Immediate Loss: 13.063436024859547 Accumlated Loss: 25.60148323482275\n",
    "[Step 30/100] Immediate Loss: 14.368169421032077 Accumlated Loss: 13.656628071468322\n",
    "[Step 40/100] Immediate Loss: 13.22696690037847 Accumlated Loss: 13.200384394928818\n",
    "[Step 50/100] Immediate Loss: 10.82502632882446 Accumlated Loss: 12.54015972718969\n",
    "[Step 60/100] Immediate Loss: 13.607501493487508 Accumlated Loss: 12.602127239657566\n",
    "[Step 70/100] Immediate Loss: 11.873259286340327 Accumlated Loss: 12.261028830483557\n",
    "[Step 80/100] Immediate Loss: 12.663115918152029 Accumlated Loss: 13.14685623406735\n",
    "[Step 90/100] Immediate Loss: 11.50295409527375 Accumlated Loss: 12.109462206128752\n",
    "[Step 100/100] Immediate Loss: 11.703510694359432 Accumlated Loss: 11.744508737028577\n",
    "        \n",
    "[Step 10/100] Immediate Loss: 12.43674047522247 Accumlated Loss: 24.461878404676916\n",
    "[Step 20/100] Immediate Loss: 9.130857482701545 Accumlated Loss: 9.534772005297242\n",
    "[Step 30/100] Immediate Loss: 9.705919693857437 Accumlated Loss: 9.004167942252009\n",
    "[Step 40/100] Immediate Loss: 8.868569773547351 Accumlated Loss: 8.909939620696473\n",
    "[Step 50/100] Immediate Loss: 9.159001196846363 Accumlated Loss: 9.057822582449765\n",
    "[Step 60/100] Immediate Loss: 8.593080069683491 Accumlated Loss: 9.010244575401769\n",
    "[Step 70/100] Immediate Loss: 8.748249461222438 Accumlated Loss: 9.46440965727344\n",
    "[Step 80/100] Immediate Loss: 9.143552917158225 Accumlated Loss: 8.756269299504345\n",
    "[Step 90/100] Immediate Loss: 9.134051732858643 Accumlated Loss: 8.803603739945217\n",
    "[Step 100/100] Immediate Loss: 8.749371745344254 Accumlated Loss: 8.988685137513095"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guide_2\n",
    "[Step 10/100] Immediate Loss: 11.012404969632627 Accumlated Loss: 20.942087798953054\n",
    "[Step 20/100] Immediate Loss: 8.874061445668339 Accumlated Loss: 9.659853560857476\n",
    "[Step 30/100] Immediate Loss: 9.305319509208205 Accumlated Loss: 8.925286741890012\n",
    "[Step 40/100] Immediate Loss: 8.64788700006902 Accumlated Loss: 8.818340191334485\n",
    "[Step 50/100] Immediate Loss: 8.317036350667474 Accumlated Loss: 8.81623889724165\n",
    "[Step 60/100] Immediate Loss: 8.895614063516259 Accumlated Loss: 8.728870033808054\n",
    "[Step 70/100] Immediate Loss: 8.995070771798492 Accumlated Loss: 8.337308281101286\n",
    "[Step 80/100] Immediate Loss: 7.560207392536105 Accumlated Loss: 9.192471126087941\n",
    "[Step 90/100] Immediate Loss: 8.08053003266454 Accumlated Loss: 8.266035702221098\n",
    "[Step 100/100] Immediate Loss: 8.43317556051537 Accumlated Loss: 8.17740331849642\n",
    "        \n",
    "        \n",
    "# 234\n",
    "[Step 10/100] Immediate Loss: 25.302488480806336 Accumlated Loss: 47.37733794337512\n",
    "[Step 20/100] Immediate Loss: 12.527038377374414 Accumlated Loss: 14.296436548128723\n",
    "[Step 30/100] Immediate Loss: 11.079159877821807 Accumlated Loss: 12.580323178894819\n",
    "[Step 40/100] Immediate Loss: 12.077737506441768 Accumlated Loss: 12.565601924456656\n",
    "[Step 50/100] Immediate Loss: 10.861950867678967 Accumlated Loss: 11.70638570809271\n",
    "[Step 60/100] Immediate Loss: 12.157005536891514 Accumlated Loss: 12.30089523732243\n",
    "[Step 70/100] Immediate Loss: 10.823414814961144 Accumlated Loss: 11.282338644220028\n",
    "[Step 80/100] Immediate Loss: 11.061381861460394 Accumlated Loss: 11.803370690903334\n",
    "[Step 90/100] Immediate Loss: 10.065180069068443 Accumlated Loss: 10.749472737520465\n",
    "[Step 100/100] Immediate Loss: 11.362863518055528 Accumlated Loss: 10.915175796517637\n",
    "        \n",
    "[Step 10/100] Immediate Loss: 17.51159867525101 Accumlated Loss: 31.737257554948332\n",
    "[Step 20/100] Immediate Loss: 8.935020160079002 Accumlated Loss: 11.130399539723994\n",
    "[Step 30/100] Immediate Loss: 8.628836766034363 Accumlated Loss: 9.472301015660165\n",
    "[Step 40/100] Immediate Loss: 9.47374799415469 Accumlated Loss: 9.460159701734781\n",
    "[Step 50/100] Immediate Loss: 10.068971928637476 Accumlated Loss: 9.447906799232587\n",
    "[Step 60/100] Immediate Loss: 9.206475379317999 Accumlated Loss: 9.109603844319468\n",
    "[Step 70/100] Immediate Loss: 9.334056782852858 Accumlated Loss: 9.140527206042782\n",
    "[Step 80/100] Immediate Loss: 9.46767045147717 Accumlated Loss: 9.042547562962397\n",
    "[Step 90/100] Immediate Loss: 9.440853979745881 Accumlated Loss: 9.332269431842954\n",
    "[Step 100/100] Immediate Loss: 8.198536510840055 Accumlated Loss: 8.586438718704972"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guide_made_0\n",
    "[Step 10/100] Immediate Loss: 15.65785191923381 Accumlated Loss: 27.521529941409828\n",
    "[Step 20/100] Immediate Loss: 9.181662517264487 Accumlated Loss: 10.142460460945964\n",
    "[Step 30/100] Immediate Loss: 9.073848144486544 Accumlated Loss: 9.41119017122686\n",
    "[Step 40/100] Immediate Loss: 9.367795079164203 Accumlated Loss: 9.333177496946416\n",
    "[Step 50/100] Immediate Loss: 9.464715682566162 Accumlated Loss: 9.114056323527358\n",
    "[Step 60/100] Immediate Loss: 9.084946442395449 Accumlated Loss: 9.0807487567747\n",
    "[Step 70/100] Immediate Loss: 9.141939090248197 Accumlated Loss: 9.209436322024091\n",
    "[Step 80/100] Immediate Loss: 9.76587741822936 Accumlated Loss: 9.230823772877457\n",
    "[Step 90/100] Immediate Loss: 8.874032034681875 Accumlated Loss: 9.172223159402929\n",
    "[Step 100/100] Immediate Loss: 8.782549491748213 Accumlated Loss: 9.1371221883737"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-massage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-providence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-perception",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Step 10/100] Immediate Loss: 22.605579520165914 Accumlated Loss: 28.435255251377818\n",
    "[Step 20/100] Immediate Loss: 9.785503182411196 Accumlated Loss: 15.070935395404696\n",
    "[Step 30/100] Immediate Loss: 8.359215153157711 Accumlated Loss: 9.295412983365358\n",
    "[Step 40/100] Immediate Loss: 9.598168624043463 Accumlated Loss: 9.164385535761715\n",
    "[Step 50/100] Immediate Loss: 8.068674355596306 Accumlated Loss: 8.565269266165792\n",
    "[Step 60/100] Immediate Loss: 9.827182017378512 Accumlated Loss: 8.96327514716238\n",
    "[Step 70/100] Immediate Loss: 8.648304045051338 Accumlated Loss: 8.286241647139192\n",
    "[Step 80/100] Immediate Loss: 8.498466676548121 Accumlated Loss: 8.398653889864683\n",
    "[Step 90/100] Immediate Loss: 9.528026899769904 Accumlated Loss: 8.742412650257348\n",
    "[Step 100/100] Immediate Loss: 8.57554764084518 Accumlated Loss: 8.74125504555926"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-geology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-genome",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Step 10/100] Immediate Loss: 22.18946010574699 Accumlated Loss: 36.19197774453461\n",
    "[Step 20/100] Immediate Loss: 12.675996540933845 Accumlated Loss: 14.352158450521527\n",
    "[Step 30/100] Immediate Loss: 12.162181057333953 Accumlated Loss: 11.896226641595362\n",
    "[Step 40/100] Immediate Loss: 10.859737789183853 Accumlated Loss: 11.411551024988293\n",
    "[Step 50/100] Immediate Loss: 10.17903754532337 Accumlated Loss: 11.342433375878263\n",
    "[Step 60/100] Immediate Loss: 10.98338151283562 Accumlated Loss: 11.449062398713082\n",
    "[Step 70/100] Immediate Loss: 11.68075368477963 Accumlated Loss: 11.4911226944793\n",
    "[Step 80/100] Immediate Loss: 10.281905074808742 Accumlated Loss: 10.482128526570275\n",
    "[Step 90/100] Immediate Loss: 11.599871308188883 Accumlated Loss: 11.049202225252518\n",
    "[Step 100/100] Immediate Loss: 10.633230464896187 Accumlated Loss: 10.5171909355768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Step 10/100] Immediate Loss: 27.250790892541414 Accumlated Loss: 39.34954361453653\n",
    "[Step 20/100] Immediate Loss: 14.490461536590015 Accumlated Loss: 16.940787393787875\n",
    "[Step 30/100] Immediate Loss: 10.330058496221902 Accumlated Loss: 11.110658845644446\n",
    "[Step 40/100] Immediate Loss: 11.674540619114415 Accumlated Loss: 11.495626495875882\n",
    "[Step 50/100] Immediate Loss: 11.353992414437235 Accumlated Loss: 10.68051692174189\n",
    "[Step 60/100] Immediate Loss: 11.955793003980077 Accumlated Loss: 10.825556288091233\n",
    "[Step 70/100] Immediate Loss: 9.96266102346126 Accumlated Loss: 10.538895298123595\n",
    "[Step 80/100] Immediate Loss: 10.261633397508415 Accumlated Loss: 10.447733637062367\n",
    "[Step 90/100] Immediate Loss: 11.351145950374196 Accumlated Loss: 11.177116281141062\n",
    "[Step 100/100] Immediate Loss: 11.037443870757707 Accumlated Loss: 11.334670969774947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-helen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-father",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 200 data 200 epoch\n",
    "[Step 10/200] Immediate Loss: 14.280843336917462 Accumlated Loss: 30.271484799887986\n",
    "[Step 20/200] Immediate Loss: 11.139596355576066 Accumlated Loss: 12.005903967740945\n",
    "[Step 30/200] Immediate Loss: 10.581600938040067 Accumlated Loss: 11.166183316794573\n",
    "[Step 40/200] Immediate Loss: 10.511969740750061 Accumlated Loss: 11.206344725826794\n",
    "[Step 50/200] Immediate Loss: 11.558517323168637 Accumlated Loss: 11.276686911717333\n",
    "[Step 60/200] Immediate Loss: 12.449468495702613 Accumlated Loss: 10.837888258436449\n",
    "[Step 70/200] Immediate Loss: 10.83976251654065 Accumlated Loss: 10.30526414034594\n",
    "[Step 80/200] Immediate Loss: 11.33583993365872 Accumlated Loss: 11.325127883728566\n",
    "[Step 90/200] Immediate Loss: 9.601195529492689 Accumlated Loss: 10.51526922870631\n",
    "[Step 100/200] Immediate Loss: 9.9159266694646 Accumlated Loss: 9.988783735342635\n",
    "[Step 110/200] Immediate Loss: 10.135975952672311 Accumlated Loss: 11.722249362450212\n",
    "[Step 120/200] Immediate Loss: 10.119355863690748 Accumlated Loss: 9.860521333661383\n",
    "[Step 130/200] Immediate Loss: 10.703016270399235 Accumlated Loss: 9.809112476408114\n",
    "[Step 140/200] Immediate Loss: 9.343886604191217 Accumlated Loss: 10.112861371667607\n",
    "[Step 150/200] Immediate Loss: 9.68512336038068 Accumlated Loss: 9.60775408556044\n",
    "[Step 160/200] Immediate Loss: 9.574256338203233 Accumlated Loss: 9.990111750755483\n",
    "[Step 170/200] Immediate Loss: 10.19078243068201 Accumlated Loss: 10.630432617803534\n",
    "[Step 180/200] Immediate Loss: 9.601510525332436 Accumlated Loss: 9.806549738568277\n",
    "[Step 190/200] Immediate Loss: 9.367307784978802 Accumlated Loss: 9.753349011258662\n",
    "[Step 200/200] Immediate Loss: 9.728243387837932 Accumlated Loss: 9.62169008827907"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 200 data 200 epoch\n",
    "[Step 10/200] Immediate Loss: 11.604771429710095 Accumlated Loss: 20.87507799563184\n",
    "[Step 20/200] Immediate Loss: 12.088215978369114 Accumlated Loss: 12.050034406543245\n",
    "[Step 30/200] Immediate Loss: 10.791772004819071 Accumlated Loss: 12.347166594883543\n",
    "[Step 40/200] Immediate Loss: 12.014587712514906 Accumlated Loss: 12.539298477068076\n",
    "[Step 50/200] Immediate Loss: 11.286738041264472 Accumlated Loss: 11.082090022159186\n",
    "[Step 60/200] Immediate Loss: 10.42745211555753 Accumlated Loss: 11.106377912412006\n",
    "[Step 70/200] Immediate Loss: 10.89729979255615 Accumlated Loss: 10.661822430367641\n",
    "[Step 80/200] Immediate Loss: 11.12664968320687 Accumlated Loss: 11.273553000268894\n",
    "[Step 90/200] Immediate Loss: 10.563842477298751 Accumlated Loss: 10.434071692100328\n",
    "[Step 100/200] Immediate Loss: 9.646292669946703 Accumlated Loss: 10.903028176131759\n",
    "[Step 110/200] Immediate Loss: 9.969144378144701 Accumlated Loss: 10.307373362012147\n",
    "[Step 120/200] Immediate Loss: 10.420882940550076 Accumlated Loss: 10.795662087340942\n",
    "[Step 130/200] Immediate Loss: 10.290492895487551 Accumlated Loss: 10.682548335150424\n",
    "[Step 140/200] Immediate Loss: 10.306532642289314 Accumlated Loss: 10.99959521952062\n",
    "[Step 150/200] Immediate Loss: 10.000429323109419 Accumlated Loss: 9.998576474693122\n",
    "[Step 160/200] Immediate Loss: 10.588428806832116 Accumlated Loss: 10.307206716992422\n",
    "[Step 170/200] Immediate Loss: 10.506099945439775 Accumlated Loss: 10.002980072034783\n",
    "[Step 180/200] Immediate Loss: 10.149811753013926 Accumlated Loss: 10.48670229063121\n",
    "[Step 190/200] Immediate Loss: 9.760906426692092 Accumlated Loss: 10.158174175907542\n",
    "[Step 200/200] Immediate Loss: 10.901707479956386 Accumlated Loss: 10.22883899761621"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-essay",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
