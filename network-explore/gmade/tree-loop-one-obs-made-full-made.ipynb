{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "standard-happening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "hid_orderings [1, 2, 2, 1, 2, 2, 1, 0, 1, 0, 2, 1, 2, 0, 0, 0] size 16\n",
      "num hid layers: 1\n",
      "expanded_input_ordering [0, 0, 0, 0, 0, 0, 0, 0, 1] size 9\n",
      "expanded_output_ordering [2, 2] size 2\n",
      "hid_orderings [1, 2, 1, 3, 2, 0, 0, 2, 0, 3, 0, 3, 0, 4, 2, 6, 1, 3, 5, 2, 2, 4, 6, 6, 2, 5, 4, 2, 1, 0, 6, 0] size 32\n",
      "num hid layers: 1\n",
      "expanded_input_ordering [0, 1, 1, 1, 1, 1, 1, 1, 1, 2] size 10\n",
      "expanded_output_ordering [6, 6, 3, 3] size 4\n",
      "hid_orderings [7, 8, 2, 9, 13, 11, 5, 1, 14, 7, 9, 12, 5, 0, 2, 10, 5, 13, 11, 14, 10, 4, 11, 0, 7, 13, 7, 2, 7, 8, 5, 13, 5, 4, 7, 3, 4, 5, 14, 1, 2, 2, 14, 13, 2, 8, 1, 4, 5, 8, 13, 12, 8, 11, 7, 10, 11, 3, 12, 8, 7, 5, 3, 7] size 64\n",
      "num hid layers: 1\n",
      "expanded_input_ordering [0, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3] size 11\n",
      "expanded_output_ordering [8, 8, 12, 12, 9, 9] size 6\n",
      "hid_orderings [7, 39, 56, 49, 38, 31, 43, 7, 3, 54, 22, 42, 26, 22, 48, 21, 47, 51, 11, 61, 3, 26, 31, 52, 11, 35, 23, 58, 7, 16, 51, 20, 59, 3, 24, 6, 36, 60, 20, 40, 16, 27, 36, 15, 43, 31, 57, 17, 60, 11, 1, 41, 46, 30, 2, 62, 8, 54, 56, 13, 54, 29, 56, 55, 48, 18, 36, 61, 20, 24, 43, 3, 15, 10, 26, 26, 38, 31, 10, 2, 33, 48, 58, 52, 19, 54, 29, 17, 5, 27, 34, 17, 12, 59, 4, 59, 25, 49, 6, 35, 47, 27, 22, 31, 12, 13, 15, 2, 57, 4, 58, 0, 41, 34, 2, 59, 46, 55, 27, 18, 11, 7, 42, 51, 13, 1, 22, 9, 14, 13, 41, 27, 35, 24, 61, 31, 2, 8, 46, 57, 7, 28, 54, 34, 1, 32, 18, 13, 59, 6, 52, 47, 37, 20, 5, 42, 60, 11, 7, 29, 53, 0, 49, 5, 2, 9, 28, 59, 41, 3, 9, 29, 5, 42, 22, 60, 51, 51, 14, 27, 42, 5, 0, 20, 25, 49, 51, 3, 4, 58, 18, 7, 49, 49, 38, 48, 54, 53, 16, 26, 62, 23, 39, 53, 55, 37, 22, 43, 0, 22, 47, 40, 5, 45, 38, 28, 6, 37, 46, 12, 39, 4, 12, 22, 49, 17, 56, 61, 10, 1, 45, 48, 56, 42, 49, 23, 6, 62, 38, 17, 56, 4, 3, 16, 8, 25, 39, 32, 34, 7, 33, 61, 20, 50, 40, 48] size 256\n",
      "num hid layers: 1\n",
      "expanded_input_ordering [0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 4, 5] size 13\n",
      "expanded_output_ordering [11, 11, 6, 6, 36, 36, 22, 22] size 8\n",
      "hid_orderings [41, 27, 14, 5, 7, 6, 48, 52, 51, 55, 22, 42, 51, 8, 0, 13, 57, 50, 8, 53, 11, 16, 10, 55, 52, 4, 1, 49, 24, 35, 40, 34, 11, 19, 37, 49, 21, 1, 9, 17, 60, 26, 31, 26, 54, 50, 9, 41, 34, 56, 3, 36, 11, 35, 51, 27, 33, 23, 59, 54, 5, 2, 33, 19, 1, 42, 9, 40, 6, 44, 52, 50, 55, 38, 21, 11, 41, 61, 41, 6, 2, 9, 50, 8, 44, 36, 22, 26, 16, 25, 11, 33, 41, 34, 62, 58, 56, 38, 9, 47, 59, 37, 28, 41, 39, 52, 30, 23, 50, 29, 40, 17, 33, 35, 36, 49, 21, 10, 23, 36, 30, 23, 26, 11, 6, 41, 4, 36, 61, 31, 44, 48, 30, 33, 14, 39, 33, 21, 59, 39, 26, 56, 48, 61, 21, 30, 14, 45, 52, 43, 44, 9, 58, 8, 50, 10, 43, 46, 11, 35, 15, 60, 11, 61, 31, 0, 9, 22, 42, 40, 14, 26, 53, 29, 20, 40, 54, 43, 61, 27, 0, 2, 61, 23, 49, 61, 34, 42, 35, 45, 51, 43, 41, 2, 7, 28, 51, 52, 26, 52, 21, 40, 7, 60, 22, 19, 4, 12, 2, 35, 61, 0, 30, 26, 44, 4, 11, 21, 20, 26, 30, 24, 22, 59, 6, 16, 43, 23, 11, 54, 26, 40, 58, 58, 27, 28, 2, 3, 58, 59, 24, 52, 46, 23, 3, 38, 7, 27, 31, 2, 12, 18, 44, 9, 38, 22] size 256\n",
      "num hid layers: 1\n",
      "expanded_input_ordering [0, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 5] size 13\n",
      "expanded_output_ordering [25, 25, 34, 34, 19, 19] size 6\n",
      "hid_orderings [3, 6, 4, 5, 1, 1, 3, 1, 2, 5, 1, 3, 2, 4, 3, 6, 3, 6, 0, 2, 3, 0, 6, 5, 2, 3, 3, 1, 0, 4, 5, 3] size 32\n",
      "num hid layers: 1\n",
      "expanded_input_ordering [0, 1, 2, 2, 2, 2, 2, 2, 2, 2] size 10\n",
      "expanded_output_ordering [6, 6] size 2\n",
      "number of levels: 6\n",
      "input_levels [['h', 'r'], ['z2', 'h', 'r'], ['z2', 'z1', 'y4', 'h'], ['y2', 'h', 'y3', 'z1', 'y4', 'x8'], ['y2', 'x6', 'x4', 'h', 'y3', 'y1'], ['x2', 'y1', 'h']]\n",
      "out_levels [['z2'], ['z1', 'y4'], ['y2', 'y3', 'x8'], ['x6', 'x4', 'x7', 'y1'], ['x3', 'x5', 'x2'], ['x1']]\n",
      "[Step 10/300] Immediate Loss: 8.02533 Accumlated Loss: 12.78738 Duration: 3.275\n",
      "[Step 20/300] Immediate Loss: 7.39169 Accumlated Loss: 7.67554 Duration: 3.528\n",
      "[Step 30/300] Immediate Loss: 7.16713 Accumlated Loss: 7.16963 Duration: 3.833\n",
      "[Step 40/300] Immediate Loss: 6.88848 Accumlated Loss: 7.02408 Duration: 3.395\n",
      "[Step 50/300] Immediate Loss: 7.01632 Accumlated Loss: 6.95018 Duration: 3.409\n",
      "[Step 60/300] Immediate Loss: 6.91366 Accumlated Loss: 6.91229 Duration: 3.137\n",
      "[Step 70/300] Immediate Loss: 6.72024 Accumlated Loss: 6.84139 Duration: 3.220\n",
      "[Step 80/300] Immediate Loss: 6.93761 Accumlated Loss: 6.88348 Duration: 3.214\n",
      "[Step 90/300] Immediate Loss: 6.86363 Accumlated Loss: 6.81640 Duration: 3.210\n",
      "[Step 100/300] Immediate Loss: 6.68868 Accumlated Loss: 6.84456 Duration: 3.103\n",
      "[Step 110/300] Immediate Loss: 6.84865 Accumlated Loss: 6.81920 Duration: 3.029\n",
      "[Step 120/300] Immediate Loss: 6.62373 Accumlated Loss: 6.77832 Duration: 3.117\n",
      "[Step 130/300] Immediate Loss: 6.95872 Accumlated Loss: 6.82942 Duration: 3.012\n",
      "[Step 140/300] Immediate Loss: 6.87927 Accumlated Loss: 6.80602 Duration: 3.030\n",
      "[Step 150/300] Immediate Loss: 6.81730 Accumlated Loss: 6.80541 Duration: 3.006\n",
      "[Step 160/300] Immediate Loss: 6.80501 Accumlated Loss: 6.79584 Duration: 3.010\n",
      "[Step 170/300] Immediate Loss: 6.78234 Accumlated Loss: 6.81659 Duration: 3.108\n",
      "[Step 180/300] Immediate Loss: 6.77877 Accumlated Loss: 6.74375 Duration: 3.112\n",
      "[Step 190/300] Immediate Loss: 6.65401 Accumlated Loss: 6.72553 Duration: 3.222\n",
      "[Step 200/300] Immediate Loss: 6.72427 Accumlated Loss: 6.75629 Duration: 3.117\n",
      "[Step 210/300] Immediate Loss: 6.78935 Accumlated Loss: 6.78749 Duration: 3.024\n",
      "[Step 220/300] Immediate Loss: 6.78856 Accumlated Loss: 6.73344 Duration: 3.128\n",
      "[Step 230/300] Immediate Loss: 6.84559 Accumlated Loss: 6.76212 Duration: 3.011\n",
      "[Step 240/300] Immediate Loss: 6.81425 Accumlated Loss: 6.75706 Duration: 3.043\n",
      "[Step 250/300] Immediate Loss: 6.81598 Accumlated Loss: 6.74790 Duration: 2.934\n",
      "[Step 260/300] Immediate Loss: 6.72477 Accumlated Loss: 6.75339 Duration: 3.090\n",
      "[Step 270/300] Immediate Loss: 6.71557 Accumlated Loss: 6.71794 Duration: 2.969\n",
      "[Step 280/300] Immediate Loss: 6.80483 Accumlated Loss: 6.75686 Duration: 3.048\n",
      "[Step 290/300] Immediate Loss: 6.75050 Accumlated Loss: 6.75572 Duration: 3.013\n",
      "[Step 300/300] Immediate Loss: 6.70245 Accumlated Loss: 6.73883 Duration: 3.193\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pyro\n",
    "import pyro.contrib.examples.polyphonic_data_loader as poly\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "from made import MADE\n",
    "from gmade import GMADE\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal, AutoNormal, AutoMultivariateNormal\n",
    "\n",
    "random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# NN used for p(x | y)\n",
    "class simpleNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden=32, out_size=1, t=\"normal\", out_non_linear=None):\n",
    "        super().__init__()\n",
    "        self.t = t\n",
    "        self.out_non_linear = out_non_linear\n",
    "        self.hiddeen_layer = nn.Linear(input_size, hidden)\n",
    "        if t == \"normal\":\n",
    "            self.loc_layer = nn.Linear(hidden, out_size)\n",
    "            self.std_layer = nn.Linear(hidden, out_size)\n",
    "            self.softplus = nn.Softplus()\n",
    "        elif t == \"bern\":\n",
    "            self.prob_layer = nn.Linear(hidden, out_size)\n",
    "        elif t == \"mlp\":\n",
    "            self.out_layer = nn.Linear(hidden, out_size)\n",
    "        \n",
    "    def forward(self, x_list):\n",
    "        for i in range(len(x_list)):\n",
    "            if x_list[i].dim() == 0:\n",
    "                x_list[i] = torch.unsqueeze(x_list[i], dim=0)\n",
    "        input_x = torch.cat(x_list)\n",
    "        hid = F.relu(self.hiddeen_layer(input_x))\n",
    "        # return loc, std\n",
    "        if self.t == \"normal\":\n",
    "            return self.loc_layer(hid), self.softplus(self.std_layer(hid))\n",
    "        elif self.t == \"bern\":\n",
    "            return torch.sigmoid(self.prob_layer(hid))\n",
    "        else:\n",
    "            if self.out_non_linear == \"tanh\":\n",
    "                return torch.tanh(self.out_layer(hid))\n",
    "            else:\n",
    "                return self.out_layer(hid)\n",
    "\n",
    "class Experiment(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # guide full_made\n",
    "        self.hidden_size_full_made = 8\n",
    "        input_dim_dict = {\n",
    "            \"h\" : self.hidden_size_full_made,\n",
    "            \"r\" : 1\n",
    "        }\n",
    "        var_dim_dict = {\n",
    "            \"x1\" : 1,\n",
    "            \"x2\" : 1,\n",
    "            \"x3\" : 1,\n",
    "            \"x4\" : 1,\n",
    "            \"x5\" : 1,\n",
    "            \"x6\" : 1,\n",
    "            \"x7\" : 1,\n",
    "            \"x8\" : 1,\n",
    "            \"y1\" : 1,\n",
    "            \"y2\" : 1,\n",
    "            \"y3\" : 1,\n",
    "            \"y4\" : 1,\n",
    "            \"z1\" : 1,\n",
    "            \"z2\" : 1\n",
    "        }\n",
    "        dependency_dict = {\n",
    "            \"z2\" : [\"h\", \"r\"],\n",
    "            \"z1\" : [\"h\", \"r\", \"z2\"],\n",
    "            \"y4\" : [\"h\", \"z2\"],\n",
    "            \"y3\" : [\"h\", \"z2\", \"y4\"],\n",
    "            \"y2\" : [\"h\", \"z1\"],\n",
    "            \"y1\" : [\"h\", \"z1\", \"y2\"],\n",
    "            \"x8\" : [\"h\", \"y4\"],\n",
    "            \"x7\" : [\"h\", \"y4\", \"x8\"],\n",
    "            \"x6\" : [\"h\", \"y3\"],\n",
    "            \"x5\" : [\"h\", \"y3\", \"x6\"],\n",
    "            \"x4\" : [\"h\", \"y2\"],\n",
    "            \"x3\" : [\"h\", \"y2\", \"x4\"],\n",
    "            \"x2\" : [\"h\", \"y1\"],\n",
    "            \"x1\" : [\"h\", \"y1\", \"x2\"]\n",
    "        }\n",
    "        \n",
    "   \n",
    "        self.full_made = GMADE(input_dim_dict, dependency_dict, var_dim_dict)\n",
    "        self.h0_full_made = nn.Parameter(torch.zeros(self.hidden_size_full_made))\n",
    "        self.hid_net_full_made = simpleNN(self.hidden_size_full_made + 8 + 4 + 2, out_size = self.hidden_size_full_made, t = \"mlp\")\n",
    "        \n",
    "    def model(self, n, obs):\n",
    "        def tree_model(i, mu):\n",
    "            x1 = pyro.sample(f\"x1{i}\", dist.Normal(mu, 1.0))\n",
    "            x2 = pyro.sample(f\"x2{i}\", dist.Normal(mu, 1.0))\n",
    "            x3 = pyro.sample(f\"x3{i}\", dist.Normal(mu, 1.0))\n",
    "            x4 = pyro.sample(f\"x4{i}\", dist.Normal(mu, 1.0))\n",
    "            x5 = pyro.sample(f\"x5{i}\", dist.Normal(mu, 1.0))\n",
    "            x6 = pyro.sample(f\"x6{i}\", dist.Normal(mu, 1.0))\n",
    "            x7 = pyro.sample(f\"x7{i}\", dist.Normal(mu, 1.0))\n",
    "            x8 = pyro.sample(f\"x8{i}\", dist.Normal(mu, 1.0))\n",
    "            y1 = pyro.sample(f\"y1{i}\", dist.Normal(x1+x2, 1.0))\n",
    "            y2 = pyro.sample(f\"y2{i}\", dist.Normal(x3+x4, 1.0))\n",
    "            y3 = pyro.sample(f\"y3{i}\", dist.Normal(x5+x6, 1.0))\n",
    "            y4 = pyro.sample(f\"y4{i}\", dist.Normal(x7+x8, 1.0))\n",
    "            z1 = pyro.sample(f\"z1{i}\", dist.Normal(y1+y2, 1.0))\n",
    "            z2 = pyro.sample(f\"z2{i}\", dist.Normal(y3+y4, 1.0))\n",
    "            return z1 + z2  \n",
    "            \n",
    "        pyro.module(\"model\", self)\n",
    "        mu = 0\n",
    "        for i in range(n):\n",
    "            mu = tree_model(i, mu)\n",
    "        \n",
    "        pyro.sample(\"obs\", dist.Normal(mu, 1.0), obs=obs)\n",
    "    \n",
    "    # reverse order of guide 6\n",
    "    def guide_full_made(self, n, obs):\n",
    "        def tree_guide(i, hid, last=False):\n",
    "            # has to assume all interations are identical\n",
    "            input_made = {\n",
    "                \"h\" : hid,\n",
    "                \"r\" : obs\n",
    "            }\n",
    "            if not last: \n",
    "                input_made[\"r\"] = torch.tensor([0.])\n",
    "            output_dict = self.full_made(input_made, suffix=str(i))\n",
    "            input_hid = [hid]\n",
    "            input_hid.extend(output_dict.values())\n",
    "            \n",
    "            # feed all outputs to update hid\n",
    "            hid = self.hid_net_full_made(input_hid)\n",
    "\n",
    "            return hid\n",
    "            \n",
    "        pyro.module(\"model\", self)\n",
    "        hid = self.h0_full_made\n",
    "        for i in range(n-1, -1, -1):\n",
    "            hid = tree_guide(i, hid, i==n-1)\n",
    "\n",
    "def generate_data():\n",
    "    \n",
    "    n_min = 2\n",
    "    n_max = 4\n",
    "    n = random.randint(n_min, n_max)\n",
    "    mu = 0\n",
    "    x_len = 8\n",
    "    for i in range(n):\n",
    "        x_noise = torch.randn(x_len) / 4\n",
    "        x_mean = torch.zeros(x_len) + mu\n",
    "        xs = torch.normal(x_mean, 1) + x_noise\n",
    "        ys = []\n",
    "        j = 0\n",
    "        while j < len(xs):\n",
    "            y = dist.Normal(xs[j] + xs[j+1], 2).sample()\n",
    "            ys.append(y)\n",
    "            j +=2\n",
    "        \n",
    "        zs = []\n",
    "        j = 0\n",
    "        while j < len(ys):\n",
    "            z = dist.Normal(ys[j] + ys[j+1], 1.5).sample()\n",
    "            zs.append(z)\n",
    "            j +=2\n",
    "        \n",
    "        \n",
    "        mu = dist.Normal(zs[0] + zs[1], 1).sample() / 10\n",
    "        \n",
    "    return n, mu\n",
    "    \n",
    "data = []\n",
    "num_data = 100\n",
    "for _ in range(num_data):\n",
    "    data.append(generate_data())\n",
    "\n",
    "print(len(data))\n",
    "experiment = Experiment()\n",
    "adam_params = {\"lr\": 0.001, \"betas\": (0.95, 0.999)}\n",
    "optimizer = Adam(adam_params)\n",
    "guide = experiment.guide_full_made # guide_1\n",
    "\n",
    "svi = SVI(experiment.model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "n_steps = 300\n",
    "log_interval = 10\n",
    "# do gradient steps\n",
    "loss = 0\n",
    "loss_track = []\n",
    "for step in range(1, n_steps + 1):\n",
    "    imme_loss = 0\n",
    "    start = time.time()\n",
    "    for n, obs in data:\n",
    "        imme_loss += svi.step(n, obs.unsqueeze(0)) / num_data\n",
    "        \n",
    "    loss_track.append(imme_loss)\n",
    "    loss += imme_loss / log_interval\n",
    "    \n",
    "    if step % log_interval == 0:\n",
    "        print(\"[Step {}/{}] Immediate Loss: {:.5f} Accumlated Loss: {:.5f} Duration: {:.3f}\".format(step, n_steps, imme_loss, loss, time.time() - start))\n",
    "        loss = 0\n",
    "    \n",
    "plt.plot(loss_track)\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-spine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-difference",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sharp-egypt",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-f38d0597d154>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-f38d0597d154>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    hidden_size = (|all_level_sets| + 1) * 4\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# full made\n",
    "hidden_size = (|all_level_sets| + 1) * 4 \n",
    "[Step 10/200] Immediate Loss: 7.73021 Accumlated Loss: 12.15332 Duration: 2.633\n",
    "[Step 20/200] Immediate Loss: 7.16565 Accumlated Loss: 7.40011 Duration: 2.587\n",
    "[Step 30/200] Immediate Loss: 7.08254 Accumlated Loss: 7.03757 Duration: 2.596\n",
    "[Step 40/200] Immediate Loss: 6.83297 Accumlated Loss: 6.98378 Duration: 2.672\n",
    "[Step 50/200] Immediate Loss: 6.86551 Accumlated Loss: 6.91275 Duration: 2.600\n",
    "[Step 60/200] Immediate Loss: 6.93487 Accumlated Loss: 6.87233 Duration: 2.598\n",
    "[Step 70/200] Immediate Loss: 6.71809 Accumlated Loss: 6.81451 Duration: 2.657\n",
    "[Step 80/200] Immediate Loss: 6.80281 Accumlated Loss: 6.88790 Duration: 2.612\n",
    "[Step 90/200] Immediate Loss: 6.90435 Accumlated Loss: 6.78826 Duration: 2.664\n",
    "[Step 100/200] Immediate Loss: 6.73903 Accumlated Loss: 6.83094 Duration: 2.606\n",
    "[Step 110/200] Immediate Loss: 6.89314 Accumlated Loss: 6.80484 Duration: 2.614\n",
    "[Step 120/200] Immediate Loss: 6.80861 Accumlated Loss: 6.74317 Duration: 2.604\n",
    "[Step 130/200] Immediate Loss: 6.80884 Accumlated Loss: 6.77493 Duration: 2.629\n",
    "[Step 140/200] Immediate Loss: 6.83799 Accumlated Loss: 6.81526 Duration: 2.629\n",
    "[Step 150/200] Immediate Loss: 6.75561 Accumlated Loss: 6.78899 Duration: 2.585\n",
    "[Step 160/200] Immediate Loss: 6.70648 Accumlated Loss: 6.76897 Duration: 2.611\n",
    "[Step 170/200] Immediate Loss: 6.71425 Accumlated Loss: 6.77154 Duration: 2.627\n",
    "[Step 180/200] Immediate Loss: 6.73846 Accumlated Loss: 6.73665 Duration: 2.650\n",
    "[Step 190/200] Immediate Loss: 6.62496 Accumlated Loss: 6.73855 Duration: 2.750\n",
    "[Step 200/200] Immediate Loss: 6.67873 Accumlated Loss: 6.73655 Duration: 2.694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-nightlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_made\n",
    "hidden_size = 100\n",
    "[Step 10/100] Immediate Loss: 7.943362760543822 Accumlated Loss: 12.182324503213167\n",
    "[Step 20/100] Immediate Loss: 7.575157041847708 Accumlated Loss: 7.837963561832905\n",
    "[Step 30/100] Immediate Loss: 6.971591109633447 Accumlated Loss: 7.41288396987319\n",
    "[Step 40/100] Immediate Loss: 7.35790877491236 Accumlated Loss: 7.329814511984587\n",
    "[Step 50/100] Immediate Loss: 7.227651714682577 Accumlated Loss: 7.152353108674289\n",
    "[Step 60/100] Immediate Loss: 7.120855849683284 Accumlated Loss: 7.094745240241287\n",
    "[Step 70/100] Immediate Loss: 6.898626097440719 Accumlated Loss: 6.994988939702512\n",
    "[Step 80/100] Immediate Loss: 6.973135902881624 Accumlated Loss: 6.9992388206422325\n",
    "[Step 90/100] Immediate Loss: 6.978269910216333 Accumlated Loss: 6.970272527605296\n",
    "[Step 100/100] Immediate Loss: 6.8891827100515375 Accumlated Loss: 6.946669360905885"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_made\n",
    "hidden_size = 256\n",
    "[Step 10/300] Immediate Loss: 7.3377627801895144 Accumlated Loss: 10.410332291990517\n",
    "[Step 20/300] Immediate Loss: 7.159581978321075 Accumlated Loss: 7.200047956764698\n",
    "[Step 30/300] Immediate Loss: 7.061564602553847 Accumlated Loss: 7.006011275857687\n",
    "[Step 40/300] Immediate Loss: 6.9438557976484265 Accumlated Loss: 6.986207850277424\n",
    "[Step 50/300] Immediate Loss: 6.79013857513666 Accumlated Loss: 6.907507795482872\n",
    "[Step 60/300] Immediate Loss: 6.964799135029316 Accumlated Loss: 6.948932709783316\n",
    "[Step 70/300] Immediate Loss: 6.827503361701965 Accumlated Loss: 6.935307196795939\n",
    "[Step 80/300] Immediate Loss: 6.932054169476035 Accumlated Loss: 6.893120049089195\n",
    "[Step 90/300] Immediate Loss: 6.959000964760777 Accumlated Loss: 6.910556671291589\n",
    "[Step 100/300] Immediate Loss: 6.850004935860634 Accumlated Loss: 6.864318913996221\n",
    "[Step 110/300] Immediate Loss: 6.94344320565462 Accumlated Loss: 6.850439666450024\n",
    "[Step 120/300] Immediate Loss: 6.898613585829735 Accumlated Loss: 6.8139530434310425\n",
    "[Step 130/300] Immediate Loss: 6.832337006330492 Accumlated Loss: 6.831135106772184\n",
    "[Step 140/300] Immediate Loss: 6.981345642507076 Accumlated Loss: 6.840172601640226\n",
    "[Step 150/300] Immediate Loss: 6.8799775546789155 Accumlated Loss: 6.84133131918311\n",
    "[Step 160/300] Immediate Loss: 6.955301420390605 Accumlated Loss: 6.884279209256171\n",
    "[Step 170/300] Immediate Loss: 6.737140662968159 Accumlated Loss: 6.837186959028243\n",
    "[Step 180/300] Immediate Loss: 6.770348101556301 Accumlated Loss: 6.8123826805353165\n",
    "[Step 190/300] Immediate Loss: 6.84570457994938 Accumlated Loss: 6.796486026227475\n",
    "[Step 200/300] Immediate Loss: 6.918634173870087 Accumlated Loss: 6.7889503496289265\n",
    "[Step 210/300] Immediate Loss: 6.818292626738547 Accumlated Loss: 6.842672590941191\n",
    "[Step 220/300] Immediate Loss: 6.885442342460156 Accumlated Loss: 6.7800200353860856\n",
    "[Step 230/300] Immediate Loss: 6.8983984541893 Accumlated Loss: 6.816990964740515\n",
    "[Step 240/300] Immediate Loss: 6.703417011201383 Accumlated Loss: 6.77779578369856\n",
    "[Step 250/300] Immediate Loss: 6.782429467439654 Accumlated Loss: 6.791635487049819\n",
    "[Step 260/300] Immediate Loss: 6.779994890689852 Accumlated Loss: 6.800842138975859\n",
    "[Step 270/300] Immediate Loss: 6.651651687920095 Accumlated Loss: 6.76608702453971\n",
    "[Step 280/300] Immediate Loss: 6.800747621953487 Accumlated Loss: 6.8188140452504165\n",
    "[Step 290/300] Immediate Loss: 6.772528566718105 Accumlated Loss: 6.796392483770848\n",
    "[Step 300/300] Immediate Loss: 6.761312774717806 Accumlated Loss: 6.7664973594844335"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_made\n",
    "hidden_size = 512\n",
    "[Step 10/200] Immediate Loss: 7.271965174973012 Accumlated Loss: 9.238334188848734\n",
    "[Step 20/200] Immediate Loss: 7.186947046220303 Accumlated Loss: 7.23573069420457\n",
    "[Step 30/200] Immediate Loss: 7.122338608205319 Accumlated Loss: 7.092619772553443\n",
    "[Step 40/200] Immediate Loss: 6.837136044204232 Accumlated Loss: 7.121531615853309\n",
    "[Step 50/200] Immediate Loss: 6.922652315497399 Accumlated Loss: 7.035050431400538\n",
    "[Step 60/200] Immediate Loss: 7.0224101865291635 Accumlated Loss: 7.050180885791779\n",
    "[Step 70/200] Immediate Loss: 6.878841565251351 Accumlated Loss: 6.977283853858709\n",
    "[Step 80/200] Immediate Loss: 6.917600260972978 Accumlated Loss: 6.960183880478144\n",
    "[Step 90/200] Immediate Loss: 7.015150077044962 Accumlated Loss: 6.9326501165926455\n",
    "[Step 100/200] Immediate Loss: 6.942776599228382 Accumlated Loss: 6.940360210925341\n",
    "[Step 110/200] Immediate Loss: 6.844236496090893 Accumlated Loss: 6.884334388613701\n",
    "[Step 120/200] Immediate Loss: 6.8093515345454225 Accumlated Loss: 6.9124150166511535\n",
    "[Step 130/200] Immediate Loss: 6.829742189943792 Accumlated Loss: 6.877818616390228\n",
    "[Step 140/200] Immediate Loss: 6.9080816498398745 Accumlated Loss: 6.879785261124372\n",
    "[Step 150/200] Immediate Loss: 6.924012284278872 Accumlated Loss: 6.912908691138029\n",
    "[Step 160/200] Immediate Loss: 6.945983737111093 Accumlated Loss: 6.874559896111489\n",
    "[Step 170/200] Immediate Loss: 6.915178183317185 Accumlated Loss: 6.880107059150934\n",
    "[Step 180/200] Immediate Loss: 6.651397094726563 Accumlated Loss: 6.78183588451147\n",
    "[Step 190/200] Immediate Loss: 6.8623131331801455 Accumlated Loss: 6.819020005971194\n",
    "[Step 200/200] Immediate Loss: 6.910607502162455 Accumlated Loss: 6.900225434094667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-wagner",
   "metadata": {},
   "outputs": [],
   "source": [
    "### guide 5_1_1 Reference\n",
    "[Step 10/200] Immediate Loss: 7.62717 Accumlated Loss: 10.73827 Duration: 7.676\n",
    "[Step 20/200] Immediate Loss: 7.26588 Accumlated Loss: 7.48846 Duration: 7.717\n",
    "[Step 30/200] Immediate Loss: 6.91100 Accumlated Loss: 7.07743 Duration: 7.722\n",
    "[Step 40/200] Immediate Loss: 6.83163 Accumlated Loss: 6.94489 Duration: 7.689\n",
    "[Step 50/200] Immediate Loss: 6.89025 Accumlated Loss: 6.88702 Duration: 7.662\n",
    "[Step 60/200] Immediate Loss: 6.86455 Accumlated Loss: 6.92596 Duration: 7.748\n",
    "[Step 70/200] Immediate Loss: 6.72298 Accumlated Loss: 6.82665 Duration: 7.640\n",
    "[Step 80/200] Immediate Loss: 6.85124 Accumlated Loss: 6.84992 Duration: 7.598\n",
    "[Step 90/200] Immediate Loss: 6.85684 Accumlated Loss: 6.83059 Duration: 7.608\n",
    "[Step 100/200] Immediate Loss: 6.70233 Accumlated Loss: 6.82399 Duration: 7.613\n",
    "[Step 110/200] Immediate Loss: 6.72158 Accumlated Loss: 6.79653 Duration: 7.902\n",
    "[Step 120/200] Immediate Loss: 6.86706 Accumlated Loss: 6.81691 Duration: 7.493\n",
    "[Step 130/200] Immediate Loss: 6.73154 Accumlated Loss: 6.78102 Duration: 7.514\n",
    "[Step 140/200] Immediate Loss: 6.74963 Accumlated Loss: 6.79802 Duration: 7.489\n",
    "[Step 150/200] Immediate Loss: 6.77680 Accumlated Loss: 6.80260 Duration: 7.672\n",
    "[Step 160/200] Immediate Loss: 6.81672 Accumlated Loss: 6.80855 Duration: 7.539\n",
    "[Step 170/200] Immediate Loss: 6.81623 Accumlated Loss: 6.77428 Duration: 7.604\n",
    "[Step 180/200] Immediate Loss: 6.72911 Accumlated Loss: 6.75416 Duration: 7.737\n",
    "[Step 190/200] Immediate Loss: 6.65551 Accumlated Loss: 6.74630 Duration: 7.945\n",
    "[Step 200/200] Immediate Loss: 6.78092 Accumlated Loss: 6.77993 Duration: 7.496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-bahamas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-impression",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
