{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ef64e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import os\n",
    "import itertools\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pyro\n",
    "import numpy as np\n",
    "import pyro.optim as optim\n",
    "import pyro.distributions as dist\n",
    "import pyro.infer\n",
    "import pyro.optim\n",
    "import time\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceGraph_ELBO\n",
    "from PIL import Image\n",
    "from torch.distributions import constraints\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d3e6be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcfg = {\n",
    "  \"name\": \"astronomers\",\n",
    "  \"terminals\": [\"astronomers\", \"ears\", \"saw\", \"stars\", \"telescopes\", \"with\"],\n",
    "  \"non_terminals\": [\"S\", \"NP\", \"VP\", \"PP\", \"P\", \"V\"],\n",
    "  \"productions\": {\n",
    "    \"S\": [[\"NP\", \"VP\"]],\n",
    "    \"NP\": [[\"NP\", \"PP\"], [\"astronomers\"], [\"ears\"], [\"saw\"], [\"stars\"], [\"telescopes\"]],\n",
    "    \"VP\": [[\"V\", \"NP\"], [\"VP\", \"PP\"]],\n",
    "    \"PP\": [[\"P\", \"NP\"]],\n",
    "    \"P\": [[\"with\"]],\n",
    "    \"V\": [[\"saw\"]]\n",
    "  },\n",
    "  \"start_symbol\": \"S\",\n",
    "}\n",
    "true_production_probs = {\n",
    "    \"S\": [1.0],\n",
    "    \"NP\": [0.4, 0.1, 0.18, 0.04, 0.18, 0.1],\n",
    "    \"VP\": [0.7, 0.3],\n",
    "    \"PP\": [1.0],\n",
    "    \"P\": [1.0],\n",
    "    \"V\": [1.0]\n",
    "}\n",
    "max_depth_parse_tree = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb158fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions from code of paper Revisiting Reweighted Wake-Sleep for Models with Stochastic Control Flow \n",
    "def word_to_index(word, terminals):\n",
    "    \"\"\"Convert word to int.\n",
    "    Args:\n",
    "        word: string\n",
    "        terminals: set of terminal strings\n",
    "    Returns: int; -1 if word is not in terminals\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        return sorted(terminals).index(word)\n",
    "    except ValueError:\n",
    "        return -1\n",
    "\n",
    "def sentence_to_indices(sentence, terminals):\n",
    "    \"\"\"Convert sentence to list of indices.\n",
    "    Args:\n",
    "        sentence: list of strings\n",
    "        terminals: set of terminal strings\n",
    "    Returns: list of indices of length len(sentence); index is -1 if word is\n",
    "        not in terminals\n",
    "    \"\"\"\n",
    "    return [word_to_index(word, terminals) for word in sentence]\n",
    "\n",
    "\n",
    "def _indices_to_string(indices):\n",
    "    return ''.join([string.printable[index] for index in indices])\n",
    "\n",
    "\n",
    "def _sentence_to_string(sentence, terminals):\n",
    "    return _indices_to_string(sentence_to_indices(sentence, terminals))\n",
    "\n",
    "def word_to_one_hot(word, terminals):\n",
    "    \"\"\"Convert word to its one-hot representation.\n",
    "    Args:\n",
    "        word: string\n",
    "        terminals: set of terminal strings\n",
    "    Returns: one hot tensor of shape [len(terminals)] or zeros if word is not\n",
    "        in terminals\n",
    "    \"\"\"\n",
    "    num_bins = len(terminals)\n",
    "    try:\n",
    "        i = sorted(terminals).index(word)\n",
    "        return one_hot(torch.tensor([i]), num_bins)[0]\n",
    "    except ValueError:\n",
    "        return torch.zeros((num_bins,))\n",
    "\n",
    "def sentence_to_one_hots(sentence, terminals):\n",
    "    \"\"\"Convert sentence to one-hots.\n",
    "    Args:\n",
    "        sentence: list of strings\n",
    "        terminals: set of terminal strings\n",
    "    Returns: matrix where ith row corresponds to a one-hot of ith word, shape\n",
    "        [num_words, len(terminals)]\n",
    "    \"\"\"\n",
    "    return torch.cat([word_to_one_hot(word, terminals).unsqueeze(0)\n",
    "                      for word in sentence])\n",
    "\n",
    "def one_hot(indices, num_bins):\n",
    "    \"\"\"Returns one hot vector given indices.\n",
    "    Args:\n",
    "        indices: tensors\n",
    "        num_bins: number of bins\n",
    "    Returns: matrix where ith row corresponds to a one\n",
    "        hot version of indices[i].\n",
    "    \"\"\"\n",
    "    return torch.zeros(len(indices), num_bins).scatter_(\n",
    "        1, indices.long().unsqueeze(-1), 1)\n",
    "\n",
    "def get_sample_address_embedding(non_terminal, non_terminals):\n",
    "    \"\"\"Returns an embedding of the sample address of a production.\n",
    "    Args:\n",
    "        non_terminal: string\n",
    "        non_terminals: set of non_terminal symbols\n",
    "    Returns: one-hot vector\n",
    "    \"\"\"\n",
    "    num_bins = len(non_terminals)\n",
    "    i = sorted(non_terminals).index(non_terminal)\n",
    "    return one_hot(torch.tensor([i]), num_bins)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48f7844f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', ['NP', ['NP', 'saw'], ['PP', ['P', 'with'], ['NP', ['NP', 'telescopes'], ['PP', ['P', 'with'], ['NP', 'astronomers']]]]], ['VP', ['V', 'saw'], ['NP', 'astronomers']]]\n",
      "['saw', 'with', 'telescopes', 'with', 'astronomers', 'saw', 'astronomers']\n",
      "['astronomers', 'ears', 'saw', 'stars', 'telescopes', 'with']\n",
      "2545020\n",
      "tensor([[0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# example:\n",
    "# tree = ['S', ['NP', ['NP', 'saw'], ['PP', ['P', 'with'], ['NP', ['NP', 'telescopes'], ['PP', ['P', 'with'], ['NP', 'astronomers']]]]], ['VP', ['V', 'saw'], ['NP', 'astronomers']]]\n",
    "# sentence = ['saw', 'with', 'telescopes', 'with', 'astronomers', 'saw', 'astronomers']\n",
    "def model(observations=None):\n",
    "    \n",
    "    production_logits = {\n",
    "        k: torch.randn((len(v),))\n",
    "        for k, v in pcfg['productions'].items()\n",
    "    }\n",
    "    \n",
    "    def get_leaves(tree):\n",
    "        \n",
    "        if isinstance(tree, list):\n",
    "            return list(itertools.chain.from_iterable(\n",
    "                [get_leaves(subtree) for subtree in tree[1:]]))\n",
    "        else:\n",
    "            return [tree]\n",
    "    \n",
    "    def sample_parse_tree(symbol = None, depth = 0, suffix = 0):\n",
    "        if symbol is None:\n",
    "            symbol = pcfg['start_symbol']\n",
    "        if symbol in pcfg['terminals']:\n",
    "            return symbol\n",
    "        elif depth > max_depth_parse_tree:\n",
    "            return symbol\n",
    "        else:\n",
    "            distribution = dist.Categorical(logits=production_logits[symbol])\n",
    "            production_index = pyro.sample(f\"production_index_{depth}_{suffix}\", distribution)\n",
    "            production = pcfg['productions'][symbol][production_index]\n",
    "            return [symbol] + \\\n",
    "                [sample_parse_tree(s, depth=depth + 1, suffix = i) for i, s in enumerate(production)]\n",
    "    tree = sample_parse_tree()\n",
    "    sentence = get_leaves(tree)\n",
    "    # how do we state observations, levenshtein distance?\n",
    "    observation_type = 1\n",
    "#     if observation_type == 1:\n",
    "#         # pad and delta distribution\n",
    "#         gen_sentence = sentence_to_one_hots(sentence, pcfg[\"terminals\"]\n",
    "#         obs_sentence = sentence_to_one_hots(observations[\"obs_sentence\"], pcfg[\"terminals\"])\n",
    "#         pyro.sample(\"obs\", )\n",
    "#     if observation_type == 2:\n",
    "#         # pad and normal distribution but very low sigma\n",
    "#     if observation_type == 3:\n",
    "#         # compute levenshtein distance between gen and obs sentences and observe with 0\n",
    "\n",
    "model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ac12c3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', ['NP', 'ears'], ['VP', ['V', 'saw'], ['NP', 'stars']]]\n"
     ]
    }
   ],
   "source": [
    "obs_embedding_dim = 100\n",
    "inference_hidden_dim = 100\n",
    "sample_embedding_dim = max([len(v) for _, v in pcfg['productions'].items()])\n",
    "sample_address_embedding_dim = len(pcfg['non_terminals'])\n",
    "sentence_embedder_gru = nn.GRU(\n",
    "                input_size=len(pcfg['terminals']),\n",
    "                hidden_size=obs_embedding_dim,\n",
    "                num_layers=1)\n",
    "\n",
    "inference_gru = nn.GRUCell(\n",
    "            input_size=obs_embedding_dim + sample_embedding_dim\n",
    "            + sample_address_embedding_dim,\n",
    "            hidden_size=inference_hidden_dim)\n",
    "\n",
    "proposal_layers = nn.ModuleDict({\n",
    "            k: nn.Sequential(nn.Linear(inference_hidden_dim, 50),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(50, 25),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(25, len(v)))\n",
    "            for k, v in pcfg['productions'].items()})\n",
    "\n",
    "def get_inference_gru_output(obs_embedding,\n",
    "                             previous_sample_embedding,\n",
    "                             sample_address_embedding,\n",
    "                             inference_hidden):\n",
    "        input = torch.cat([obs_embedding,\n",
    "                       previous_sample_embedding,\n",
    "                       sample_address_embedding]).unsqueeze(0)\n",
    "        return inference_gru(input, inference_hidden.unsqueeze(0)).squeeze(0)\n",
    "    \n",
    "def get_logits_from_inference_gru_output(inference_gru_output,\n",
    "                                             non_terminal):\n",
    "        \"\"\"Args:\n",
    "            inference_gru_output: tensor of shape [inference_hidden_dim]\n",
    "            non_terminal: string\n",
    "        Returns: logits for Categorical distribution\n",
    "        \"\"\"\n",
    "\n",
    "        input_ = inference_gru_output.unsqueeze(0)\n",
    "        return proposal_layers[non_terminal](input_).squeeze(0)\n",
    "    \n",
    "def guide(observations=None):\n",
    "    obs_embedding, _ = sentence_embedder_gru(sentence_to_one_hots(observations[\"obs_sentence\"], pcfg[\"terminals\"]).unsqueeze(1))\n",
    "    obs_embedding = obs_embedding[-1][0]\n",
    "    def sample_parse_tree(symbol = None, previous_sample_embedding = None, inference_hidden = None, depth = 0, suffix = 0):\n",
    "        if symbol is None:\n",
    "            symbol = pcfg['start_symbol']\n",
    "    \n",
    "        if previous_sample_embedding is None:\n",
    "            previous_sample_embedding = torch.zeros(\n",
    "                (sample_embedding_dim,))\n",
    "        \n",
    "        if inference_hidden is None:\n",
    "            inference_hidden = torch.zeros((inference_hidden_dim,))\n",
    "            \n",
    "        if symbol in pcfg['terminals']:\n",
    "            return symbol\n",
    "        elif depth > max_depth_parse_tree:\n",
    "            return symbol\n",
    "        else:\n",
    "            # one-hot representation of non-terminal\n",
    "            sample_address_embedding = get_sample_address_embedding(\n",
    "                symbol, pcfg['non_terminals'])\n",
    "            # get the output from inference gru\n",
    "            inference_gru_output = get_inference_gru_output(\n",
    "                obs_embedding, previous_sample_embedding,\n",
    "                sample_address_embedding, inference_hidden)\n",
    "            # compute the logits from proposal layers\n",
    "            logits = get_logits_from_inference_gru_output(\n",
    "                inference_gru_output, symbol)\n",
    "            distribution = dist.Categorical(logits=logits)\n",
    "            production_index = pyro.sample(f\"production_index_{depth}_{suffix}\", distribution)\n",
    "            production = pcfg['productions'][symbol][production_index]\n",
    "            sample_embedding = one_hot(production_index.unsqueeze(0), sample_embedding_dim)[0]\n",
    "            return [symbol] + \\\n",
    "                [sample_parse_tree(s, sample_embedding, inference_gru_output, depth=depth + 1, suffix = i) for i, s in enumerate(production)]\n",
    "    return sample_parse_tree()\n",
    "print(guide({\"obs_sentence\" : ['saw', 'with', 'telescopes', 'with', 'astronomers', 'saw', 'astronomers']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae37eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba3310a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
